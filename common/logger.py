"""
Logging utilities for The Projection Wizard.
Provides run-scoped logging with cloud-native output to stdout/stderr for Google Cloud Logging.
Includes both human-readable summaries and machine-parseable JSON logs.

=============================================================================
âš ï¸  CLOUD-NATIVE LOGGING REFACTORING âš ï¸
=============================================================================

This logging module has been refactored for cloud-native operation in environments
like Google Cloud Run. Key changes:

Removed Local File Logging:
- No more writing to local log files (*.log, *.jsonl)
- All log output now goes to stdout/stderr for Cloud Logging capture
- Local file paths and stage-specific log files are obsolete

JSON-First Approach:
- Structured logging uses JSON formatting for better parsing in Cloud Logging
- Human-readable logging still available but JSON preferred for production
- All logs include run_id and stage context for filtering and aggregation

Cloud Logging Integration:
- stdout logs are automatically captured by Google Cloud Logging
- JSON logs are parsed and indexed for advanced querying
- Log levels and structured data preserved for monitoring and alerting

Backward Compatibility:
- Function signatures remain unchanged for existing pipeline code
- Same logger names and hierarchies for easy migration
- Error handling patterns preserved

=============================================================================
"""

import logging
import sys
import json
from pathlib import Path
from typing import Optional, Dict, Any, List, Union
from datetime import datetime
import numpy as np

from .constants import PIPELINE_STAGES


def json_serializer(obj):
    """
    Custom JSON serializer that handles numpy types and other non-serializable objects.
    
    Args:
        obj: Object to serialize
        
    Returns:
        JSON-serializable version of the object
    """
    if isinstance(obj, (np.integer, np.floating)):
        return obj.item()  # Convert numpy types to Python types
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif hasattr(obj, 'isoformat'):  # datetime objects
        return obj.isoformat()
    elif hasattr(obj, '__dict__'):  # Custom objects
        return obj.__dict__
    else:
        # Fallback: convert to string
        return str(obj)


class JSONFormatter(logging.Formatter):
    """
    Custom JSON formatter for structured logging compatible with Google Cloud Logging.
    Produces clean JSON-line format suitable for cloud logging ingestion and parsing.
    """
    
    def format(self, record: logging.LogRecord) -> str:
        """
        Format a log record as a JSON line for cloud logging.
        
        Args:
            record: The log record to format
            
        Returns:
            JSON string representation of the log record
        """
        # Base log structure compatible with Google Cloud Logging
        log_entry = {
            "timestamp": datetime.fromtimestamp(record.created).isoformat(),
            "severity": record.levelname,  # Use 'severity' for Google Cloud Logging
            "logger": record.name,
            "message": record.getMessage(),
        }
        
        # Extract run_id and stage from logger name if available
        # Logger names follow pattern: projection_wizard.{logger_type}.{stage}.{run_id}
        name_parts = record.name.split('.')
        if len(name_parts) >= 4 and name_parts[0] == 'projection_wizard':
            log_entry["run_id"] = name_parts[-1]
            if len(name_parts) >= 3:
                log_entry["stage"] = name_parts[-2]
        
        # Add any extra JSON data from the log record
        if hasattr(record, 'extra_json') and isinstance(record.extra_json, dict):
            log_entry.update(record.extra_json)
        
        # Handle exceptions with proper formatting for cloud logging
        if record.exc_info:
            log_entry["exception"] = self.formatException(record.exc_info)
            log_entry["severity"] = "ERROR"  # Ensure exceptions are marked as errors
        
        return json.dumps(log_entry, ensure_ascii=False, separators=(',', ':'), default=json_serializer)


def get_logger(run_id: str, logger_name: str = 'pipeline', log_level: str = 'INFO', 
               stage: Optional[str] = None) -> logging.Logger:
    """
    Get a run-scoped logger that outputs to stdout for cloud logging capture.
    Uses JSON formatting for better integration with Google Cloud Logging.
    
    Args:
        run_id: Unique run identifier
        logger_name: Name for the logger (default: 'pipeline')
        log_level: Logging level as string (default: 'INFO')
        stage: Pipeline stage name for stage-specific logging (optional)
        
    Returns:
        Configured logger instance that outputs JSON to stdout
    """
    # Create a unique logger name that includes run_id and optionally stage
    if stage:
        full_logger_name = f"projection_wizard.{logger_name}.{stage}.{run_id}"
    else:
        full_logger_name = f"projection_wizard.{logger_name}.{run_id}"
    
    logger = logging.getLogger(full_logger_name)
    
    # Don't add handlers if logger already exists and has handlers
    if logger.handlers:
        return logger
        
    # Convert string log level to logging constant
    numeric_level = getattr(logging, log_level.upper(), logging.INFO)
    logger.setLevel(numeric_level)
    
    # Console handler for cloud logging (JSON format preferred)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(numeric_level)
    
    # Use JSON formatter for better cloud logging integration
    json_formatter = JSONFormatter()
    console_handler.setFormatter(json_formatter)
    
    logger.addHandler(console_handler)
    
    # Prevent propagation to root logger to avoid duplicate messages
    logger.propagate = False
    
    return logger


def get_structured_logger(run_id: str, logger_name: str = 'structured', log_level: str = 'INFO',
                         stage: Optional[str] = None) -> logging.Logger:
    """
    Get a run-scoped structured logger that outputs JSON to stdout for cloud logging.
    Optimized for machine parsing and cloud logging aggregation.
    
    Args:
        run_id: Unique run identifier
        logger_name: Name for the logger (default: 'structured')
        log_level: Logging level as string (default: 'INFO')
        stage: Pipeline stage name for stage-specific logging (optional)
        
    Returns:
        Configured logger instance that outputs JSON lines to stdout
    """
    # Create a unique logger name for structured logging
    if stage:
        full_logger_name = f"projection_wizard.{logger_name}.{stage}.{run_id}"
    else:
        full_logger_name = f"projection_wizard.{logger_name}.{run_id}"
    
    logger = logging.getLogger(full_logger_name)
    
    # Don't add handlers if logger already exists and has handlers
    if logger.handlers:
        return logger
        
    # Convert string log level to logging constant
    numeric_level = getattr(logging, log_level.upper(), logging.INFO)
    logger.setLevel(numeric_level)
    
    # Console handler for structured JSON output to stdout
    console_handler_structured = logging.StreamHandler(sys.stdout)
    console_handler_structured.setLevel(numeric_level)
    
    # Use JSON formatter for structured output
    json_formatter = JSONFormatter()
    console_handler_structured.setFormatter(json_formatter)
    
    logger.addHandler(console_handler_structured)
    
    # Prevent propagation to root logger
    logger.propagate = False
    
    return logger


def get_stage_logger(run_id: str, stage: str, level: int = logging.INFO) -> logging.Logger:
    """
    Get a stage-specific logger for a run that outputs to stdout for cloud logging.
    
    Args:
        run_id: Unique run identifier
        stage: Pipeline stage name
        level: Logging level (default: INFO)
        
    Returns:
        Configured logger instance with stage context
    """
    logger = get_logger(run_id, f"stage.{stage}", logging.getLevelName(level), stage=stage)
    return logger


def get_stage_structured_logger(run_id: str, stage: str, level: int = logging.INFO) -> logging.Logger:
    """
    Get a stage-specific structured logger that outputs JSON to stdout for cloud logging.
    
    Args:
        run_id: Unique run identifier
        stage: Pipeline stage name
        level: Logging level (default: INFO)
        
    Returns:
        Configured structured logger instance
    """
    logger = get_structured_logger(run_id, f"structured.{stage}", logging.getLevelName(level), stage=stage)
    return logger


def get_general_logger(run_id: str, logger_name: str = 'pipeline', level: int = logging.INFO) -> logging.Logger:
    """
    Get a general logger for a run that outputs to stdout for cloud logging.
    Useful for cross-stage logging or general pipeline operations.
    
    Args:
        run_id: Unique run identifier
        logger_name: Name for the logger (default: 'pipeline')
        level: Logging level (default: INFO)
        
    Returns:
        Configured logger instance that outputs to stdout
    """
    logger = get_logger(run_id, logger_name, logging.getLevelName(level), stage=None)
    return logger


def setup_root_logger(level: int = logging.WARNING) -> None:
    """
    Setup root logger to suppress verbose output from dependencies.
    This function remains unchanged as it's useful for cloud environments too.
    
    Args:
        level: Logging level for root logger (default: WARNING)
    """
    root_logger = logging.getLogger()
    root_logger.setLevel(level)
    
    # Suppress specific noisy loggers
    noisy_loggers = [
        'urllib3.connectionpool',
        'matplotlib',
        'PIL.PngImagePlugin',
        'pycaret'
    ]
    
    for logger_name in noisy_loggers:
        logging.getLogger(logger_name).setLevel(logging.WARNING)


def log_stage_start(logger: logging.Logger, stage: str, run_id: str) -> None:
    """Log the start of a pipeline stage."""
    logger.info(f"Starting stage '{stage}' for run {run_id}")


def log_stage_end(logger: logging.Logger, stage: str, run_id: str, 
                  duration_seconds: float) -> None:
    """Log the completion of a pipeline stage."""
    logger.info(f"Completed stage '{stage}' for run {run_id} in {duration_seconds:.2f}s")


def log_error(logger: logging.Logger, stage: str, error: Exception, run_id: str) -> None:
    """Log an error during a pipeline stage."""
    logger.error(f"Error in stage '{stage}' for run {run_id}: {str(error)}", exc_info=True)


# =============================
# STRUCTURED LOGGING HELPERS
# =============================

def log_structured_event(logger: logging.Logger, event_type: str, 
                         event_data: Optional[Dict[str, Any]] = None,
                         message: str = "") -> None:
    """
    Log a structured event with consistent schema for cloud logging.
    
    Args:
        logger: Structured logger instance
        event_type: Type of event (e.g., 'ingestion_complete', 'model_trained')
        event_data: Additional event-specific data
        message: Human-readable message (optional)
    """
    extra_json = {
        "event": event_type,
        "event_data": event_data or {}
    }
    
    logger.info(message or f"Event: {event_type}", extra={"extra_json": extra_json})


def log_structured_metric(logger: logging.Logger, metric_name: str, 
                         metric_value: Union[float, int, str],
                         metric_type: str = "performance",
                         additional_data: Optional[Dict[str, Any]] = None) -> None:
    """
    Log a structured metric for cloud monitoring and alerting.
    
    Args:
        logger: Structured logger instance
        metric_name: Name of the metric
        metric_value: Value of the metric
        metric_type: Type of metric (e.g., 'performance', 'data_quality', 'timing')
        additional_data: Additional metric context
    """
    extra_json = {
        "metric_name": metric_name,
        "metric_value": metric_value,
        "metric_type": metric_type
    }
    
    if additional_data:
        extra_json.update(additional_data)
    
    logger.info(f"Metric: {metric_name} = {metric_value}", extra={"extra_json": extra_json})


def log_structured_error(logger: logging.Logger, error_type: str, error_message: str,
                        error_context: Optional[Dict[str, Any]] = None) -> None:
    """
    Log a structured error event for cloud monitoring and alerting.
    
    Args:
        logger: Structured logger instance
        error_type: Type of error (e.g., 'validation_failed', 'model_training_error')
        error_message: Error message
        error_context: Additional error context
    """
    extra_json = {
        "error_type": error_type,
        "error_context": error_context or {}
    }
    
    logger.error(error_message, extra={"extra_json": extra_json})


# =============================
# PIPELINE SUMMARY LOGGING
# =============================

class PipelineSummaryLogger:
    """
    High-level pipeline summary logger that creates clear, concise summaries
    of pipeline execution including key metrics and transformations.
    Outputs to stdout for cloud logging integration with structured events.
    """
    
    def __init__(self, run_id: str):
        """Initialize the pipeline summary logger with cloud-native output."""
        self.run_id = run_id
        self.logger = get_general_logger(run_id, "SUMMARY")
        self.structured_logger = get_structured_logger(run_id, "pipeline_events")
        self.start_time = datetime.now()
        
    def log_pipeline_start(self, original_filename: str) -> None:
        """Log the start of the pipeline with input file information."""
        self.logger.info("="*80)
        self.logger.info("ðŸ”® PROJECTION WIZARD PIPELINE STARTED")
        self.logger.info("="*80)
        self.logger.info(f"ðŸ“ Input File: {original_filename}")
        self.logger.info(f"ðŸ†” Run ID: {self.run_id}")
        self.logger.info(f"â° Started at: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("")
        
        # Structured event
        log_structured_event(
            self.structured_logger,
            "pipeline_started",
            {
                "input_filename": original_filename,
                "started_at": self.start_time.isoformat()
            },
            f"Pipeline started with file: {original_filename}"
        )
        
    def log_ingestion_summary(self, rows: int, cols: int, filename: str, 
                             warnings: Optional[List[str]] = None) -> None:
        """Log a summary of the data ingestion stage."""
        self.logger.info("ðŸ“¥ DATA INGESTION COMPLETE")
        self.logger.info(f"   â€¢ File: {filename}")
        self.logger.info(f"   â€¢ Shape: {rows:,} rows Ã— {cols} columns")
        
        if warnings:
            self.logger.info(f"   âš ï¸  Warnings: {len(warnings)}")
            for warning in warnings:
                self.logger.info(f"     - {warning}")
        else:
            self.logger.info("   âœ… No issues detected")
        self.logger.info("")
        
        # Structured event
        log_structured_event(
            self.structured_logger,
            "ingestion_complete",
            {
                "filename": filename,
                "rows": rows,
                "columns": cols,
                "warnings": warnings or [],
                "warning_count": len(warnings) if warnings else 0
            },
            f"Data ingestion complete: {rows:,} rows Ã— {cols} columns"
        )
        
    def log_schema_summary(self, target_column: str, task_type: str, 
                          feature_count: int, categorical_count: int, 
                          numeric_count: int) -> None:
        """Log a summary of the schema definition stage."""
        self.logger.info("ðŸŽ¯ SCHEMA DEFINITION COMPLETE")
        self.logger.info(f"   â€¢ Target Column: '{target_column}' ({task_type})")
        self.logger.info(f"   â€¢ Features: {feature_count} total")
        self.logger.info(f"     - Categorical: {categorical_count}")
        self.logger.info(f"     - Numeric: {numeric_count}")
        self.logger.info("")
        
        # Structured event
        log_structured_event(
            self.structured_logger,
            "schema_definition_complete",
            {
                "target_column": target_column,
                "task_type": task_type,
                "feature_count": feature_count,
                "categorical_count": categorical_count,
                "numeric_count": numeric_count
            },
            f"Schema defined: {target_column} ({task_type}) with {feature_count} features"
        )
        
    def log_validation_summary(self, validation_results: Dict[str, Any]) -> None:
        """Log a summary of the data validation stage."""
        passed = validation_results.get('expectations_passed', 0)
        total = validation_results.get('total_expectations', 0)
        success_rate = (passed / total * 100) if total > 0 else 0
        
        self.logger.info("âœ… DATA VALIDATION COMPLETE")
        self.logger.info(f"   â€¢ Validation Score: {success_rate:.1f}% ({passed}/{total} checks passed)")
        
        # Log key data quality metrics
        quality_metrics = validation_results.get('quality_metrics', {})
        if quality_metrics:
            missing_pct = quality_metrics.get('missing_percentage', 0)
            duplicate_pct = quality_metrics.get('duplicate_percentage', 0)
            self.logger.info(f"   â€¢ Missing Values: {missing_pct:.1f}%")
            self.logger.info(f"   â€¢ Duplicate Rows: {duplicate_pct:.1f}%")
            
        warnings = validation_results.get('warnings', [])
        if warnings:
            self.logger.info(f"   âš ï¸  Data Quality Warnings: {len(warnings)}")
            for warning in warnings[:3]:  # Show first 3 warnings
                self.logger.info(f"     - {warning}")
            if len(warnings) > 3:
                self.logger.info(f"     ... and {len(warnings) - 3} more")
        self.logger.info("")
        
        # Structured event
        log_structured_event(
            self.structured_logger,
            "validation_complete",
            {
                "validation_score": success_rate,
                "expectations_passed": passed,
                "total_expectations": total,
                "quality_metrics": quality_metrics,
                "warnings": warnings,
                "warning_count": len(warnings)
            },
            f"Data validation complete: {success_rate:.1f}% score ({passed}/{total} checks passed)"
        )
        
    def log_preparation_summary(self, input_shape: tuple, output_shape: tuple,
                               cleaning_steps: List[str], encoding_steps: List[str]) -> None:
        """Log a summary of the data preparation stage."""
        self.logger.info("ðŸ”§ DATA PREPARATION COMPLETE")
        self.logger.info(f"   â€¢ Input Shape: {input_shape[0]:,} rows Ã— {input_shape[1]} columns")
        self.logger.info(f"   â€¢ Output Shape: {output_shape[0]:,} rows Ã— {output_shape[1]} columns")
        
        row_change = output_shape[0] - input_shape[0]
        col_change = output_shape[1] - input_shape[1]
        
        if row_change != 0:
            self.logger.info(f"   â€¢ Rows {'Added' if row_change > 0 else 'Removed'}: {abs(row_change):,}")
        if col_change != 0:
            self.logger.info(f"   â€¢ Columns {'Added' if col_change > 0 else 'Removed'}: {abs(col_change)}")
            
        self.logger.info(f"   â€¢ Cleaning Steps: {', '.join(cleaning_steps) if cleaning_steps else 'None'}")
        self.logger.info(f"   â€¢ Encoding Steps: {', '.join(encoding_steps) if encoding_steps else 'None'}")
        self.logger.info("")
        
        # Structured event
        log_structured_event(
            self.structured_logger,
            "preparation_complete",
            {
                "input_shape": {"rows": input_shape[0], "columns": input_shape[1]},
                "output_shape": {"rows": output_shape[0], "columns": output_shape[1]},
                "rows_changed": row_change,
                "columns_changed": col_change,
                "cleaning_steps": cleaning_steps,
                "encoding_steps": encoding_steps
            },
            f"Data preparation complete: {input_shape} â†’ {output_shape}"
        )
        
    def log_automl_summary(self, model_name: str, task_type: str, target_column: str,
                          training_shape: tuple, metrics: Dict[str, float],
                          training_duration: Optional[float] = None) -> None:
        """Log a summary of the AutoML training stage."""
        self.logger.info("ðŸ¤– MODEL TRAINING COMPLETE")
        self.logger.info(f"   â€¢ Algorithm: {model_name}")
        self.logger.info(f"   â€¢ Task Type: {task_type.title()}")
        self.logger.info(f"   â€¢ Target: '{target_column}'")
        self.logger.info(f"   â€¢ Training Data: {training_shape[0]:,} samples Ã— {training_shape[1]} features")
        
        if training_duration:
            self.logger.info(f"   â€¢ Training Time: {training_duration:.1f}s")
            
        # Log performance metrics in a clean format
        self.logger.info("   â€¢ Performance Metrics:")
        for metric_name, value in metrics.items():
            # Format different types of metrics appropriately
            if isinstance(value, float):
                if 0 <= value <= 1:
                    self.logger.info(f"     - {metric_name}: {value:.4f}")
                else:
                    self.logger.info(f"     - {metric_name}: {value:.2f}")
            else:
                self.logger.info(f"     - {metric_name}: {value}")
        self.logger.info("")
        
        # Structured event
        log_structured_event(
            self.structured_logger,
            "automl_training_complete",
            {
                "model_name": model_name,
                "task_type": task_type,
                "target_column": target_column,
                "training_shape": {"samples": training_shape[0], "features": training_shape[1]},
                "metrics": metrics,
                "training_duration": training_duration
            },
            f"AutoML training complete: {model_name} with {len(metrics)} metrics"
        )
        
    def log_explainability_summary(self, plots_generated: List[str], 
                                  top_features: Optional[List[str]] = None) -> None:
        """Log a summary of the explainability stage."""
        self.logger.info("ðŸ” MODEL EXPLAINABILITY COMPLETE")
        self.logger.info(f"   â€¢ Plots Generated: {len(plots_generated)}")
        for plot in plots_generated:
            self.logger.info(f"     - {plot}")
            
        if top_features:
            self.logger.info(f"   â€¢ Top Important Features:")
            for i, feature in enumerate(top_features[:5], 1):  # Show top 5
                self.logger.info(f"     {i}. {feature}")
        self.logger.info("")
        
        # Structured event
        log_structured_event(
            self.structured_logger,
            "explainability_complete",
            {
                "plots_generated": plots_generated,
                "plot_count": len(plots_generated),
                "top_features": top_features or [],
                "feature_count": len(top_features) if top_features else 0
            },
            f"Model explainability complete: {len(plots_generated)} plots generated"
        )
        
    def log_pipeline_completion(self, success: bool = True, error_message: Optional[str] = None) -> None:
        """Log the completion of the entire pipeline with cloud-native artifact locations."""
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        if success:
            self.logger.info("ðŸŽ‰ PIPELINE COMPLETED SUCCESSFULLY")
            self.logger.info("="*80)
            self.logger.info(f"â±ï¸  Total Duration: {duration:.1f} seconds ({duration/60:.1f} minutes)")
            self.logger.info(f"ðŸ†” Run ID: {self.run_id}")
            self.logger.info(f"ðŸ“ Run artifacts saved to GCS under prefix: runs/{self.run_id}/")
            self.logger.info(f"ðŸ“ Detailed execution logs available in Google Cloud Logging for run_id: {self.run_id}")
            self.logger.info("")
            self.logger.info("Generated Artifacts (in GCS):")
            
            # List expected artifacts in GCS
            artifacts = [
                "ðŸ“Š original_data.csv - Raw uploaded data",
                "ðŸ§¹ cleaned_data.csv - Preprocessed data ready for ML", 
                "ðŸ“‹ metadata.json - Complete pipeline metadata",
                "ðŸ¤– model/ - Trained model files",
                "ðŸ“ˆ plots/ - Explainability visualizations"
            ]
            
            for artifact in artifacts:
                self.logger.info(f"   â€¢ {artifact}")
        else:
            self.logger.info("âŒ PIPELINE FAILED")
            self.logger.info("="*80)
            self.logger.info(f"â±ï¸  Duration before failure: {duration:.1f} seconds")
            self.logger.info(f"ðŸ†” Run ID: {self.run_id}")
            self.logger.info(f"ðŸ“ Error logs available in Google Cloud Logging for run_id: {self.run_id}")
            if error_message:
                self.logger.info(f"â— Error: {error_message}")
                
        self.logger.info("="*80)
        self.logger.info("")
        
        # Structured event
        event_type = "pipeline_completed" if success else "pipeline_failed"
        event_data = {
            "success": success,
            "duration_seconds": duration,
            "completed_at": end_time.isoformat(),
            "gcs_prefix": f"runs/{self.run_id}/"
        }
        
        if error_message:
            event_data["error_message"] = error_message
            
        log_structured_event(
            self.structured_logger,
            event_type,
            event_data,
            f"Pipeline {'completed successfully' if success else 'failed'} in {duration:.1f}s"
        )


def get_pipeline_summary_logger(run_id: str) -> PipelineSummaryLogger:
    """
    Get a pipeline summary logger instance optimized for cloud logging.
    
    Args:
        run_id: Unique run identifier
        
    Returns:
        PipelineSummaryLogger instance with cloud-native output
    """
    return PipelineSummaryLogger(run_id) 