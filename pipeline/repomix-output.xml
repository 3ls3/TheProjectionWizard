This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
step_1_ingest/
  ingest_logic.py
step_2_schema/
  __init__.py
  feature_definition_logic.py
  target_definition_logic.py
step_3_validation/
  __init__.py
  ge_logic.py
  validation_runner.py
step_4_prep/
  cleaning_logic.py
  encoding_logic.py
  prep_runner.py
  profiling_logic.py
step_5_automl/
  automl_runner.py
  pycaret_logic.py
step_6_explain/
  explain_runner.py
  shap_logic.py
orchestrator.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="step_1_ingest/ingest_logic.py">
"""
Core ingestion logic for The Projection Wizard.
Handles CSV file upload, run initialization, and metadata creation.
"""

import pandas as pd
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, BinaryIO, Union
import io

# Import common modules
from common import constants, schemas, utils, storage, logger


def run_ingestion(uploaded_file_object: Union[BinaryIO, object], base_runs_path_str: str) -> str:
    """
    Process uploaded CSV file and initialize a new pipeline run.
    
    Args:
        uploaded_file_object: File object from Streamlit's st.file_uploader or similar
        base_runs_path_str: Base directory path for runs (e.g., "data/runs")
        
    Returns:
        Generated run_id string
        
    Raises:
        Exception: If critical errors occur during ingestion
    """
    # Generate run_id
    run_id = utils.generate_run_id()
    
    # Setup logging for this run
    logger_instance = logger.get_stage_logger(run_id=run_id, stage=constants.INGEST_STAGE)
    structured_log = logger.get_stage_structured_logger(run_id=run_id, stage=constants.INGEST_STAGE)
    
    logger_instance.info("Starting data ingestion process")
    
    # Structured log: Ingestion started
    logger.log_structured_event(
        structured_log,
        "ingestion_started",
        {
            "run_id": run_id,
            "stage": constants.INGEST_STAGE,
            "base_runs_path": base_runs_path_str
        },
        f"Data ingestion started for run {run_id}"
    )
    
    # Initialize variables for error handling
    initial_rows = None
    initial_cols = None 
    initial_dtypes = None
    csv_read_successful = False
    errors_list = []
    original_filename = "unknown_file.csv"
    
    try:
        # Construct and create run directory
        run_dir_path = storage.get_run_dir(run_id)
        logger_instance.info(f"Created run directory: {run_dir_path}")
        
        # Structured log: Run directory created
        logger.log_structured_event(
            structured_log,
            "run_directory_created",
            {
                "run_directory": str(run_dir_path),
                "run_id": run_id
            },
            f"Run directory created: {run_dir_path}"
        )
        
        # Save original data
        original_data_path = run_dir_path / constants.ORIGINAL_DATA_FILE
        logger_instance.info(f"Saving original data to: {original_data_path}")
        
        # Handle different file object types
        try:
            # Check if it's a Streamlit UploadedFile or similar
            if hasattr(uploaded_file_object, 'getvalue'):
                # Streamlit UploadedFile
                file_content = uploaded_file_object.getvalue()
                original_filename = getattr(uploaded_file_object, 'name', 'uploaded_file.csv')
            elif hasattr(uploaded_file_object, 'read'):
                # Standard file object
                file_content = uploaded_file_object.read()
                original_filename = getattr(uploaded_file_object, 'name', 'uploaded_file.csv')
                # Reset file pointer if possible
                if hasattr(uploaded_file_object, 'seek'):
                    uploaded_file_object.seek(0)
            else:
                raise ValueError("Unsupported file object type")
            
            # Write file content
            with open(original_data_path, 'wb') as f:
                f.write(file_content)
            
            logger_instance.info("Successfully saved original data file")
            
            # Structured log: File saved
            logger.log_structured_event(
                structured_log,
                "file_saved",
                {
                    "original_filename": original_filename,
                    "saved_path": constants.ORIGINAL_DATA_FILE,
                    "file_size_bytes": len(file_content)
                },
                f"Original data file saved: {original_filename}"
            )
            
        except Exception as e:
            error_msg = f"Failed to save original data: {str(e)}"
            logger_instance.error(error_msg)
            errors_list.append(error_msg)
            
            # Structured log: File save failed
            logger.log_structured_error(
                structured_log,
                "file_save_failed",
                error_msg,
                {"stage": constants.INGEST_STAGE, "original_filename": original_filename}
            )
            
            # Continue with limited metadata creation
            original_filename = "upload_failed.csv"
        
        # Read basic data statistics (with error handling)
        logger_instance.info("Attempting to read and analyze CSV data")
        
        try:
            # Try to read the CSV file
            df = pd.read_csv(original_data_path)
            
            # Extract basic statistics
            initial_rows = df.shape[0]
            initial_cols = df.shape[1]
            initial_dtypes = {col: str(dtype) for col, dtype in df.dtypes.items()}
            
            csv_read_successful = True
            logger_instance.info(f"Successfully read CSV: {initial_rows} rows, {initial_cols} columns")
            
            # Log basic data info
            logger_instance.info(f"Column dtypes: {initial_dtypes}")
            
            # Structured log: CSV parsed successfully
            logger.log_structured_event(
                structured_log,
                "csv_parsed_successfully",
                {
                    "rows": int(initial_rows),
                    "columns": int(initial_cols),
                    "column_names": list(df.columns),
                    "dtypes": initial_dtypes,
                    "memory_usage_mb": float(df.memory_usage(deep=True).sum() / 1024**2),
                    "missing_values_total": int(df.isnull().sum().sum())
                },
                f"CSV parsed successfully: {initial_rows} rows × {initial_cols} columns"
            )
            
            # Log data quality metrics as structured metrics (converting numpy types to Python types)
            logger.log_structured_metric(
                structured_log,
                "dataset_rows",
                int(initial_rows),
                "data_quality",
                {"dataset_columns": int(initial_cols)}
            )
            
            logger.log_structured_metric(
                structured_log,
                "dataset_columns",
                int(initial_cols),
                "data_quality",
                {"dataset_rows": int(initial_rows)}
            )
            
            total_missing = int(df.isnull().sum().sum())
            total_cells = int(initial_rows * initial_cols)
            missing_percentage = float((total_missing / total_cells) * 100)
            logger.log_structured_metric(
                structured_log,
                "missing_values_percentage",
                missing_percentage,
                "data_quality",
                {"total_missing": total_missing, "total_cells": total_cells}
            )
            
        except Exception as e:
            error_msg = f"Failed to read or parse uploaded CSV: {str(e)}"
            logger_instance.error(error_msg)
            errors_list.append(error_msg)
            csv_read_successful = False
            
            # Structured log: CSV parsing failed
            logger.log_structured_error(
                structured_log,
                "csv_parsing_failed",
                error_msg,
                {"stage": constants.INGEST_STAGE, "file": original_filename}
            )
            
            # Set fallback values
            initial_rows = None
            initial_cols = None
            initial_dtypes = None
            
        # Create initial metadata.json
        logger_instance.info("Creating initial metadata")
        
        try:
            metadata_model = schemas.BaseMetadata(
                run_id=run_id,
                timestamp=datetime.now(timezone.utc),
                original_filename=original_filename,
                initial_rows=initial_rows,
                initial_cols=initial_cols,
                initial_dtypes=initial_dtypes
            )
            
            # Convert to dictionary for JSON serialization
            metadata_dict = metadata_model.model_dump(mode='json')
            
            # Write metadata atomically
            storage.write_json_atomic(
                run_id=run_id,
                filename=constants.METADATA_FILE,
                data=metadata_dict
            )
            
            logger_instance.info("Successfully created metadata.json")
            
            # Structured log: Metadata created
            logger.log_structured_event(
                structured_log,
                "metadata_created",
                {
                    "file": constants.METADATA_FILE,
                    "run_id": run_id,
                    "has_data_stats": csv_read_successful,
                    "metadata_keys": list(metadata_dict.keys())
                },
                f"Initial metadata created: {constants.METADATA_FILE}"
            )
            
        except Exception as e:
            error_msg = f"Failed to create metadata.json: {str(e)}"
            logger_instance.error(error_msg)
            errors_list.append(error_msg)
            
            # Structured log: Metadata creation failed
            logger.log_structured_error(
                structured_log,
                "metadata_creation_failed",
                error_msg,
                {"stage": constants.INGEST_STAGE}
            )
        
        # Create initial status.json
        logger_instance.info("Creating initial status")
        
        try:
            # Determine status based on CSV read success
            if csv_read_successful and len(errors_list) == 0:
                status_val = 'completed'
                message_val = 'Ingestion successful.'
            elif csv_read_successful and len(errors_list) > 0:
                status_val = 'completed'
                message_val = 'Ingestion completed with warnings.'
            else:
                status_val = 'failed'
                message_val = 'Failed to read or parse uploaded CSV.'
            
            status_model = schemas.StageStatus(
                stage=constants.INGEST_STAGE,
                status=status_val,
                message=message_val,
                errors=errors_list if errors_list else None
            )
            
            # Convert to dictionary for JSON serialization
            status_dict = status_model.model_dump(mode='json')
            
            # Write status atomically
            storage.write_json_atomic(
                run_id=run_id,
                filename=constants.STATUS_FILE,
                data=status_dict
            )
            
            logger_instance.info(f"Successfully created status.json with status: {status_val}")
            
            # Structured log: Status created
            logger.log_structured_event(
                structured_log,
                "status_created",
                {
                    "file": constants.STATUS_FILE,
                    "status": status_val,
                    "message": message_val,
                    "errors_count": len(errors_list),
                    "csv_read_successful": csv_read_successful
                },
                f"Initial status created: {status_val}"
            )
            
        except Exception as e:
            error_msg = f"Failed to create status.json: {str(e)}"
            logger_instance.error(error_msg)
            
            # Structured log: Status creation failed
            logger.log_structured_error(
                structured_log,
                "status_creation_failed",
                error_msg,
                {"stage": constants.INGEST_STAGE}
            )
            # This is critical - we should still try to continue
        
        # Append to run index (if ingestion was successful enough)
        logger_instance.info("Adding entry to run index")
        
        try:
            # Determine final status for index
            if csv_read_successful and len(errors_list) == 0:
                index_status = "Ingestion Completed"
            elif csv_read_successful:
                index_status = "Ingestion Completed with Warnings"
            else:
                index_status = "Ingestion Failed: CSV Parse Error"
            
            # Prepare run entry data
            run_entry_data = {
                'run_id': run_id,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'original_filename': original_filename,
                'status': index_status
            }
            
            # Append to run index
            storage.append_to_run_index(run_entry_data=run_entry_data)
            
            logger_instance.info("Successfully added entry to run index")
            
            # Structured log: Run index updated
            logger.log_structured_event(
                structured_log,
                "run_index_updated",
                {
                    "run_id": run_id,
                    "index_status": index_status,
                    "original_filename": original_filename
                },
                f"Run index updated: {index_status}"
            )
            
        except Exception as e:
            error_msg = f"Failed to update run index: {str(e)}"
            logger_instance.error(error_msg)
            
            # Structured log: Run index update failed
            logger.log_structured_error(
                structured_log,
                "run_index_update_failed",
                error_msg,
                {"stage": constants.INGEST_STAGE, "run_id": run_id}
            )
            # Not critical for the run to continue
        
        # Log completion
        final_status = "successful" if csv_read_successful else "failed"
        logger_instance.info(f"Ingestion process completed with status: {final_status}")
        
        # Structured log: Ingestion completed
        logger.log_structured_event(
            structured_log,
            "ingestion_completed",
            {
                "run_id": run_id,
                "success": csv_read_successful,
                "final_status": final_status,
                "errors_count": len(errors_list),
                "data_rows": initial_rows,
                "data_columns": initial_cols,
                "original_filename": original_filename,
                "artifacts_created": [
                    constants.ORIGINAL_DATA_FILE,
                    constants.METADATA_FILE,
                    constants.STATUS_FILE
                ]
            },
            f"Ingestion completed: {final_status} for {original_filename}"
        )
        
        return run_id
        
    except Exception as e:
        # Critical error - log and re-raise
        logger_instance.error(f"Critical error during ingestion: {str(e)}", exc_info=True)
        
        # Structured log: Critical error
        logger.log_structured_error(
            structured_log,
            "critical_ingestion_error",
            f"Critical error during ingestion: {str(e)}",
            {"stage": constants.INGEST_STAGE, "run_id": run_id}
        )
        
        raise
</file>

<file path="step_2_schema/__init__.py">
"""
Step 2 Schema module for The Projection Wizard.
Contains target definition and schema confirmation logic.
"""

from .target_definition_logic import suggest_target_and_task, confirm_target_definition
from .feature_definition_logic import (
    identify_key_features,
    suggest_initial_feature_schemas,
    confirm_feature_schemas
)

__all__ = [
    'suggest_target_and_task', 
    'confirm_target_definition',
    'identify_key_features',
    'suggest_initial_feature_schemas',
    'confirm_feature_schemas'
]
</file>

<file path="step_2_schema/feature_definition_logic.py">
"""
Feature definition logic for The Projection Wizard.
Contains functions for identifying important features, suggesting data types and encoding roles,
and confirming feature schemas for the key features.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timezone
from typing import Dict, List, Optional
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression, f_regression, chi2
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder
import warnings

from common import logger, storage, constants


def _perform_minimal_stable_cleaning(df: pd.DataFrame, target_column_name: str) -> pd.DataFrame:
    """
    Internal helper function to perform very basic cleaning for stability of importance metric calculations.
    This cleaned state is temporary and NOT persisted for subsequent pipeline stages.
    
    Args:
        df: A copy of the original pandas DataFrame
        target_column_name: The name of the confirmed target column
        
    Returns:
        A pandas DataFrame that has undergone basic cleaning for metric calculation stability
    """
    df_cleaned = df.copy()  
    
    # Clean target column first
    target_series = df_cleaned[target_column_name]
    
    if pd.api.types.is_object_dtype(target_series.dtype) or pd.api.types.is_categorical_dtype(target_series.dtype):
        # Categorical target - encode numerically
        if target_series.isna().sum() > 0:
            # Fill NaN with a placeholder for encoding
            df_cleaned[target_column_name] = target_series.fillna("_MISSING_TARGET_")
        
        # Simple label encoding for target
        le = LabelEncoder()
        df_cleaned[target_column_name] = le.fit_transform(df_cleaned[target_column_name].astype(str))
        
    elif pd.api.types.is_numeric_dtype(target_series.dtype):
        # Numeric target
        if target_series.isna().sum() > 0:
            if pd.api.types.is_integer_dtype(target_series.dtype):
                # Classification likely - use mode
                fill_value = target_series.mode().iloc[0] if not target_series.mode().empty else 0
            else:
                # Regression likely - use median
                fill_value = target_series.median()
            df_cleaned[target_column_name] = target_series.fillna(fill_value)
    
    # Clean feature columns
    feature_columns = [col for col in df_cleaned.columns if col != target_column_name]
    
    for col in feature_columns:
        col_series = df_cleaned[col]
        
        if pd.api.types.is_object_dtype(col_series.dtype):
            # Try to convert to numeric first
            numeric_converted = pd.to_numeric(col_series, errors='coerce')
            if numeric_converted.notna().sum() / len(col_series) >= constants.SCHEMA_CONFIG["min_numeric_threshold"]:
                # Most values are numeric - treat as numeric
                df_cleaned[col] = numeric_converted.fillna(numeric_converted.median())
            else:
                # Treat as categorical
                df_cleaned[col] = col_series.fillna("_MISSING_")
        
        elif pd.api.types.is_numeric_dtype(col_series.dtype):
            # Already numeric - just fill NaN
            fill_value = col_series.median() if col_series.notna().sum() > 0 else 0
            df_cleaned[col] = col_series.fillna(fill_value)
        
        elif pd.api.types.is_bool_dtype(col_series.dtype):
            # Boolean - convert to int and fill NaN
            df_cleaned[col] = col_series.astype(int).fillna(0)
        
        else:
            # Other types (datetime, etc.) - convert to string for now
            df_cleaned[col] = col_series.astype(str).fillna("_MISSING_")
    
    return df_cleaned


def identify_key_features(df_original: pd.DataFrame, target_info: dict, 
                         num_features_to_surface: int = 5) -> List[str]:
    """
    Identify potentially important features using basic importance metrics.
    
    Args:
        df_original: The original pandas DataFrame (loaded from original_data.csv)
        target_info: The dictionary for the target column (name, task_type, ml_type) from metadata.json
        num_features_to_surface: How many top features to suggest (default to ~5)
        
    Returns:
        A list of top N feature column names
    """
    target_column_name = target_info['name']
    task_type = target_info['task_type']
    
    # Get feature columns (exclude target)
    feature_columns = [col for col in df_original.columns if col != target_column_name]
    
    if len(feature_columns) == 0:
        return []
    
    # Limit to available features if requested number is too high
    num_features_to_surface = min(num_features_to_surface, len(feature_columns))
    
    try:
        # Make a copy and perform minimal cleaning
        df_for_analysis = df_original.copy()
        df_cleaned = _perform_minimal_stable_cleaning(df_for_analysis, target_column_name)
        
        # Separate features and target
        X = df_cleaned[feature_columns]
        y = df_cleaned[target_column_name]
        
        # Ensure target is in proper format
        if task_type == "classification":
            y = y.astype(int)
        else:  # regression
            y = y.astype(float)
        
        # Calculate importance scores
        feature_scores = {}
        
        if task_type == "classification":
            # For classification, use mutual information
            try:
                # Prepare features for mutual info
                X_prepared = X.copy()
                
                # Encode categorical features for mutual info
                for col in X_prepared.columns:
                    if pd.api.types.is_object_dtype(X_prepared[col].dtype):
                        le = LabelEncoder()
                        X_prepared[col] = le.fit_transform(X_prepared[col].astype(str))
                
                # Calculate mutual information
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    mi_scores = mutual_info_classif(
                        X_prepared, y, 
                        random_state=constants.SCHEMA_CONFIG["mutual_info_random_state"]
                    )
                
                feature_scores = dict(zip(feature_columns, mi_scores))
                
            except Exception as e:
                # Fallback to simpler correlation-based method
                for col in feature_columns:
                    try:
                        if pd.api.types.is_numeric_dtype(X[col].dtype):
                            # For numeric features, use correlation
                            corr = abs(X[col].corr(y))
                            feature_scores[col] = corr if not pd.isna(corr) else 0.0
                        else:
                            # For categorical, use simple association measure
                            feature_scores[col] = 0.1  # Low default score
                    except:
                        feature_scores[col] = 0.0
        
        else:  # regression
            try:
                # Prepare features for f_regression
                X_numeric = X.copy()
                
                # Convert categorical to numeric for f_regression
                for col in X_numeric.columns:
                    if pd.api.types.is_object_dtype(X_numeric[col].dtype):
                        le = LabelEncoder()
                        X_numeric[col] = le.fit_transform(X_numeric[col].astype(str))
                
                # Use f_regression
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    f_scores, _ = f_regression(X_numeric, y)
                
                # Convert to positive scores (handle NaN)
                f_scores = np.nan_to_num(f_scores, nan=0.0)
                feature_scores = dict(zip(feature_columns, f_scores))
                
            except Exception as e:
                # Fallback to correlation
                for col in feature_columns:
                    try:
                        if pd.api.types.is_numeric_dtype(X[col].dtype):
                            corr = abs(X[col].corr(y))
                            feature_scores[col] = corr if not pd.isna(corr) else 0.0
                        else:
                            feature_scores[col] = 0.1  # Low default score
                    except:
                        feature_scores[col] = 0.0
        
        # Sort features by importance score (descending)
        sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)
        
        # Return top N feature names
        top_features = [feature for feature, score in sorted_features[:num_features_to_surface]]
        
        return top_features
        
    except Exception as e:
        # If all else fails, return first N features
        return feature_columns[:num_features_to_surface]


def suggest_initial_feature_schemas(df: pd.DataFrame) -> Dict[str, Dict[str, str]]:
    """
    Suggest initial data types and encoding roles for all columns based on heuristics.
    
    Args:
        df: The original pandas DataFrame
        
    Returns:
        A dictionary where keys are column names, and values are dicts like 
        {'initial_dtype': str(df[col].dtype), 'suggested_encoding_role': 'role_suggestion'}
    """
    schema_suggestions = {}
    
    for col in df.columns:
        col_series = df[col]
        initial_dtype = str(col_series.dtype)
        nunique = col_series.nunique()
        total_values = len(col_series.dropna())
        
        # Default role
        suggested_encoding_role = "numeric-continuous"
        
        # Apply heuristics
        if pd.api.types.is_bool_dtype(col_series.dtype):
            suggested_encoding_role = "boolean"
            
        elif pd.api.types.is_numeric_dtype(col_series.dtype):
            if pd.api.types.is_integer_dtype(col_series.dtype) and nunique <= 10:
                # Few unique integers - might be categorical
                suggested_encoding_role = "categorical-nominal"
            else:
                suggested_encoding_role = "numeric-continuous"
                
        elif pd.api.types.is_datetime64_any_dtype(col_series.dtype):
            suggested_encoding_role = "datetime"
            
        elif pd.api.types.is_object_dtype(col_series.dtype) or pd.api.types.is_categorical_dtype(col_series.dtype):
            # Analyze cardinality for object/categorical columns
            cardinality_ratio = nunique / total_values if total_values > 0 else 0
            
            # Check for identifier patterns
            col_lower = col.lower()
            if any(id_keyword in col_lower for id_keyword in ['id', 'uuid', 'key', 'index']):
                suggested_encoding_role = "text"  # Treat IDs as text to be ignored/hashed
            elif nunique <= 2:
                suggested_encoding_role = "categorical-nominal"
            elif nunique <= constants.SCHEMA_CONFIG["max_categorical_cardinality"]:
                suggested_encoding_role = "categorical-nominal"
            elif cardinality_ratio > 0.8:  # High cardinality
                suggested_encoding_role = "text"
            else:
                # Medium cardinality - could be ordinal
                suggested_encoding_role = "categorical-nominal"
        else:
            # Unknown types
            suggested_encoding_role = "text"
        
        schema_suggestions[col] = {
            'initial_dtype': initial_dtype,
            'suggested_encoding_role': suggested_encoding_role
        }
    
    return schema_suggestions


def confirm_feature_schemas(run_id: str, user_confirmed_schemas: Dict[str, Dict[str, str]], 
                           all_initial_schemas: Dict[str, Dict[str, str]]) -> bool:
    """
    Confirm feature schemas and update metadata.json with feature schema information.
    
    Args:
        run_id: The ID of the current run
        user_confirmed_schemas: Dictionary where keys are column names (only for columns the user 
                               explicitly reviewed/changed) and values are dicts: 
                               {'final_dtype': str, 'final_encoding_role': str}
        all_initial_schemas: The full dictionary of initial suggestions for all columns 
                            (from suggest_initial_feature_schemas)
        
    Returns:
        True if successful, False otherwise
    """
    # Get loggers
    run_logger = logger.get_stage_logger(run_id, constants.SCHEMA_STAGE)
    structured_log = logger.get_stage_structured_logger(run_id, constants.SCHEMA_STAGE)
    
    try:
        # Structured log: Feature schema confirmation started
        logger.log_structured_event(
            structured_log,
            "feature_schema_confirmation_started",
            {
                "total_columns": len(all_initial_schemas),
                "user_confirmed_count": len(user_confirmed_schemas),
                "system_default_count": len(all_initial_schemas) - len(user_confirmed_schemas)
            },
            f"Feature schema confirmation started for {len(all_initial_schemas)} columns"
        )
        
        # Read existing metadata.json
        metadata_dict = storage.read_metadata(run_id)
        if metadata_dict is None:
            error_msg = f"Could not read metadata.json for run {run_id}"
            run_logger.error(error_msg)
            logger.log_structured_error(
                structured_log,
                "metadata_load_failed",
                error_msg,
                {"stage": constants.SCHEMA_STAGE}
            )
            return False
        
        # Construct final feature schemas
        final_feature_schemas = {}
        
        for column_name in all_initial_schemas.keys():
            if column_name in user_confirmed_schemas:
                # User explicitly confirmed/changed this column
                final_dtype = user_confirmed_schemas[column_name]['final_dtype']
                final_encoding_role = user_confirmed_schemas[column_name]['final_encoding_role']
                source = 'user_confirmed'
            else:
                # Use system defaults for non-reviewed columns
                final_dtype = all_initial_schemas[column_name]['initial_dtype']
                final_encoding_role = all_initial_schemas[column_name]['suggested_encoding_role']
                source = 'system_defaulted'
            
            final_feature_schemas[column_name] = {
                'dtype': final_dtype,
                'encoding_role': final_encoding_role,
                'source': source
            }
        
        # Analyze feature schema distribution for structured logging
        encoding_role_counts = {}
        source_counts = {"user_confirmed": 0, "system_defaulted": 0}
        
        for schema_info in final_feature_schemas.values():
            role = schema_info['encoding_role']
            source = schema_info['source']
            
            encoding_role_counts[role] = encoding_role_counts.get(role, 0) + 1
            source_counts[source] += 1
        
        # Structured log: Feature schemas processed
        logger.log_structured_event(
            structured_log,
            "feature_schemas_processed",
            {
                "total_features": len(final_feature_schemas),
                "encoding_role_distribution": encoding_role_counts,
                "source_distribution": source_counts,
                "user_confirmed_count": source_counts["user_confirmed"],
                "system_defaulted_count": source_counts["system_defaulted"]
            },
            f"Feature schemas processed: {len(final_feature_schemas)} columns configured"
        )
        
        # Update metadata with feature schemas
        metadata_dict['feature_schemas'] = final_feature_schemas
        metadata_dict['feature_schemas_confirmed_at'] = datetime.now(timezone.utc).isoformat()
        
        # Write updated metadata.json
        storage.write_metadata(run_id, metadata_dict)
        
        # Structured log: Metadata updated
        logger.log_structured_event(
            structured_log,
            "metadata_updated",
            {
                "feature_schemas_count": len(final_feature_schemas),
                "metadata_keys_added": ["feature_schemas", "feature_schemas_confirmed_at"]
            },
            f"Metadata updated with {len(final_feature_schemas)} feature schemas"
        )
        
        # Update status.json
        status_data = {
            'stage': constants.SCHEMA_STAGE,
            'status': 'completed',
            'message': f"Feature schemas confirmed for {len(final_feature_schemas)} columns.",
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'errors': []
        }
        
        storage.write_status(run_id, status_data)
        
        # Structured log: Status updated
        logger.log_structured_event(
            structured_log,
            "status_updated",
            {
                "status": "completed",
                "stage": constants.SCHEMA_STAGE,
                "feature_count": len(final_feature_schemas)
            },
            "Feature schema confirmation status updated to completed"
        )
        
        run_logger.info(f"Feature schemas confirmed for {len(final_feature_schemas)} columns. "
                       f"User confirmed: {len(user_confirmed_schemas)}, "
                       f"System defaulted: {len(final_feature_schemas) - len(user_confirmed_schemas)}")
        
        # Structured log: Feature schema confirmation completed
        logger.log_structured_event(
            structured_log,
            "feature_schema_confirmation_completed",
            {
                "success": True,
                "total_columns": len(final_feature_schemas),
                "user_confirmed": len(user_confirmed_schemas),
                "system_defaulted": len(final_feature_schemas) - len(user_confirmed_schemas),
                "encoding_roles": list(encoding_role_counts.keys())
            },
            f"Feature schema confirmation completed successfully for {len(final_feature_schemas)} columns"
        )
        
        return True
        
    except Exception as e:
        error_msg = f"Failed to confirm feature schemas: {str(e)}"
        run_logger.error(error_msg)
        
        # Structured log: Feature schema confirmation failed
        logger.log_structured_error(
            structured_log,
            "feature_schema_confirmation_failed",
            error_msg,
            {
                "stage": constants.SCHEMA_STAGE,
                "total_columns": len(all_initial_schemas),
                "user_confirmed_count": len(user_confirmed_schemas)
            }
        )
        
        # Update status.json with error
        try:
            status_data = {
                'stage': constants.SCHEMA_STAGE,
                'status': 'failed',
                'message': f"Failed to confirm feature schemas: {str(e)}",
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'errors': [str(e)]
            }
            storage.write_status(run_id, status_data)
        except Exception as status_error:
            run_logger.error(f"Failed to update status.json with error: {str(status_error)}")
        
        return False
</file>

<file path="step_2_schema/target_definition_logic.py">
"""
Target definition logic for The Projection Wizard.
Contains functions for suggesting and confirming target column and task type.
"""

import pandas as pd
from datetime import datetime, timezone
from typing import Tuple, Optional
import re

from common import logger, storage, constants


def suggest_target_and_task(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    """
    Suggest target column and task type based on DataFrame analysis.
    
    Args:
        df: The pandas DataFrame loaded from original_data.csv
        
    Returns:
        Tuple containing:
        - suggested_target_column_name (str | None): The name of the column most likely to be the target
        - suggested_task_type (str | None): Suggested task, e.g., "classification" or "regression"
        - suggested_target_ml_type (str | None): Suggested ML-ready type/format for the target
    """
    if df.empty or len(df.columns) == 0:
        return None, None, None
    
    # Target column name heuristics
    target_keywords = ['target', 'label', 'class', 'outcome', 'output', 'result', 'y', 'response']
    suggested_target_column = None
    
    # Look for columns with target-like names (case insensitive)
    for col in df.columns:
        col_lower = col.lower()
        for keyword in target_keywords:
            if keyword in col_lower:
                suggested_target_column = col
                break
        if suggested_target_column:
            break
    
    # If no obvious name match, consider the last column
    if suggested_target_column is None:
        suggested_target_column = df.columns[-1]
    
    # If still no suitable candidate (shouldn't happen), return None
    if suggested_target_column is None:
        return None, None, None
    
    # Analyze the suggested target column to determine task type and ML type
    target_series = df[suggested_target_column]
    target_dtype = target_series.dtype
    unique_values = target_series.nunique()
    total_values = len(target_series.dropna())  # Exclude NaN values
    
    suggested_task_type = None
    suggested_target_ml_type = None
    
    # Check for boolean dtype first (pandas treats bool as numeric)
    if pd.api.types.is_bool_dtype(target_dtype):
        suggested_task_type = "classification"
        suggested_target_ml_type = "binary_boolean"
    elif pd.api.types.is_numeric_dtype(target_dtype):
        # Numeric target
        if unique_values <= 10 and target_series.dropna().dtype == int:
            # Few unique integer values - likely classification
            suggested_task_type = "classification"
            if unique_values == 2:
                # Check if binary (0,1) or similar
                unique_vals = sorted(target_series.dropna().unique())
                if len(unique_vals) == 2 and unique_vals[0] == 0 and unique_vals[1] == 1:
                    suggested_target_ml_type = "binary_01"
                else:
                    suggested_target_ml_type = "binary_numeric"
            else:
                suggested_target_ml_type = "multiclass_int_labels"
        else:
            # Many unique values or float - likely regression
            suggested_task_type = "regression"
            suggested_target_ml_type = "numeric_continuous"
    
    elif pd.api.types.is_object_dtype(target_dtype) or pd.api.types.is_categorical_dtype(target_dtype):
        # Categorical/text target
        if unique_values == 2:
            suggested_task_type = "classification"
            suggested_target_ml_type = "binary_text_labels"
        elif 2 < unique_values <= constants.SCHEMA_CONFIG["max_categorical_cardinality"]:
            suggested_task_type = "classification"
            suggested_target_ml_type = "multiclass_text_labels"
        else:
            # Very high cardinality - might not be suitable as target
            # But still suggest classification as it's the only option for text
            suggested_task_type = "classification"
            suggested_target_ml_type = "high_cardinality_text"
    
    else:
        # Other dtypes (datetime, etc.)
        # Default to classification for unknown types
        suggested_task_type = "classification"
        suggested_target_ml_type = "unknown_type"
    
    return suggested_target_column, suggested_task_type, suggested_target_ml_type


def confirm_target_definition(run_id: str, confirmed_target_column: str, 
                            confirmed_task_type: str, confirmed_target_ml_type: str) -> bool:
    """
    Confirm target definition and update metadata.json with target information.
    
    Args:
        run_id: The ID of the current run
        confirmed_target_column: The target column name selected by the user
        confirmed_task_type: The task type (e.g., "classification", "regression") selected by the user
        confirmed_target_ml_type: The ML-ready type/format for the target selected by the user
        
    Returns:
        True if successful, False otherwise
    """
    # Get loggers
    run_logger = logger.get_stage_logger(run_id, constants.SCHEMA_STAGE)
    structured_log = logger.get_stage_structured_logger(run_id, constants.SCHEMA_STAGE)
    
    # Validate inputs
    if confirmed_task_type not in constants.TASK_TYPES:
        error_msg = f"Invalid task type: {confirmed_task_type}. Must be one of {constants.TASK_TYPES}"
        run_logger.error(error_msg)
        logger.log_structured_error(
            structured_log,
            "invalid_task_type",
            error_msg,
            {
                "provided_task_type": confirmed_task_type,
                "valid_task_types": constants.TASK_TYPES,
                "stage": constants.SCHEMA_STAGE
            }
        )
        return False
        
    if confirmed_target_ml_type not in constants.TARGET_ML_TYPES:
        error_msg = f"Invalid target ML type: {confirmed_target_ml_type}. Must be one of {constants.TARGET_ML_TYPES}"
        run_logger.error(error_msg)
        logger.log_structured_error(
            structured_log,
            "invalid_target_ml_type",
            error_msg,
            {
                "provided_ml_type": confirmed_target_ml_type,
                "valid_ml_types": constants.TARGET_ML_TYPES,
                "stage": constants.SCHEMA_STAGE
            }
        )
        return False
    
    try:
        # Structured log: Target definition started
        logger.log_structured_event(
            structured_log,
            "target_definition_started",
            {
                "target_column": confirmed_target_column,
                "task_type": confirmed_task_type,
                "ml_type": confirmed_target_ml_type
            },
            f"Target definition confirmation started for column '{confirmed_target_column}'"
        )
        
        # Read existing metadata.json
        metadata_dict = storage.read_metadata(run_id)
        if metadata_dict is None:
            error_msg = f"Could not read metadata.json for run {run_id}"
            run_logger.error(error_msg)
            logger.log_structured_error(
                structured_log,
                "metadata_load_failed",
                error_msg,
                {"stage": constants.SCHEMA_STAGE}
            )
            return False
        
        # Update metadata dictionary with target information
        metadata_dict['target_info'] = {
            'name': confirmed_target_column,
            'task_type': confirmed_task_type,
            'ml_type': confirmed_target_ml_type,
            'user_confirmed_at': datetime.now(timezone.utc).isoformat()
        }
        
        # Optionally update top-level task_type for convenience
        metadata_dict['task_type'] = confirmed_task_type
        
        # Write updated metadata.json
        storage.write_metadata(run_id, metadata_dict)
        
        # Structured log: Target definition completed
        logger.log_structured_event(
            structured_log,
            "target_definition_completed",
            {
                "target_column": confirmed_target_column,
                "task_type": confirmed_task_type,
                "ml_type": confirmed_target_ml_type,
                "metadata_updated": True
            },
            f"Target definition completed: {confirmed_target_column} ({confirmed_task_type})"
        )
        
        # Update status.json
        status_data = {
            'stage': constants.SCHEMA_STAGE,
            'status': 'completed',
            'message': f"Target '{confirmed_target_column}' and task '{confirmed_task_type}' confirmed.",
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'errors': []
        }
        
        storage.write_status(run_id, status_data)
        
        # Structured log: Status updated
        logger.log_structured_event(
            structured_log,
            "status_updated",
            {
                "status": "completed",
                "stage": constants.SCHEMA_STAGE,
                "message": f"Target '{confirmed_target_column}' confirmed"
            },
            "Target definition status updated to completed"
        )
        
        run_logger.info(f"Target definition confirmed: column='{confirmed_target_column}', "
                       f"task='{confirmed_task_type}', ml_type='{confirmed_target_ml_type}'")
        
        return True
        
    except Exception as e:
        error_msg = f"Failed to confirm target definition: {str(e)}"
        run_logger.error(error_msg)
        
        # Structured log: Target definition failed
        logger.log_structured_error(
            structured_log,
            "target_definition_failed",
            error_msg,
            {
                "stage": constants.SCHEMA_STAGE,
                "target_column": confirmed_target_column,
                "task_type": confirmed_task_type
            }
        )
        
        # Update status.json with error
        try:
            status_data = {
                'stage': constants.SCHEMA_STAGE,
                'status': 'failed',
                'message': f"Failed to confirm target definition: {str(e)}",
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'errors': [str(e)]
            }
            storage.write_status(run_id, status_data)
        except Exception as status_error:
            run_logger.error(f"Failed to update status.json with error: {str(status_error)}")
        
        return False
</file>

<file path="step_3_validation/__init__.py">
"""
Step 3 Validation module for The Projection Wizard.
Contains data validation logic using Great Expectations.
"""

from .validation_runner import run_validation_stage, get_validation_summary, check_validation_status
from .ge_logic import generate_ge_suite_from_metadata, run_ge_validation_on_dataframe

__all__ = [
    'run_validation_stage',
    'get_validation_summary', 
    'check_validation_status',
    'generate_ge_suite_from_metadata',
    'run_ge_validation_on_dataframe'
]
</file>

<file path="step_3_validation/ge_logic.py">
"""
Great Expectations logic for The Projection Wizard.
Contains functions to generate expectation suites and run validation.
"""

import pandas as pd
import great_expectations as gx
from great_expectations.expectations.expectation_configuration import ExpectationConfiguration
from typing import Dict, List, Optional, Any
import warnings
import logging

# Import project modules
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from common import logger, constants
from common.schemas import FeatureSchemaInfo, TargetInfo


def _map_dtype_to_ge_type(dtype_str: str) -> str:
    """
    Map pandas/Pydantic dtype strings to Great Expectations type strings.
    
    Args:
        dtype_str: The dtype string (e.g., 'int64', 'float64', 'object', 'bool')
        
    Returns:
        Great Expectations compatible type string
    """
    dtype_mapping = {
        # Integer types
        'int8': 'int',
        'int16': 'int',
        'int32': 'int',
        'int64': 'int',
        'Int8': 'int',
        'Int16': 'int',
        'Int32': 'int',
        'Int64': 'int',
        'integer': 'int',
        
        # Float types
        'float16': 'float',
        'float32': 'float',
        'float64': 'float',
        'Float32': 'float',
        'Float64': 'float',
        'float': 'float',
        
        # String types
        'object': 'str',
        'string': 'str',
        'str': 'str',
        
        # Boolean types
        'bool': 'bool',
        'boolean': 'bool',
        
        # Datetime types
        'datetime64[ns]': 'datetime',
        'datetime': 'datetime',
        'timestamp': 'datetime',
        
        # Category types
        'category': 'str'  # Treat categories as strings for GE
    }
    
    # Normalize dtype string
    normalized_dtype = dtype_str.lower().strip()
    
    # Direct mapping
    if normalized_dtype in dtype_mapping:
        return dtype_mapping[normalized_dtype]
    
    # Partial matching for complex types
    if 'int' in normalized_dtype:
        return 'int'
    elif 'float' in normalized_dtype:
        return 'float'
    elif 'datetime' in normalized_dtype:
        return 'datetime'
    elif 'bool' in normalized_dtype:
        return 'bool'
    else:
        # Default to string for unknown types
        return 'str'


def _get_target_value_expectations(target_info: TargetInfo, column_name: str) -> List[Dict[str, Any]]:
    """
    Generate target-specific expectations based on ML type.
    
    Args:
        target_info: TargetInfo object with target metadata
        column_name: Name of the target column
        
    Returns:
        List of expectation dictionaries for the target column
    """
    expectations = []
    
    ml_type = target_info.ml_type
    
    if ml_type == "binary_01":
        # Binary classification with 0/1 values
        expectations.append({
            "expectation_type": "expect_column_values_to_be_in_set",
            "kwargs": {
                "column": column_name,
                "value_set": [0, 1]
            }
        })
    elif ml_type == "binary_boolean":
        # Binary classification with boolean values
        expectations.append({
            "expectation_type": "expect_column_values_to_be_in_type_list",
            "kwargs": {
                "column": column_name,
                "type_list": ["bool", "boolean"]
            }
        })
    elif ml_type == "binary_numeric":
        # Binary classification with numeric values (could be 0/1 or other pairs)
        expectations.append({
            "expectation_type": "expect_column_distinct_values_to_equal_set",
            "kwargs": {
                "column": column_name,
                "value_set": None  # Will be determined from data
            }
        })
        # Add constraint that there should be exactly 2 unique values
        expectations.append({
            "expectation_type": "expect_column_unique_value_count_to_be_between",
            "kwargs": {
                "column": column_name,
                "min_value": 2,
                "max_value": 2
            }
        })
    elif ml_type in ["multiclass_int_labels", "multiclass_text_labels", "high_cardinality_text"]:
        # Multiclass classification
        expectations.append({
            "expectation_type": "expect_column_unique_value_count_to_be_between",
            "kwargs": {
                "column": column_name,
                "min_value": 3,  # At least 3 classes for multiclass
                "max_value": None  # No upper limit
            }
        })
    elif ml_type == "numeric_continuous":
        # Regression target
        expectations.append({
            "expectation_type": "expect_column_values_to_be_of_type",
            "kwargs": {
                "column": column_name,
                "type_": "float"
            }
        })
    
    return expectations


def generate_ge_suite_from_metadata(
    feature_schemas: Dict[str, FeatureSchemaInfo], 
    target_info: Optional[TargetInfo], 
    df_columns: List[str],
    run_id: str = "unknown"
) -> dict:
    """
    Generate a Great Expectations suite based on user-confirmed schema metadata.
    
    Args:
        feature_schemas: Dictionary mapping column names to FeatureSchemaInfo objects
        target_info: TargetInfo object with target column metadata (optional)
        df_columns: Actual list of columns from the DataFrame being validated
        run_id: Run ID for naming the expectation suite
        
    Returns:
        Dictionary representing a Great Expectations expectation suite
    """
    # Initialize logger for this function
    run_logger = logger.get_stage_logger(run_id, constants.VALIDATION_STAGE)
    
    expectations = []
    
    # Table-level expectations
    # Ensure the table has the expected columns in order
    expectations.append({
        "expectation_type": "expect_table_columns_to_match_ordered_list",
        "kwargs": {
            "column_list": df_columns
        }
    })
    
    # Ensure table has at least 1 row
    expectations.append({
        "expectation_type": "expect_table_row_count_to_be_between",
        "kwargs": {
            "min_value": 1,
            "max_value": None
        }
    })
    
    # Column-level expectations
    for col_name in df_columns:
        # Basic existence check
        expectations.append({
            "expectation_type": "expect_column_to_exist",
            "kwargs": {
                "column": col_name
            }
        })
        
        # Get schema info for this column
        if isinstance(feature_schemas.get(col_name), dict):
            # Handle dictionary format
            schema_info_dict = feature_schemas[col_name]
            schema_dtype = schema_info_dict.get('dtype', 'object')
            schema_encoding_role = schema_info_dict.get('encoding_role', 'text')
        elif hasattr(feature_schemas.get(col_name), 'dtype'):
            # Handle FeatureSchemaInfo object
            schema_info = feature_schemas[col_name]
            schema_dtype = schema_info.dtype
            schema_encoding_role = schema_info.encoding_role
        else:
            # Column not found in feature schemas - log warning and skip type-specific expectations
            run_logger.warning(f"Column '{col_name}' not found in feature_schemas. Skipping type-specific expectations.")
            continue
        
        # Map dtype to GE type
        ge_type = _map_dtype_to_ge_type(schema_dtype)
        
        # Add type expectation
        expectations.append({
            "expectation_type": "expect_column_values_to_be_of_type",
            "kwargs": {
                "column": col_name,
                "type_": ge_type
            }
        })
        
        # Add null value expectation (allow up to 30% missing values by default)
        missing_threshold = constants.VALIDATION_CONFIG.get("missing_value_threshold", 0.3)
        expectations.append({
            "expectation_type": "expect_column_values_to_not_be_null",
            "kwargs": {
                "column": col_name,
                "mostly": 1.0 - missing_threshold  # 70% of values should not be null
            }
        })
        
        # Encoding role-specific expectations
        if schema_encoding_role == "categorical-nominal" or schema_encoding_role == "categorical-ordinal":
            # For categorical columns, expect reasonable cardinality
            cardinality_threshold = constants.VALIDATION_CONFIG.get("cardinality_threshold", 50)
            expectations.append({
                "expectation_type": "expect_column_unique_value_count_to_be_between",
                "kwargs": {
                    "column": col_name,
                    "min_value": 1,
                    "max_value": cardinality_threshold
                }
            })
        
        elif schema_encoding_role == "boolean":
            # For boolean columns, expect exactly 2 unique values (or 3 with nulls)
            expectations.append({
                "expectation_type": "expect_column_unique_value_count_to_be_between",
                "kwargs": {
                    "column": col_name,
                    "min_value": 2,
                    "max_value": 3  # Allow for nulls
                }
            })
        
        elif schema_encoding_role in ["numeric-continuous", "numeric-discrete"]:
            # For numeric columns, add range checks (will be determined from data)
            if ge_type in ["int", "float"]:
                expectations.append({
                    "expectation_type": "expect_column_values_to_be_between",
                    "kwargs": {
                        "column": col_name,
                        "min_value": None,  # Will be determined from data during validation
                        "max_value": None,
                        "mostly": 0.95  # Allow for 5% outliers
                    }
                })
    
    # Target-specific expectations
    if target_info and target_info.name in df_columns:
        target_expectations = _get_target_value_expectations(target_info, target_info.name)
        expectations.extend(target_expectations)
    
    # Construct the full GE suite dictionary
    ge_suite = {
        "expectation_suite_name": f"run_{run_id}_validation_suite",
        "ge_cloud_id": None,
        "expectations": expectations,
        "data_asset_type": "Dataset",
        "meta": {
            "great_expectations_version": gx.__version__,
            "created_by": "projection_wizard_step_3_validation",
            "run_id": run_id,
            "total_expectations": len(expectations)
        }
    }
    
    run_logger.info(f"Generated GE suite with {len(expectations)} expectations for {len(df_columns)} columns")
    
    return ge_suite


def run_ge_validation_on_dataframe(df: pd.DataFrame, ge_suite: dict, run_id: str = "unknown") -> dict:
    """
    Run Great Expectations validation on a DataFrame using the provided suite.
    
    Args:
        df: The pandas DataFrame to validate
        ge_suite: The Great Expectations suite dictionary
        run_id: Run ID for logging purposes
        
    Returns:
        Dictionary representing the Great Expectations validation results
    """
    # Initialize logger for this function
    run_logger = logger.get_stage_logger(run_id, constants.VALIDATION_STAGE)
    
    try:
        # Suppress Great Expectations warnings for cleaner output
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning, module="great_expectations")
            
            # Create a Great Expectations dataset from the pandas DataFrame
            # Using the from_pandas method which creates a PandasDataset with the suite attached
            run_logger.info(f"Creating GE dataset from DataFrame with shape {df.shape}")
            
            # Use the most compatible approach - direct validation on dataframe
            run_logger.info("Running Great Expectations validation...")
            
            # Create a simple context if needed
            try:
                context = gx.get_context()
            except:
                context = gx.data_context.BaseDataContext()
            
            # Build individual expectation results
            validation_results = {
                "success": True,
                "statistics": {
                    "evaluated_expectations": 0,
                    "successful_expectations": 0,
                    "unsuccessful_expectations": 0,
                    "success_percent": 100.0
                },
                "results": [],
                "meta": {
                    "great_expectations_version": gx.__version__,
                    "run_id": run_id
                }
            }
            
            # Process each expectation manually for better compatibility
            for expectation_dict in ge_suite.get("expectations", []):
                expectation_type = expectation_dict.get("expectation_type")
                kwargs = expectation_dict.get("kwargs", {})
                
                validation_results["statistics"]["evaluated_expectations"] += 1
                
                # Run basic validations manually for key expectation types
                result = {"success": True, "expectation_config": expectation_dict}
                
                try:
                    if expectation_type == "expect_table_columns_to_match_ordered_list":
                        expected_columns = kwargs.get("column_list", [])
                        actual_columns = list(df.columns)
                        result["success"] = actual_columns == expected_columns
                    
                    elif expectation_type == "expect_table_row_count_to_be_between":
                        min_val = kwargs.get("min_value", 0)
                        max_val = kwargs.get("max_value")
                        row_count = len(df)
                        result["success"] = (row_count >= min_val and (max_val is None or row_count <= max_val))
                    
                    elif expectation_type == "expect_column_to_exist":
                        column = kwargs.get("column")
                        result["success"] = column in df.columns
                    
                    elif expectation_type == "expect_column_values_to_be_of_type":
                        column = kwargs.get("column")
                        expected_type = kwargs.get("type_")
                        if column in df.columns:
                            # Basic type checking
                            if expected_type == "int":
                                result["success"] = df[column].dtype.kind in ['i']
                            elif expected_type == "float":
                                result["success"] = df[column].dtype.kind in ['f', 'i']
                            elif expected_type == "str":
                                result["success"] = df[column].dtype.kind in ['O', 'U', 'S']
                            else:
                                result["success"] = True
                        else:
                            result["success"] = False
                    
                    elif expectation_type == "expect_column_values_to_not_be_null":
                        column = kwargs.get("column")
                        mostly = kwargs.get("mostly", 1.0)
                        if column in df.columns:
                            null_pct = df[column].isnull().mean()
                            result["success"] = (1.0 - null_pct) >= mostly
                        else:
                            result["success"] = False
                    
                    else:
                        # For other expectation types, assume success for now
                        result["success"] = True
                        
                except Exception as e:
                    run_logger.warning(f"Error validating {expectation_type}: {str(e)}")
                    result["success"] = False
                
                # Update statistics - ensure boolean values
                is_success = result["success"] in [True, "True", 1, "1"]
                result["success"] = is_success  # Normalize to boolean
                
                if is_success:
                    validation_results["statistics"]["successful_expectations"] += 1
                else:
                    validation_results["statistics"]["unsuccessful_expectations"] += 1
                    validation_results["success"] = False
                
                validation_results["results"].append(result)
            
            # Calculate success percentage
            total = validation_results["statistics"]["evaluated_expectations"]
            successful = validation_results["statistics"]["successful_expectations"]
            if total > 0:
                validation_results["statistics"]["success_percent"] = (successful / total) * 100.0
            

            
            # Results are already in dictionary format
            results_dict = validation_results
            
            # Log summary statistics
            success_count = results_dict.get("statistics", {}).get("successful_expectations", 0)
            total_count = results_dict.get("statistics", {}).get("evaluated_expectations", 0)
            success_percentage = (success_count / total_count * 100) if total_count > 0 else 0
            
            run_logger.info(f"Validation completed: {success_count}/{total_count} expectations passed ({success_percentage:.1f}%)")
            
            # Add metadata to results
            results_dict["meta"] = {
                **results_dict.get("meta", {}),
                "validation_run_id": run_id,
                "dataframe_shape": list(df.shape),
                "validation_timestamp": pd.Timestamp.now().isoformat()
            }
            
            return results_dict
            
    except Exception as e:
        error_msg = f"Failed to run GE validation: {str(e)}"
        run_logger.error(error_msg)
        
        # Return error result in GE format
        return {
            "success": False,
            "results": [],
            "statistics": {
                "evaluated_expectations": 0,
                "successful_expectations": 0,
                "unsuccessful_expectations": 0,
                "success_percent": 0.0
            },
            "meta": {
                "validation_run_id": run_id,
                "validation_timestamp": pd.Timestamp.now().isoformat(),
                "error": error_msg
            }
        }
</file>

<file path="step_3_validation/validation_runner.py">
"""
Data validation runner for The Projection Wizard.
Orchestrates the validation process using Great Expectations.
"""

import pandas as pd
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, Optional
import sys
sys.path.append(str(Path(__file__).parent.parent))

from common import storage, logger, constants
from .ge_logic import generate_ge_suite_from_metadata, run_ge_validation_on_dataframe


def run_validation_stage(run_id: str) -> bool:
    """
    Run the data validation stage according to the specification.
    
    Args:
        run_id: The ID of the run to validate
        
    Returns:
        True if successful, False otherwise
    """
    # Get loggers
    logger_instance = logger.get_stage_logger(run_id, constants.VALIDATION_STAGE)
    structured_log = logger.get_stage_structured_logger(run_id, constants.VALIDATION_STAGE)
    
    # Track timing
    start_time = datetime.now()
    
    try:
        logger_instance.info(f"Starting validation stage for run {run_id}")
        
        # Structured log: Stage started
        logger.log_structured_event(
            structured_log,
            "stage_started",
            {"stage": constants.VALIDATION_STAGE},
            "Validation stage started"
        )
        
        # Load metadata.json
        metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
        if metadata_dict is None:
            logger_instance.error(f"Could not read metadata.json for run {run_id}")
            logger.log_structured_error(
                structured_log,
                "metadata_load_failed",
                f"Could not read metadata.json for run {run_id}",
                {"stage": constants.VALIDATION_STAGE}
            )
            return False
        
        # Extract feature_schemas and target_info
        feature_schemas = metadata_dict.get('feature_schemas', {})
        target_info_dict = metadata_dict.get('target_info')
        
        # Convert target_info dictionary to TargetInfo object if it exists
        target_info = None
        if target_info_dict:
            from common.schemas import TargetInfo
            target_info = TargetInfo(**target_info_dict)
        
        # Convert feature_schemas dictionaries to FeatureSchemaInfo objects
        converted_feature_schemas = {}
        if feature_schemas:
            from common.schemas import FeatureSchemaInfo
            for col_name, schema_dict in feature_schemas.items():
                converted_feature_schemas[col_name] = FeatureSchemaInfo(**schema_dict)
        
        if not converted_feature_schemas:
            logger_instance.error("Feature schemas not found in metadata. Please complete schema confirmation first.")
            logger.log_structured_error(
                structured_log,
                "feature_schemas_missing",
                "Feature schemas not found in metadata",
                {"stage": constants.VALIDATION_STAGE}
            )
            return False
        
        logger_instance.info(f"Found feature schemas for {len(converted_feature_schemas)} columns")
        
        # Structured log: Metadata loaded
        logger.log_structured_event(
            structured_log,
            "metadata_loaded",
            {
                "feature_schemas_count": len(converted_feature_schemas),
                "has_target_info": target_info is not None,
                "target_column": target_info.name if target_info else None
            },
            f"Metadata loaded: {len(converted_feature_schemas)} feature schemas"
        )
        
        # Load original_data.csv
        df = pd.read_csv(storage.get_run_dir(run_id) / constants.ORIGINAL_DATA_FILENAME)
        logger_instance.info(f"Loaded original data with shape {df.shape}")
        
        # Structured log: Data loaded
        logger.log_structured_event(
            structured_log,
            "data_loaded",
            {
                "data_shape": {"rows": df.shape[0], "columns": df.shape[1]},
                "file": constants.ORIGINAL_DATA_FILENAME
            },
            f"Original data loaded: {df.shape}"
        )
        
        # Generate GE Suite
        ge_suite_dict = generate_ge_suite_from_metadata(
            converted_feature_schemas, 
            target_info, 
            list(df.columns),
            run_id
        )
        logger_instance.info(f"Generated GE suite with {len(ge_suite_dict.get('expectations', []))} expectations")
        
        # Structured log: GE suite generated
        expectations_count = len(ge_suite_dict.get('expectations', []))
        logger.log_structured_event(
            structured_log,
            "ge_suite_generated",
            {
                "expectations_count": expectations_count,
                "suite_name": ge_suite_dict.get('expectation_suite_name', 'unknown')
            },
            f"Generated Great Expectations suite with {expectations_count} expectations"
        )
        
        # Run GE Validation
        ge_results_dict = run_ge_validation_on_dataframe(df, ge_suite_dict, run_id)
        logger_instance.info("Completed GE validation")
        
        # Extract key validation metrics
        overall_success = ge_results_dict.get("success", False)
        stats = ge_results_dict.get("statistics", {})
        total_expectations = stats.get("evaluated_expectations", 0)
        successful_expectations = stats.get("successful_expectations", 0)
        failed_expectations = stats.get("unsuccessful_expectations", 0)
        success_rate = (successful_expectations / total_expectations * 100) if total_expectations > 0 else 0
        
        # Structured log: Validation completed
        logger.log_structured_event(
            structured_log,
            "validation_completed",
            {
                "overall_success": overall_success,
                "total_expectations": total_expectations,
                "successful_expectations": successful_expectations,
                "failed_expectations": failed_expectations,
                "success_rate": success_rate,
                "runtime_seconds": ge_results_dict.get("meta", {}).get("run_time", 0)
            },
            f"Validation completed: {success_rate:.1f}% success rate ({successful_expectations}/{total_expectations})"
        )
        
        # Log individual validation metrics
        logger.log_structured_metric(
            structured_log,
            "validation_success_rate",
            success_rate,
            "data_quality",
            {"total_expectations": total_expectations, "successful": successful_expectations}
        )
        
        logger.log_structured_metric(
            structured_log,
            "expectations_evaluated",
            total_expectations,
            "data_quality",
            {"successful": successful_expectations, "failed": failed_expectations}
        )
        
        # Prepare validation.json Content
        validation_summary = {
            "overall_success": overall_success,
            "total_expectations": total_expectations,
            "successful_expectations": successful_expectations,
            "failed_expectations": failed_expectations,
            "run_time_s": ge_results_dict.get("meta", {}).get("run_time", 0),  # Path might vary
            "ge_version": ge_results_dict.get("meta", {}).get("great_expectations_version", "unknown"),
            "results_ge_native": ge_results_dict  # Store the full raw GE result
        }
        
        # Save validation.json
        storage.write_json_atomic(run_id, constants.VALIDATION_FILENAME, validation_summary)
        logger_instance.info("Saved validation.json")
        
        # Structured log: Validation results saved
        logger.log_structured_event(
            structured_log,
            "validation_results_saved",
            {
                "file": constants.VALIDATION_FILENAME,
                "overall_success": overall_success
            },
            f"Validation results saved to {constants.VALIDATION_FILENAME}"
        )
        
        # Update metadata.json
        validation_info = {
            'passed': validation_summary['overall_success'], 
            'report_filename': constants.VALIDATION_FILENAME,
            'total_expectations_evaluated': validation_summary['total_expectations'],
            'successful_expectations': validation_summary['successful_expectations']
        }
        metadata_dict['validation_info'] = validation_info
        storage.write_metadata(run_id, metadata_dict)
        logger_instance.info("Updated metadata.json with validation info")
        
        # Structured log: Metadata updated
        logger.log_structured_event(
            structured_log,
            "metadata_updated",
            {
                "validation_info_keys": list(validation_info.keys()),
                "validation_passed": validation_info['passed']
            },
            "Metadata updated with validation results"
        )
        
        # Update status.json - Fail pipeline if validation success rate is too low
        validation_success_threshold = constants.VALIDATION_CONFIG.get("pipeline_failure_threshold", 0.90)  # 90% threshold for pipeline continuation
        success_rate_decimal = successful_expectations / total_expectations if total_expectations > 0 else 0.0
        
        if success_rate_decimal < validation_success_threshold:
            # Validation failed - stop pipeline execution
            status_value = 'failed'
            failure_reasons = []
            failure_reasons.append(f"Validation success rate {success_rate:.1f}% is below required threshold {validation_success_threshold*100:.1f}%")
            failure_reasons.append(f"Failed expectations: {failed_expectations}/{total_expectations}")
            
            # Add critical failure details if available
            ge_results = ge_results_dict.get('results', [])
            critical_failures = []
            for result in ge_results:
                if not result.get('success', True):
                    expectation_config = result.get('expectation_config', {})
                    expectation_type = expectation_config.get('expectation_type', 'unknown')
                    column = expectation_config.get('kwargs', {}).get('column', 'table_level')
                    
                    # Identify critical failures that warrant pipeline stoppage
                    if expectation_type in [
                        "expect_column_to_exist",
                        "expect_table_columns_to_match_ordered_list", 
                        "expect_column_values_to_be_of_type"
                    ]:
                        critical_failures.append(f"{expectation_type} failed for {column}")
            
            if critical_failures:
                failure_reasons.extend(critical_failures[:5])  # Limit to first 5 critical failures
                
            message = f"Pipeline stopped: Validation failed with {failed_expectations} critical issues"
            
        elif not validation_summary['overall_success']:
            # Validation had issues but passed threshold - continue with warning
            status_value = 'completed'
            message = f"Validation completed with warnings: {failed_expectations} non-critical issues found"
        else:
            # Validation fully passed
            status_value = 'completed' 
            message = "Validation checks completed successfully"
        
        status_data = {
            'stage': constants.VALIDATION_STAGE,
            'status': status_value,
            'message': message,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'errors': failure_reasons if status_value == 'failed' else []
        }
        storage.write_status(run_id, status_data)
        logger_instance.info(f"Updated status.json with status: {status_value}")
        
        # Structured log: Status updated
        logger.log_structured_event(
            structured_log,
            "status_updated",
            {
                "status": status_value,
                "stage": constants.VALIDATION_STAGE
            },
            f"Status updated to {status_value}"
        )
        
        # Calculate stage duration
        end_time = datetime.now()
        stage_duration = (end_time - start_time).total_seconds()
        
        # Structured log: Stage completed
        logger.log_structured_event(
            structured_log,
            "stage_completed",
            {
                "stage": constants.VALIDATION_STAGE,
                "success": True,
                "duration_seconds": stage_duration,
                "validation_success_rate": success_rate,
                "completed_at": end_time.isoformat()
            },
            f"Validation stage completed successfully in {stage_duration:.1f}s"
        )
        
        # Log overall result
        if status_value == 'failed':
            logger_instance.error(f"❌ Pipeline STOPPED: Validation failed with {failed_expectations}/{total_expectations} expectations ({success_rate:.1f}%)")
            return False  # Signal pipeline failure
        elif validation_summary['overall_success']:
            logger_instance.info(f"✅ Validation PASSED: {validation_summary['successful_expectations']}/{validation_summary['total_expectations']} expectations")
        else:
            logger_instance.warning(f"⚠️ Validation completed with warnings: {validation_summary['successful_expectations']}/{validation_summary['total_expectations']} expectations passed ({success_rate:.1f}%)")
        
        return True
        
    except Exception as e:
        error_msg = f"Validation stage failed: {str(e)}"
        logger_instance.error(error_msg)
        
        # Structured log: Stage failed
        logger.log_structured_error(
            structured_log,
            "stage_failed",
            error_msg,
            {"stage": constants.VALIDATION_STAGE}
        )
        
        # Update status.json with error
        try:
            status_data = {
                'stage': constants.VALIDATION_STAGE,
                'status': 'failed',
                'message': error_msg,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'errors': [str(e)]
            }
            storage.write_status(run_id, status_data)
        except Exception as status_error:
            logger_instance.error(f"Failed to update status.json with error: {str(status_error)}")
        
        return False


def _extract_failed_expectations(validation_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    Extract a summary of failed expectations from validation results.
    
    Args:
        validation_results: Full GE validation results dictionary
        
    Returns:
        Dictionary summarizing failed expectations
    """
    failed_summary = {
        "total_failed": 0,
        "by_column": {},
        "by_expectation_type": {},
        "critical_failures": []
    }
    
    results = validation_results.get("results", [])
    
    for result in results:
        if not result.get("success", True):
            failed_summary["total_failed"] += 1
            
            # Extract expectation details
            expectation_type = result.get("expectation_config", {}).get("expectation_type", "unknown")
            column = result.get("expectation_config", {}).get("kwargs", {}).get("column", "table_level")
            
            # Count by column
            if column not in failed_summary["by_column"]:
                failed_summary["by_column"][column] = 0
            failed_summary["by_column"][column] += 1
            
            # Count by expectation type
            if expectation_type not in failed_summary["by_expectation_type"]:
                failed_summary["by_expectation_type"][expectation_type] = 0
            failed_summary["by_expectation_type"][expectation_type] += 1
            
            # Identify critical failures (data type mismatches, missing columns, etc.)
            if expectation_type in [
                "expect_column_to_exist",
                "expect_table_columns_to_match_ordered_list",
                "expect_column_values_to_be_of_type"
            ]:
                failed_summary["critical_failures"].append({
                    "expectation_type": expectation_type,
                    "column": column,
                    "result": result.get("result", {})
                })
    
    return failed_summary


def get_validation_summary(run_id: str) -> Optional[Dict[str, Any]]:
    """
    Get a summary of validation results for a run.
    
    Args:
        run_id: The ID of the run
        
    Returns:
        Dictionary with validation summary or None if not found
    """
    try:
        metadata_dict = storage.read_metadata(run_id)
        if metadata_dict and 'validation_summary' in metadata_dict:
            return metadata_dict['validation_summary']
        return None
    except Exception:
        return None


def check_validation_status(run_id: str) -> str:
    """
    Check the validation status for a run.
    
    Args:
        run_id: The ID of the run
        
    Returns:
        Status string: 'not_started', 'passed', 'failed', 'completed_with_warnings', or 'error'
    """
    try:
        validation_summary = get_validation_summary(run_id)
        if validation_summary is None:
            return 'not_started'
        
        if validation_summary.get('validation_passed', False):
            return 'passed'
        else:
            return 'completed_with_warnings'
            
    except Exception:
        return 'error'
</file>

<file path="step_4_prep/cleaning_logic.py">
"""
Data cleaning logic for The Projection Wizard.
Contains functions for data cleaning operations in the prep stage.
"""

import pandas as pd
from typing import Dict, List, Tuple, Optional

from common import logger, schemas


def clean_data(df_original: pd.DataFrame, 
               feature_schemas: Dict[str, schemas.FeatureSchemaInfo], 
               target_info: schemas.TargetInfo,
               cleaning_config: Optional[dict] = None) -> Tuple[pd.DataFrame, List[str]]:
    """
    Clean the original DataFrame based on feature schemas and target information.
    
    Args:
        df_original: The raw DataFrame loaded from original_data.csv
        feature_schemas: Dictionary of FeatureSchemaInfo objects (converted from dicts from metadata.json)
        target_info: TargetInfo object with target column information
        cleaning_config: Optional dictionary for future cleaning strategy configurations
        
    Returns:
        Tuple containing:
        - cleaned_dataframe: The cleaned DataFrame
        - cleaning_steps_performed: List of strings describing cleaning steps taken
    """
    # Create a copy to avoid modifying original data
    df_cleaned = df_original.copy()
    cleaning_steps_performed = []
    
    # Track initial state
    initial_rows = len(df_cleaned)
    initial_cols = len(df_cleaned.columns)
    
    cleaning_steps_performed.append(f"Starting data cleaning with {initial_rows} rows and {initial_cols} columns")
    
    # Step 1: Handle Missing Values
    _handle_missing_values(df_cleaned, feature_schemas, target_info, cleaning_steps_performed)
    
    # Step 2: Remove Duplicates
    _remove_duplicates(df_cleaned, cleaning_steps_performed)
    
    # Log final state
    final_rows = len(df_cleaned)
    final_cols = len(df_cleaned.columns)
    cleaning_steps_performed.append(f"Cleaning completed with {final_rows} rows and {final_cols} columns")
    
    return df_cleaned, cleaning_steps_performed


def _handle_missing_values(df: pd.DataFrame, 
                          feature_schemas: Dict[str, schemas.FeatureSchemaInfo],
                          target_info: schemas.TargetInfo,
                          steps_log: List[str]) -> None:
    """
    Handle missing values based on encoding roles from feature schemas.
    
    Args:
        df: DataFrame to clean (modified in place)
        feature_schemas: Dictionary of FeatureSchemaInfo objects
        target_info: TargetInfo object with target information
        steps_log: List to append cleaning steps to
    """
    steps_log.append("Starting missing value imputation")
    
    # Track columns that have missing values before cleaning
    columns_with_missing = []
    for col in df.columns:
        missing_count = df[col].isnull().sum()
        if missing_count > 0:
            columns_with_missing.append((col, missing_count))
    
    if not columns_with_missing:
        steps_log.append("No missing values found in dataset")
        return
    
    steps_log.append(f"Found missing values in {len(columns_with_missing)} columns")
    
    # Process each column that has missing values
    for col, missing_count in columns_with_missing:
        # Get the encoding role for this column
        if col in feature_schemas:
            encoding_role = feature_schemas[col].encoding_role
        elif col == target_info.name:
            # For target column, use the target's ML type to determine strategy
            encoding_role = _get_encoding_role_for_target(target_info)
        else:
            # Fallback - shouldn't happen in well-formed data, but handle gracefully
            encoding_role = _infer_encoding_role_fallback(df[col])
            steps_log.append(f"Warning: Column '{col}' not found in feature schemas, inferred role: {encoding_role}")
        
        # Apply appropriate imputation strategy based on encoding role
        if encoding_role in ["numeric-continuous", "numeric-discrete"]:
            # Impute with median for numeric columns
            median_value = df[col].median()
            if pd.isna(median_value):
                # If median is NaN (all values are NaN), fill with 0
                median_value = 0
            df[col].fillna(median_value, inplace=True)
            steps_log.append(f"Imputed {missing_count} NaNs with median ({median_value}) for numeric column: {col}")
            
        elif encoding_role in ["categorical-nominal", "categorical-ordinal", "text"]:
            # Impute with mode for categorical/text columns, or use "_UNKNOWN_" if no mode exists
            mode_values = df[col].mode()
            if len(mode_values) > 0:
                mode_value = mode_values.iloc[0]
                df[col].fillna(mode_value, inplace=True)
                steps_log.append(f"Imputed {missing_count} NaNs with mode ('{mode_value}') for categorical column: {col}")
            else:
                # No mode exists (all values are NaN), use placeholder
                df[col].fillna("_UNKNOWN_", inplace=True)
                steps_log.append(f"Imputed {missing_count} NaNs with '_UNKNOWN_' for categorical column: {col}")
                
        elif encoding_role == "boolean":
            # Impute with mode for boolean columns
            mode_values = df[col].mode()
            if len(mode_values) > 0:
                mode_value = mode_values.iloc[0]
                df[col].fillna(mode_value, inplace=True)
                steps_log.append(f"Imputed {missing_count} NaNs with mode ({mode_value}) for boolean column: {col}")
            else:
                # Default to False if no mode exists
                df[col].fillna(False, inplace=True)
                steps_log.append(f"Imputed {missing_count} NaNs with False for boolean column: {col}")
                
        elif encoding_role == "datetime":
            # For datetime columns, we'll forward fill or use a placeholder date
            # This is a simple strategy - could be enhanced in the future
            if not df[col].dropna().empty:
                # Forward fill if we have some valid dates (using newer pandas syntax)
                df[col] = df[col].ffill().bfill()
                steps_log.append(f"Imputed {missing_count} NaNs with forward/backward fill for datetime column: {col}")
            else:
                # All values are NaN - use a placeholder
                placeholder_date = pd.Timestamp('1900-01-01')
                df[col].fillna(placeholder_date, inplace=True)
                steps_log.append(f"Imputed {missing_count} NaNs with placeholder date for datetime column: {col}")
                
        else:
            # Unknown encoding role - use a conservative approach
            if pd.api.types.is_numeric_dtype(df[col]):
                median_value = df[col].median()
                if pd.isna(median_value):
                    median_value = 0
                df[col].fillna(median_value, inplace=True)
                steps_log.append(f"Imputed {missing_count} NaNs with median for unknown-role numeric column: {col}")
            else:
                df[col].fillna("_UNKNOWN_", inplace=True)
                steps_log.append(f"Imputed {missing_count} NaNs with '_UNKNOWN_' for unknown-role non-numeric column: {col}")


def _remove_duplicates(df: pd.DataFrame, steps_log: List[str]) -> None:
    """
    Remove duplicate rows from the DataFrame.
    
    Args:
        df: DataFrame to clean (modified in place)
        steps_log: List to append cleaning steps to
    """
    initial_rows = len(df)
    
    # Remove duplicates (keep first occurrence)
    df.drop_duplicates(inplace=True)
    
    rows_removed = initial_rows - len(df)
    if rows_removed > 0:
        steps_log.append(f"Removed {rows_removed} duplicate rows")
    else:
        steps_log.append("No duplicate rows found")


def _get_encoding_role_for_target(target_info: schemas.TargetInfo) -> str:
    """
    Get appropriate encoding role for target column based on its ML type.
    
    Args:
        target_info: TargetInfo object
        
    Returns:
        Appropriate encoding role string
    """
    ml_type = target_info.ml_type
    
    if ml_type in ["binary_01", "binary_numeric", "numeric_continuous"]:
        return "numeric-continuous"
    elif ml_type in ["binary_boolean"]:
        return "boolean"
    elif ml_type in ["binary_text_labels", "multiclass_text_labels", "high_cardinality_text"]:
        return "categorical-nominal"
    elif ml_type in ["multiclass_int_labels"]:
        return "categorical-ordinal"
    else:
        # Default fallback
        return "categorical-nominal"


def _infer_encoding_role_fallback(series: pd.Series) -> str:
    """
    Fallback method to infer encoding role when not found in schemas.
    
    Args:
        series: Pandas Series to analyze
        
    Returns:
        Inferred encoding role string
    """
    if pd.api.types.is_bool_dtype(series):
        return "boolean"
    elif pd.api.types.is_numeric_dtype(series):
        return "numeric-continuous"
    elif pd.api.types.is_datetime64_any_dtype(series):
        return "datetime"
    else:
        return "categorical-nominal"
</file>

<file path="step_4_prep/encoding_logic.py">
"""
Feature encoding logic for The Projection Wizard.
Contains functions for converting cleaned data into ML-ready formats.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import joblib
import warnings

# sklearn imports
from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder
from sklearn.feature_extraction.text import TfidfVectorizer

from common import logger, storage, constants, schemas


def encode_features(df_cleaned: pd.DataFrame, 
                   feature_schemas: Dict[str, schemas.FeatureSchemaInfo], 
                   target_info: schemas.TargetInfo,
                   run_id: str) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Encode features for ML based on feature schemas and target information.
    
    Args:
        df_cleaned: The DataFrame after cleaning from cleaning_logic.py
        feature_schemas: Dictionary of FeatureSchemaInfo objects
        target_info: TargetInfo object with target information
        run_id: Current run ID, used for saving encoders/scalers
        
    Returns:
        Tuple containing:
        - encoded_dataframe: The DataFrame with ML-ready encoded features
        - encoders_scalers_info: Dictionary with paths and info about saved encoders/scalers
    """
    # Create a copy to avoid modifying the input
    df_encoded = df_cleaned.copy()
    encoders_scalers_info = {}
    
    # Get logger for this run
    run_logger = logger.get_stage_logger(run_id, constants.PREP_STAGE)
    
    # Create model artifacts directory
    model_artefacts_dir = storage.get_run_dir(run_id) / constants.MODEL_DIR
    model_artefacts_dir.mkdir(exist_ok=True)
    
    run_logger.info(f"Starting feature encoding for {len(df_encoded.columns)} columns")
    
    # Step 1: Handle Target Variable Encoding
    df_encoded, target_encoder_info = _encode_target_variable(
        df_encoded, target_info, model_artefacts_dir, run_logger
    )
    if target_encoder_info:
        encoders_scalers_info.update(target_encoder_info)
    
    # Step 2: Handle Feature Encoding (non-target columns)
    df_encoded, feature_encoder_info = _encode_features(
        df_encoded, feature_schemas, target_info, model_artefacts_dir, run_logger
    )
    encoders_scalers_info.update(feature_encoder_info)
    
    run_logger.info(f"Encoding completed. Final shape: {df_encoded.shape}")
    run_logger.info(f"Saved {len(encoders_scalers_info)} encoders/scalers")
    
    return df_encoded, encoders_scalers_info


def _encode_target_variable(df: pd.DataFrame, 
                          target_info: schemas.TargetInfo,
                          model_artefacts_dir: Path,
                          logger_instance) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Encode the target variable based on its ML type.
    
    Args:
        df: DataFrame to modify (modified in place)
        target_info: TargetInfo object
        model_artefacts_dir: Directory to save encoders
        logger_instance: Logger instance
        
    Returns:
        Tuple of (modified DataFrame, encoder info dict)
    """
    target_col = target_info.name
    ml_type = target_info.ml_type
    encoder_info = {}
    
    if target_col not in df.columns:
        logger_instance.warning(f"Target column '{target_col}' not found in DataFrame")
        return df, encoder_info
        
    logger_instance.info(f"Encoding target column '{target_col}' with ML type '{ml_type}'")
    
    if ml_type == "binary_01":
        # Ensure column is int 0/1
        unique_vals = df[target_col].unique()
        if set(unique_vals) == {0, 1} or set(unique_vals) == {0.0, 1.0}:
            df[target_col] = df[target_col].astype(int)
            logger_instance.info(f"Target '{target_col}' already in 0/1 format, converted to int")
        elif set(unique_vals) == {True, False}:
            df[target_col] = df[target_col].astype(int)
            logger_instance.info(f"Target '{target_col}' converted from boolean to 0/1")
        else:
            # Map other values to 0/1 (take first unique as 0, second as 1)
            sorted_vals = sorted(unique_vals)
            mapping = {sorted_vals[0]: 0, sorted_vals[1]: 1}
            df[target_col] = df[target_col].map(mapping)
            logger_instance.info(f"Target '{target_col}' mapped to 0/1: {mapping}")
            
    elif ml_type == "multiclass_int_labels":
        # Ensure column is int
        df[target_col] = df[target_col].astype(int)
        logger_instance.info(f"Target '{target_col}' converted to int labels")
        
    elif ml_type in ["binary_text_labels", "multiclass_text_labels"]:
        # Use LabelEncoder
        le = LabelEncoder()
        df[target_col] = le.fit_transform(df[target_col])
        
        # Save the encoder
        encoder_path = model_artefacts_dir / f"{target_col}_label_encoder.joblib"
        joblib.dump(le, encoder_path)
        
        encoder_info[f"{target_col}_label_encoder"] = {
            "type": "LabelEncoder",
            "path": str(encoder_path),
            "classes": le.classes_.tolist(),
            "column": target_col
        }
        
        logger_instance.info(f"Target '{target_col}' encoded with LabelEncoder, classes: {le.classes_}")
        
    elif ml_type == "numeric_continuous":
        # Ensure it's float
        df[target_col] = df[target_col].astype(float)
        logger_instance.info(f"Target '{target_col}' converted to float")
        
    else:
        logger_instance.warning(f"Unknown target ML type '{ml_type}', leaving as-is")
    
    return df, encoder_info


def _encode_features(df: pd.DataFrame,
                   feature_schemas: Dict[str, schemas.FeatureSchemaInfo],
                   target_info: schemas.TargetInfo,
                   model_artefacts_dir: Path,
                   logger_instance) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Encode feature columns based on their encoding roles.
    
    Args:
        df: DataFrame to modify
        feature_schemas: Dictionary of feature schema info
        target_info: Target information
        model_artefacts_dir: Directory to save encoders
        logger_instance: Logger instance
        
    Returns:
        Tuple of (modified DataFrame, encoders info dict)
    """
    encoders_info = {}
    columns_to_drop = []
    
    # Process each feature column (excluding target)
    for col in df.columns:
        if col == target_info.name:
            continue  # Skip target column
            
        if col not in feature_schemas:
            logger_instance.warning(f"Column '{col}' not found in feature schemas, skipping encoding")
            continue
            
        encoding_role = feature_schemas[col].encoding_role
        logger_instance.info(f"Encoding column '{col}' with role '{encoding_role}'")
        
        if encoding_role in ["numeric-continuous", "numeric-discrete"]:
            # Apply StandardScaler
            scaler = StandardScaler()
            df[col] = scaler.fit_transform(df[[col]]).flatten()
            
            # Save scaler
            scaler_path = model_artefacts_dir / f"{col}_scaler.joblib"
            joblib.dump(scaler, scaler_path)
            
            encoders_info[f"{col}_scaler"] = {
                "type": "StandardScaler",
                "path": str(scaler_path),
                "column": col,
                "mean": scaler.mean_[0],
                "scale": scaler.scale_[0]
            }
            
            logger_instance.info(f"Applied StandardScaler to '{col}' (mean={scaler.mean_[0]:.3f}, scale={scaler.scale_[0]:.3f})")
            
        elif encoding_role == "categorical-nominal":
            # Use pd.get_dummies for one-hot encoding
            original_cols = set(df.columns)
            df = pd.get_dummies(df, columns=[col], prefix=col, dummy_na=False)
            new_cols = list(set(df.columns) - original_cols)
            
            encoders_info[f"{col}_onehot"] = {
                "type": "OneHotEncoder",
                "original_column": col,
                "new_columns": new_cols,
                "n_categories": len(new_cols)
            }
            
            logger_instance.info(f"Applied one-hot encoding to '{col}', created {len(new_cols)} new columns")
            
        elif encoding_role == "categorical-ordinal":
            # Handle ordinal encoding - for MVP, use simple integer codes
            # In future, could add ordinal_order to FeatureSchemaInfo
            if pd.api.types.is_categorical_dtype(df[col]):
                df[col] = df[col].cat.codes
            else:
                # Convert to categorical first, then to codes
                df[col] = pd.Categorical(df[col]).codes
                
            encoders_info[f"{col}_ordinal"] = {
                "type": "OrdinalEncoder_Simple",
                "column": col,
                "method": "categorical_codes"
            }
            
            logger_instance.info(f"Applied simple ordinal encoding to '{col}' using categorical codes")
            
        elif encoding_role == "boolean":
            # Ensure 0/1 or True/False
            unique_vals = df[col].unique()
            if set(unique_vals) <= {True, False}:
                df[col] = df[col].astype(int)
                logger_instance.info(f"Boolean column '{col}' converted to 0/1")
            elif set(unique_vals) <= {0, 1, 0.0, 1.0}:
                df[col] = df[col].astype(int)
                logger_instance.info(f"Boolean column '{col}' ensured as int 0/1")
            else:
                # Handle text boolean values
                df[col] = _encode_text_boolean(df[col])
                logger_instance.info(f"Text boolean column '{col}' converted to 0/1")
                
        elif encoding_role == "datetime":
            # Extract datetime features
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                df[f'{col}_year'] = df[col].dt.year
                df[f'{col}_month'] = df[col].dt.month
                df[f'{col}_day'] = df[col].dt.day
                df[f'{col}_dayofweek'] = df[col].dt.dayofweek
                
                new_features = [f'{col}_year', f'{col}_month', f'{col}_day', f'{col}_dayofweek']
                columns_to_drop.append(col)
                
                encoders_info[f"{col}_datetime"] = {
                    "type": "DatetimeFeatureExtractor",
                    "original_column": col,
                    "new_features": new_features
                }
                
                logger_instance.info(f"Extracted datetime features from '{col}': {new_features}")
            else:
                logger_instance.warning(f"Column '{col}' marked as datetime but not datetime type")
                
        elif encoding_role == "text":
            # Apply TF-IDF vectorization
            vectorizer = TfidfVectorizer(max_features=50, stop_words='english')
            
            # Ensure text data is string type and handle NaN
            text_data = df[col].fillna('').astype(str)
            
            try:
                tfidf_matrix = vectorizer.fit_transform(text_data)
                
                # Create new columns for TF-IDF features
                feature_names = [f"{col}_tfidf_{i}" for i in range(tfidf_matrix.shape[1])]
                tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=df.index)
                
                # Add TF-IDF columns to main dataframe
                df = pd.concat([df, tfidf_df], axis=1)
                columns_to_drop.append(col)
                
                # Save vectorizer
                vectorizer_path = model_artefacts_dir / f"{col}_tfidf_vectorizer.joblib"
                joblib.dump(vectorizer, vectorizer_path)
                
                encoders_info[f"{col}_tfidf"] = {
                    "type": "TfidfVectorizer",
                    "path": str(vectorizer_path),
                    "original_column": col,
                    "new_features": feature_names,
                    "n_features": len(feature_names),
                    "max_features": 50
                }
                
                logger_instance.info(f"Applied TF-IDF to '{col}', created {len(feature_names)} features")
                
            except Exception as e:
                logger_instance.error(f"Failed to apply TF-IDF to '{col}': {str(e)}")
                # Keep original column if TF-IDF fails
                
        elif encoding_role == "identifier_ignore":
            # Drop column
            columns_to_drop.append(col)
            logger_instance.info(f"Dropping identifier column '{col}'")
            
        else:
            logger_instance.warning(f"Unknown encoding role '{encoding_role}' for column '{col}', leaving as-is")
    
    # Drop columns that were replaced
    if columns_to_drop:
        df = df.drop(columns=columns_to_drop)
        logger_instance.info(f"Dropped {len(columns_to_drop)} original columns: {columns_to_drop}")
    
    return df, encoders_info


def _encode_text_boolean(series: pd.Series) -> pd.Series:
    """
    Convert text boolean values to 0/1.
    
    Args:
        series: Pandas Series with text boolean values
        
    Returns:
        Series with 0/1 values
    """
    # Common boolean text patterns
    true_values = {'yes', 'y', 'true', '1', 'on', 'enabled'}
    false_values = {'no', 'n', 'false', '0', 'off', 'disabled'}
    
    # Convert to lowercase string for comparison
    lower_series = series.astype(str).str.lower()
    
    result = pd.Series(index=series.index, dtype=int)
    result[lower_series.isin(true_values)] = 1
    result[lower_series.isin(false_values)] = 0
    
    # For any values not matching patterns, use the original logic
    # (map first unique to 0, second unique to 1)
    unmapped = result.isna()
    if unmapped.any():
        unique_vals = series[unmapped].unique()
        if len(unique_vals) >= 2:
            mapping = {unique_vals[0]: 0, unique_vals[1]: 1}
            result[unmapped] = series[unmapped].map(mapping)
        else:
            result[unmapped] = 0  # Default to 0
    
    return result
</file>

<file path="step_4_prep/prep_runner.py">
"""
Prep Stage Runner for The Projection Wizard.
Orchestrates the complete data preparation stage including cleaning, encoding, and profiling.
"""

import pandas as pd
from pathlib import Path
from typing import Optional
from datetime import datetime

from common import logger, storage, constants, schemas
from . import cleaning_logic, encoding_logic, profiling_logic


def run_preparation_stage(run_id: str) -> bool:
    """
    Execute the complete data preparation stage for a given run.
    
    This function orchestrates:
    1. Loading inputs (metadata.json, original_data.csv)
    2. Data cleaning (missing values, duplicates)
    3. Feature encoding (ML-ready transformations)
    4. Data profiling (ydata-profiling report)
    5. Saving outputs and updating metadata
    
    Args:
        run_id: Unique run identifier
        
    Returns:
        True if stage completes successfully, False otherwise
    """
    # Get loggers for this run and stage
    log = logger.get_stage_logger(run_id, constants.PREP_STAGE)
    structured_log = logger.get_stage_structured_logger(run_id, constants.PREP_STAGE)
    
    # Track timing
    start_time = datetime.now()
    
    try:
        # Log stage start
        log.info(f"Starting data preparation stage for run {run_id}")
        
        # Structured log: Stage started
        logger.log_structured_event(
            structured_log,
            "stage_started",
            {"stage": constants.PREP_STAGE},
            "Data preparation stage started"
        )
        
        # Check if validation stage failed - prevent prep from running
        try:
            status_data = storage.read_json(run_id, constants.STATUS_FILENAME)
            if status_data and status_data.get('stage') == constants.VALIDATION_STAGE:
                if status_data.get('status') == 'failed':
                    error_msg = "Cannot run data preparation: validation stage failed"
                    validation_errors = status_data.get('errors', [])
                    if validation_errors:
                        error_msg += f" - Reasons: {'; '.join(validation_errors[:3])}"
                    
                    log.error(error_msg)
                    logger.log_structured_error(
                        structured_log,
                        "validation_failed_prerequisite",
                        error_msg,
                        {"stage": constants.PREP_STAGE, "validation_errors": validation_errors}
                    )
                    _update_status_failed(run_id, error_msg)
                    return False
        except Exception as e:
            log.warning(f"Could not check validation status: {e}")
            # Continue execution - don't fail on status check errors
        
        # Update status to running
        try:
            status_data = {
                "stage": constants.PREP_STAGE,
                "status": "running",
                "message": "Data preparation in progress..."
            }
            storage.write_json_atomic(run_id, constants.STATUS_FILENAME, status_data)
        except Exception as e:
            log.warning(f"Could not update status to running: {e}")
        
        # =============================
        # 1. LOAD INPUTS
        # =============================
        log.info("Loading inputs: metadata and original data")
        
        # Load metadata.json
        try:
            metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
        except Exception as e:
            log.error(f"Failed to load metadata.json: {e}")
            logger.log_structured_error(
                structured_log,
                "metadata_load_failed",
                f"Failed to load metadata.json: {str(e)}",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, f"Failed to load metadata: {str(e)}")
            return False
        
        if not metadata_dict:
            log.error("metadata.json is empty or invalid")
            logger.log_structured_error(
                structured_log,
                "metadata_empty",
                "metadata.json is empty or invalid",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, "Empty or invalid metadata")
            return False
        
        # Convert target_info dict to Pydantic object (Critical: Task 5 learnings)
        target_info_dict = metadata_dict.get('target_info')
        if not target_info_dict:
            log.error("No target_info found in metadata")
            logger.log_structured_error(
                structured_log,
                "target_info_missing",
                "No target_info found in metadata",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, "Missing target information in metadata")
            return False
        
        try:
            target_info = schemas.TargetInfo(**target_info_dict)
            log.info(f"Loaded target info: column='{target_info.name}', ml_type='{target_info.ml_type}'")
        except Exception as e:
            log.error(f"Failed to parse target_info: {e}")
            logger.log_structured_error(
                structured_log,
                "target_info_parse_failed",
                f"Failed to parse target_info: {str(e)}",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, f"Invalid target info format: {str(e)}")
            return False
        
        # Convert feature_schemas dict to Pydantic objects
        feature_schemas_dict = metadata_dict.get('feature_schemas', {})
        if not feature_schemas_dict:
            log.warning("No feature_schemas found in metadata - will use inference")
            feature_schemas_obj = {}
        else:
            try:
                feature_schemas_obj = {
                    col: schemas.FeatureSchemaInfo(**schema_dict) 
                    for col, schema_dict in feature_schemas_dict.items()
                }
                log.info(f"Loaded feature schemas for {len(feature_schemas_obj)} columns")
            except Exception as e:
                log.error(f"Failed to parse feature_schemas: {e}")
                logger.log_structured_error(
                    structured_log,
                    "feature_schemas_parse_failed",
                    f"Failed to parse feature_schemas: {str(e)}",
                    {"stage": constants.PREP_STAGE}
                )
                _update_status_failed(run_id, f"Invalid feature schemas format: {str(e)}")
                return False
        
        # Load original data
        try:
            original_data_path = storage.get_run_dir(run_id) / constants.ORIGINAL_DATA_FILENAME
            if not original_data_path.exists():
                log.error(f"Original data file not found: {original_data_path}")
                logger.log_structured_error(
                    structured_log,
                    "original_data_not_found",
                    f"Original data file not found: {original_data_path}",
                    {"stage": constants.PREP_STAGE}
                )
                _update_status_failed(run_id, "Original data file not found")
                return False
                
            df_orig = pd.read_csv(original_data_path)
            log.info(f"Loaded original data: shape {df_orig.shape}")
            
            if df_orig.empty:
                log.error("Original data is empty")
                logger.log_structured_error(
                    structured_log,
                    "original_data_empty",
                    "Original data file is empty",
                    {"stage": constants.PREP_STAGE}
                )
                _update_status_failed(run_id, "Original data file is empty")
                return False
            
            # Structured log: Data loaded
            logger.log_structured_event(
                structured_log,
                "data_loaded",
                {
                    "data_shape": {"rows": df_orig.shape[0], "columns": df_orig.shape[1]},
                    "target_column": target_info.name,
                    "feature_schemas_count": len(feature_schemas_obj)
                },
                f"Original data loaded: {df_orig.shape}"
            )
                
        except Exception as e:
            log.error(f"Failed to load original data: {e}")
            logger.log_structured_error(
                structured_log,
                "data_load_failed",
                f"Failed to load original data: {str(e)}",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, f"Failed to read original data: {str(e)}")
            return False
        
        # =============================
        # 2. DATA CLEANING
        # =============================
        log.info("Starting data cleaning...")
        
        try:
            df_cleaned, cleaning_steps = cleaning_logic.clean_data(
                df_original=df_orig,
                feature_schemas=feature_schemas_obj,
                target_info=target_info,
                cleaning_config=None  # Using defaults for MVP
            )
            log.info(f"Cleaning completed: {df_orig.shape} → {df_cleaned.shape}")
            log.info(f"Cleaning steps performed: {len(cleaning_steps)}")
            for step in cleaning_steps:
                log.info(f"  - {step}")
            
            # Structured log: Cleaning completed
            logger.log_structured_event(
                structured_log,
                "cleaning_completed",
                {
                    "input_shape": {"rows": df_orig.shape[0], "columns": df_orig.shape[1]},
                    "output_shape": {"rows": df_cleaned.shape[0], "columns": df_cleaned.shape[1]},
                    "cleaning_steps": cleaning_steps,
                    "rows_removed": df_orig.shape[0] - df_cleaned.shape[0]
                },
                f"Data cleaning completed: {df_orig.shape} → {df_cleaned.shape}"
            )
                
        except Exception as e:
            log.error(f"Data cleaning failed: {e}")
            logger.log_structured_error(
                structured_log,
                "cleaning_failed",
                f"Data cleaning failed: {str(e)}",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, f"Data cleaning failed: {str(e)}")
            return False
        
        # =============================
        # 3. FEATURE ENCODING
        # =============================
        log.info("Starting feature encoding...")
        
        try:
            df_encoded, encoders_info = encoding_logic.encode_features(
                df_cleaned=df_cleaned,
                feature_schemas=feature_schemas_obj,
                target_info=target_info,
                run_id=run_id
            )
            log.info(f"Encoding completed: {df_cleaned.shape} → {df_encoded.shape}")
            log.info(f"Encoders/scalers saved: {len(encoders_info)}")
            for encoder_name in encoders_info.keys():
                log.info(f"  - {encoder_name}")
            
            # Structured log: Encoding completed
            logger.log_structured_event(
                structured_log,
                "encoding_completed",
                {
                    "input_shape": {"rows": df_cleaned.shape[0], "columns": df_cleaned.shape[1]},
                    "output_shape": {"rows": df_encoded.shape[0], "columns": df_encoded.shape[1]},
                    "encoders_count": len(encoders_info),
                    "encoder_types": list(encoders_info.keys()),
                    "columns_added": df_encoded.shape[1] - df_cleaned.shape[1]
                },
                f"Feature encoding completed: {df_cleaned.shape} → {df_encoded.shape}"
            )
                
        except Exception as e:
            log.error(f"Feature encoding failed: {e}")
            logger.log_structured_error(
                structured_log,
                "encoding_failed",
                f"Feature encoding failed: {str(e)}",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, f"Feature encoding failed: {str(e)}")
            return False
        
        # =============================
        # 4. SAVE CLEANED DATA
        # =============================
        log.info("Saving cleaned and encoded data...")
        
        try:
            cleaned_data_path = storage.get_run_dir(run_id) / constants.CLEANED_DATA_FILE
            df_encoded.to_csv(cleaned_data_path, index=False)
            log.info(f"Cleaned data saved to: {cleaned_data_path}")
            log.info(f"Final data shape: {df_encoded.shape}")
            
            # Structured log: Data saved
            logger.log_structured_event(
                structured_log,
                "cleaned_data_saved",
                {
                    "file_path": constants.CLEANED_DATA_FILE,
                    "file_size_bytes": cleaned_data_path.stat().st_size if cleaned_data_path.exists() else None,
                    "final_shape": {"rows": df_encoded.shape[0], "columns": df_encoded.shape[1]}
                },
                f"Cleaned data saved: {constants.CLEANED_DATA_FILE}"
            )
            
        except Exception as e:
            log.error(f"Failed to save cleaned data: {e}")
            logger.log_structured_error(
                structured_log,
                "save_cleaned_data_failed",
                f"Failed to save cleaned data: {str(e)}",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, f"Failed to save cleaned data: {str(e)}")
            return False
        
        # =============================
        # 5. GENERATE PROFILING REPORT
        # =============================
        log.info("Generating data profiling report...")
        
        # Use run_id in filename for uniqueness, or could use PROFILE_REPORT_FILE constant
        profile_report_path = storage.get_run_dir(run_id) / f"{run_id}_profile.html"
        
        try:
            profiling_success = profiling_logic.generate_profile_report_with_fallback(
                df_final_prepared=df_encoded,
                report_path=profile_report_path,
                title=f"Data Profile for Run {run_id}",
                run_id=run_id
            )
            
            if profiling_success:
                log.info(f"Profiling report generated: {profile_report_path}")
                # Structured log: Profiling completed
                logger.log_structured_event(
                    structured_log,
                    "profiling_completed",
                    {
                        "report_path": profile_report_path.name,
                        "file_size_bytes": profile_report_path.stat().st_size if profile_report_path.exists() else None
                    },
                    f"Profiling report generated: {profile_report_path.name}"
                )
            else:
                log.warning("Profiling report generation failed, but continuing...")
                logger.log_structured_event(
                    structured_log,
                    "profiling_failed",
                    {"non_critical": True},
                    "Profiling report generation failed (non-critical)"
                )
                
        except Exception as e:
            log.warning(f"Profiling report failed (non-critical): {e}")
            profiling_success = False
            logger.log_structured_event(
                structured_log,
                "profiling_error",
                {"error": str(e), "non_critical": True},
                f"Profiling report error (non-critical): {str(e)}"
            )
        
        # =============================
        # 6. UPDATE METADATA
        # =============================
        log.info("Updating metadata with prep results...")
        
        try:
            # Create prep_info dictionary
            prep_info = {
                'cleaning_steps_performed': cleaning_steps,
                'encoders_scalers_info': encoders_info,
                'cleaned_data_path': constants.CLEANED_DATA_FILE,
                'profiling_report_path': profile_report_path.name if profiling_success else None,
                'final_shape_after_prep': list(df_encoded.shape)
            }
            
            # Add prep_info to existing metadata
            metadata_dict['prep_info'] = prep_info
            
            # Save updated metadata
            storage.write_json_atomic(run_id, constants.METADATA_FILENAME, metadata_dict)
            log.info("Metadata updated with prep results")
            
            # Structured log: Metadata updated
            logger.log_structured_event(
                structured_log,
                "metadata_updated",
                {
                    "prep_info_keys": list(prep_info.keys()),
                    "cleaning_steps_count": len(cleaning_steps),
                    "encoders_count": len(encoders_info)
                },
                "Metadata updated with prep results"
            )
            
        except Exception as e:
            log.error(f"Failed to update metadata: {e}")
            logger.log_structured_error(
                structured_log,
                "metadata_update_failed",
                f"Failed to update metadata: {str(e)}",
                {"stage": constants.PREP_STAGE}
            )
            _update_status_failed(run_id, f"Failed to update metadata: {str(e)}")
            return False
        
        # =============================
        # 7. UPDATE STATUS TO COMPLETED
        # =============================
        try:
            status_data = {
                "stage": constants.PREP_STAGE,
                "status": "completed",
                "message": "Data preparation completed successfully."
            }
            storage.write_json_atomic(run_id, constants.STATUS_FILENAME, status_data)
            log.info("Status updated to completed")
            
        except Exception as e:
            log.warning(f"Could not update final status: {e}")
        
        # =============================
        # 8. LOG COMPLETION
        # =============================
        log.info("="*50)
        log.info("DATA PREPARATION STAGE COMPLETED SUCCESSFULLY")
        log.info("="*50)
        log.info(f"Original data shape: {df_orig.shape}")
        log.info(f"Final data shape: {df_encoded.shape}")
        log.info(f"Cleaning steps: {len(cleaning_steps)}")
        log.info(f"Encoders saved: {len(encoders_info)}")
        log.info(f"Profiling report: {'✅' if profiling_success else '❌'}")
        log.info(f"Output files:")
        log.info(f"  - {constants.CLEANED_DATA_FILE}")
        log.info(f"  - {constants.METADATA_FILENAME} (updated)")
        log.info(f"  - {constants.STATUS_FILENAME} (updated)")
        if profiling_success:
            log.info(f"  - {profile_report_path.name}")
        log.info("="*50)
        
        return True
        
    except Exception as e:
        log.error(f"Unexpected error in prep stage: {e}")
        log.error("Full traceback:", exc_info=True)
        _update_status_failed(run_id, f"Unexpected error: {str(e)}")
        return False


def _update_status_failed(run_id: str, error_message: str) -> None:
    """
    Helper function to update status.json to failed state.
    
    Args:
        run_id: Run identifier
        error_message: Error message to include in status
    """
    try:
        status_data = {
            "stage": constants.PREP_STAGE,
            "status": "failed",
            "message": f"Data preparation failed: {error_message}"
        }
        storage.write_json_atomic(run_id, constants.STATUS_FILENAME, status_data)
    except Exception as e:
        # If we can't even update status, log it but don't raise
        log = logger.get_stage_logger(run_id, constants.PREP_STAGE)
        log.error(f"Could not update status to failed: {e}")


def validate_prep_stage_inputs(run_id: str) -> bool:
    """
    Validate that all required inputs for the prep stage are available.
    
    Args:
        run_id: Run identifier
        
    Returns:
        True if all inputs are valid, False otherwise
    """
    log = logger.get_stage_logger(run_id, constants.PREP_STAGE)
    
    try:
        # Check if run directory exists
        run_dir = storage.get_run_dir(run_id)
        if not run_dir.exists():
            log.error(f"Run directory does not exist: {run_dir}")
            return False
        
        # Check if metadata.json exists
        metadata_path = run_dir / constants.METADATA_FILENAME
        if not metadata_path.exists():
            log.error(f"Metadata file does not exist: {metadata_path}")
            return False
        
        # Check if original_data.csv exists
        original_data_path = run_dir / constants.ORIGINAL_DATA_FILENAME
        if not original_data_path.exists():
            log.error(f"Original data file does not exist: {original_data_path}")
            return False
        
        # Try to load and validate metadata structure
        try:
            metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
            
            # Check for required keys
            if 'target_info' not in metadata_dict:
                log.error("Missing 'target_info' in metadata")
                return False
            
            # Validate target_info can be parsed
            target_info_dict = metadata_dict['target_info']
            schemas.TargetInfo(**target_info_dict)
            
            # Feature schemas are optional, but if present, validate format
            feature_schemas_dict = metadata_dict.get('feature_schemas', {})
            if feature_schemas_dict:
                # Try to parse at least one schema to validate format
                first_schema = next(iter(feature_schemas_dict.values()))
                schemas.FeatureSchemaInfo(**first_schema)
            
            log.info("All prep stage inputs validated successfully")
            return True
            
        except Exception as e:
            log.error(f"Metadata validation failed: {e}")
            return False
        
    except Exception as e:
        log.error(f"Input validation failed: {e}")
        return False


# Additional utility functions for testing and debugging
def get_prep_stage_summary(run_id: str) -> Optional[dict]:
    """
    Get a summary of the prep stage results for a given run.
    
    Args:
        run_id: Run identifier
        
    Returns:
        Dictionary with prep stage summary or None if not available
    """
    try:
        metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
        prep_info = metadata_dict.get('prep_info')
        
        if not prep_info:
            return None
        
        return {
            'run_id': run_id,
            'final_shape': prep_info.get('final_shape_after_prep'),
            'cleaning_steps_count': len(prep_info.get('cleaning_steps_performed', [])),
            'encoders_count': len(prep_info.get('encoders_scalers_info', {})),
            'has_profile_report': prep_info.get('profiling_report_path') is not None,
            'cleaned_data_available': (storage.get_run_dir(run_id) / constants.CLEANED_DATA_FILE).exists()
        }
        
    except Exception:
        return None
</file>

<file path="step_4_prep/profiling_logic.py">
"""
Data profiling logic for The Projection Wizard.
Contains functions for generating ydata-profiling reports of prepared data.
"""

import pandas as pd
from pathlib import Path
from typing import Optional
import warnings
import signal
import time

from common import logger
from common import constants


class TimeoutError(Exception):
    """Custom timeout exception"""
    pass


def timeout_handler(signum, frame):
    """Signal handler for timeout"""
    raise TimeoutError("Profile generation timed out")


def generate_profile_report_with_timeout(df_final_prepared: pd.DataFrame, 
                                       report_path: Path, 
                                       title: str,
                                       timeout_seconds: int = 300,
                                       run_id: Optional[str] = None) -> bool:
    """
    Generate profile report with timeout protection.
    
    Args:
        df_final_prepared: The DataFrame after cleaning and encoding
        report_path: Full Path object where the HTML report should be saved
        title: Title for the ydata-profiling report
        timeout_seconds: Maximum time to allow for profile generation (default: 5 minutes)
        
    Returns:
        True if report generation successful, False otherwise
    """
    try:
        # Set up timeout signal (Unix/Mac only)
        if hasattr(signal, 'SIGALRM'):
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout_seconds)
        
        # Generate the profile
        result = generate_profile_report(df_final_prepared, report_path, title, run_id)
        
        # Cancel the alarm if we succeeded
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)
            
        return result
        
    except TimeoutError:
        # Create logger for timeout error
        log = logger.get_stage_logger(run_id, constants.PREP_STAGE) if run_id else logger.get_logger("profiling_temp", "profiling")
        log.error(f"Profile generation timed out after {timeout_seconds} seconds")
        
        # Clean up partial file if it exists
        try:
            if report_path.exists():
                report_path.unlink()
                log.info("Cleaned up partial report file after timeout")
        except Exception as cleanup_error:
            log.warning(f"Could not clean up partial file after timeout: {cleanup_error}")
        
        return False
        
    except Exception as e:
        # Cancel the alarm on any exception
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)
        
        log = logger.get_stage_logger(run_id, constants.PREP_STAGE) if run_id else logger.get_logger("profiling_temp", "profiling")
        log.error(f"Profile generation failed with exception: {str(e)}")
        return False
    
    finally:
        # Ensure alarm is always cancelled
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)


def generate_profile_report(df_final_prepared: pd.DataFrame, 
                          report_path: Path, 
                          title: str,
                          run_id: Optional[str] = None) -> bool:
    """
    Generate a ydata-profiling HTML report for the prepared DataFrame.
    
    Args:
        df_final_prepared: The DataFrame after cleaning and encoding
        report_path: Full Path object where the HTML report should be saved
        title: Title for the ydata-profiling report
        
    Returns:
        True if report generation successful, False otherwise
    """
    try:
        # Import ydata-profiling - handle potential import issues
        try:
            from ydata_profiling import ProfileReport
        except ImportError:
            # Fallback to older pandas-profiling package name
            try:
                from pandas_profiling import ProfileReport
            except ImportError:
                # Create logger for error reporting
                log = logger.get_stage_logger(run_id, constants.PREP_STAGE) if run_id else logger.get_logger("profiling_temp", "profiling")
                log.error("Neither ydata-profiling nor pandas-profiling is installed. "
                         "Please install with: pip install ydata-profiling")
                return False
        
        # Ensure parent directory exists
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Create a simple logger for this function since we don't have run_id
        log = logger.get_stage_logger(run_id, constants.PREP_STAGE) if run_id else logger.get_logger("profiling_temp", "profiling")
        
        # Log start of profiling
        log.info(f"Starting profile report generation for '{title}'")
        log.info(f"DataFrame shape: {df_final_prepared.shape}")
        log.info(f"Report will be saved to: {report_path}")
        
        # Configure profiling settings for robustness and performance
        # Suppress warnings during profiling to avoid cluttering logs
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            
            # Generate profile with conservative settings for stability
            profile = ProfileReport(
                df_final_prepared,
                title=title,
                explorative=True,
                # Additional settings for robustness
                minimal=False,  # Full report
                samples={"head": 5, "tail": 5},  # Limit sample size for performance
                correlations={
                    "auto": {"calculate": False},      # DISABLE - can hang on large datasets
                    "pearson": {"calculate": False},   # DISABLE - can hang on large datasets
                    "spearman": {"calculate": False},  # DISABLE - can hang on large datasets
                    "kendall": {"calculate": False},   # DISABLE - can hang on large datasets
                    "phi_k": {"calculate": False},     # DISABLE - can hang on large datasets
                    "cramers": {"calculate": False}    # DISABLE - can hang on large datasets
                },
                missing_diagrams={
                    "matrix": True,
                    "bar": True,
                    "heatmap": False  # Skip heatmap for performance with large datasets
                },
                interactions={"continuous": False, "targets": []},  # Disable interactions for performance
                duplicates={"head": 5}  # Limit duplicate examples
            )
        
        # Save the report
        log.info("Generating and saving profile report...")
        profile.to_file(report_path)
        
        # Verify the file was created successfully
        if report_path.exists() and report_path.stat().st_size > 0:
            file_size_mb = report_path.stat().st_size / (1024 * 1024)
            log.info(f"Profile report successfully generated: {report_path}")
            log.info(f"Report file size: {file_size_mb:.2f} MB")
            return True
        else:
            log.error("Profile report file was not created or is empty")
            return False
            
    except Exception as e:
        # Create logger in exception handler too
        log = logger.get_stage_logger(run_id, constants.PREP_STAGE) if run_id else logger.get_logger("profiling_temp", "profiling")
        log.error(f"Failed to generate profile report: {str(e)}")
        log.error(f"Error type: {type(e).__name__}")
        
        # Clean up partial file if it exists
        try:
            if report_path.exists():
                report_path.unlink()
                log.info("Cleaned up partial report file")
        except Exception as cleanup_error:
            log.warning(f"Could not clean up partial file: {cleanup_error}")
        
        return False


def generate_profile_report_with_fallback(df_final_prepared: pd.DataFrame, 
                                        report_path: Path, 
                                        title: str,
                                        run_id: Optional[str] = None) -> bool:
    """
    Generate a ydata-profiling report with fallback options for better compatibility.
    
    Args:
        df_final_prepared: The DataFrame after cleaning and encoding
        report_path: Full Path object where the HTML report should be saved
        title: Title for the ydata-profiling report
        run_id: Optional run ID for logging context
        
    Returns:
        True if report generation successful, False otherwise
    """
    # Get logger with optional run context
    log = logger.get_stage_logger(run_id, constants.PREP_STAGE) if run_id else logger.get_logger("profiling_temp", "profiling")
    
    # First attempt: Full profile report with timeout protection
    log.info(f"Attempting full profile report generation for '{title}' (with 5-minute timeout)")
    success = generate_profile_report_with_timeout(df_final_prepared, report_path, title, timeout_seconds=300, run_id=run_id)
    
    if success:
        return True
    
    # Fallback 1: Try with minimal profile
    log.warning("Full profile failed, attempting minimal profile...")
    try:
        from ydata_profiling import ProfileReport
        
        # Ensure parent directory exists
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            
            # Minimal profile with basic statistics only - ULTRA SAFE
            profile = ProfileReport(
                df_final_prepared,
                title=f"{title} (Minimal)",
                minimal=True,  # Minimal report
                explorative=False,
                # DISABLE ALL CORRELATIONS - these cause hangs
                correlations={
                    "auto": {"calculate": False},
                    "pearson": {"calculate": False},
                    "spearman": {"calculate": False},
                    "kendall": {"calculate": False},
                    "phi_k": {"calculate": False},
                    "cramers": {"calculate": False}
                },
                missing_diagrams={"matrix": False, "bar": True, "heatmap": False},
                interactions={"continuous": False, "targets": []},
                duplicates={"head": 0},
                # Additional safety settings
                samples={"head": 0, "tail": 0},  # No sample display
                infer_dtypes=False  # Skip dtype inference
            )
            
        profile.to_file(report_path)
        
        if report_path.exists() and report_path.stat().st_size > 0:
            log.info(f"Minimal profile report successfully generated: {report_path}")
            return True
            
    except Exception as e:
        log.error(f"Minimal profile also failed: {str(e)}")
    
    # Fallback 2: Create basic HTML summary
    log.warning("All profiling attempts failed, creating basic HTML summary...")
    try:
        basic_html = _create_basic_html_summary(df_final_prepared, title)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(basic_html)
            
        if report_path.exists() and report_path.stat().st_size > 0:
            log.info(f"Basic HTML summary created: {report_path}")
            return True
            
    except Exception as e:
        log.error(f"Even basic HTML generation failed: {str(e)}")
    
    return False


def _create_basic_html_summary(df: pd.DataFrame, title: str) -> str:
    """
    Create a basic HTML summary as a fallback when ydata-profiling fails.
    
    Args:
        df: DataFrame to summarize
        title: Title for the report
        
    Returns:
        HTML string with basic statistics
    """
    html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>{title}</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
            .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
            table {{ border-collapse: collapse; width: 100%; }}
            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
            th {{ background-color: #f2f2f2; }}
            .warning {{ background-color: #fff3cd; border: 1px solid #ffeaa7; padding: 10px; border-radius: 5px; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h1>{title}</h1>
            <p><strong>Basic Data Summary Report</strong></p>
            <p><em>Generated as fallback - full profiling was not available</em></p>
        </div>
        
        <div class="warning">
            <strong>Note:</strong> This is a basic summary report. For detailed profiling, 
            please ensure ydata-profiling is properly installed and compatible.
        </div>
        
        <div class="section">
            <h2>Dataset Overview</h2>
            <ul>
                <li><strong>Rows:</strong> {df.shape[0]:,}</li>
                <li><strong>Columns:</strong> {df.shape[1]:,}</li>
                <li><strong>Memory Usage:</strong> {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>Column Information</h2>
            <table>
                <tr>
                    <th>Column</th>
                    <th>Data Type</th>
                    <th>Non-Null Count</th>
                    <th>Missing %</th>
                </tr>
    """
    
    # Add column information
    for col in df.columns:
        non_null_count = df[col].count()
        missing_pct = (1 - non_null_count / len(df)) * 100
        html += f"""
                <tr>
                    <td>{col}</td>
                    <td>{df[col].dtype}</td>
                    <td>{non_null_count:,}</td>
                    <td>{missing_pct:.1f}%</td>
                </tr>
        """
    
    html += """
            </table>
        </div>
        
        <div class="section">
            <h2>Numeric Columns Summary</h2>
    """
    
    # Add numeric summary if any numeric columns exist
    numeric_cols = df.select_dtypes(include=['number']).columns
    if len(numeric_cols) > 0:
        html += f"<p>Found {len(numeric_cols)} numeric columns</p>"
        desc_stats = df[numeric_cols].describe()
        html += desc_stats.to_html(classes='', table_id='numeric-summary')
    else:
        html += "<p>No numeric columns found</p>"
    
    html += """
        </div>
        
        <div class="section">
            <h2>Categorical Columns Summary</h2>
    """
    
    # Add categorical summary
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        html += f"<p>Found {len(categorical_cols)} categorical columns</p>"
        html += "<ul>"
        for col in categorical_cols[:10]:  # Limit to first 10
            unique_count = df[col].nunique()
            html += f"<li><strong>{col}:</strong> {unique_count} unique values</li>"
        if len(categorical_cols) > 10:
            html += f"<li><em>... and {len(categorical_cols) - 10} more columns</em></li>"
        html += "</ul>"
    else:
        html += "<p>No categorical columns found</p>"
    
    html += """
        </div>
    </body>
    </html>
    """
    
    return html
</file>

<file path="step_5_automl/automl_runner.py">
"""
AutoML Stage Runner for The Projection Wizard.
Orchestrates the complete AutoML stage using PyCaret for model training.
"""

import pandas as pd
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

from common import logger, storage, constants, schemas
from . import pycaret_logic


def run_automl_stage(run_id: str, test_mode: bool = False) -> bool:
    """
    Execute the complete AutoML stage for a given run.
    
    This function orchestrates:
    1. Loading inputs (metadata.json, cleaned_data.csv)
    2. Running PyCaret AutoML experiment
    3. Saving model and updating metadata
    4. Updating pipeline status
    
    Args:
        run_id: Unique run identifier
        test_mode: Flag to allow AutoML to work with very small datasets for testing purposes
        
    Returns:
        True if stage completes successfully, False otherwise
    """
    # Get loggers for this run and stage
    log = logger.get_stage_logger(run_id, constants.AUTOML_STAGE)
    structured_log = logger.get_stage_structured_logger(run_id, constants.AUTOML_STAGE)
    
    # Get pipeline summary logger
    summary_logger = logger.get_pipeline_summary_logger(run_id)
    
    # Track timing for summary
    start_time = datetime.now()
    
    try:
        # =============================
        # 1. VALIDATE INPUTS
        # =============================
        log.info("Starting AutoML stage validation...")
        
        # Structured log: Stage started
        logger.log_structured_event(
            structured_log,
            "stage_started",
            {"stage": constants.AUTOML_STAGE, "test_mode": test_mode},
            "AutoML stage validation started"
        )
        
        # Check if validation stage failed - prevent AutoML from running
        try:
            status_data = storage.read_json(run_id, constants.STATUS_FILENAME)
            if status_data and status_data.get('stage') == constants.VALIDATION_STAGE:
                if status_data.get('status') == 'failed':
                    error_msg = "Cannot run AutoML: validation stage failed"
                    validation_errors = status_data.get('errors', [])
                    if validation_errors:
                        error_msg += f" - Reasons: {'; '.join(validation_errors[:3])}"
                    
                    log.error(error_msg)
                    logger.log_structured_error(
                        structured_log,
                        "validation_failed_prerequisite",
                        error_msg,
                        {"stage": constants.AUTOML_STAGE, "validation_errors": validation_errors}
                    )
                    _update_status_failed(run_id, error_msg)
                    return False
        except Exception as e:
            log.warning(f"Could not check validation status: {e}")
            # Continue execution - don't fail on status check errors
        
        if not validate_automl_stage_inputs(run_id):
            log.error("AutoML stage input validation failed")
            logger.log_structured_error(
                structured_log,
                "input_validation_failed",
                "AutoML stage input validation failed",
                {"stage": constants.AUTOML_STAGE}
            )
            _update_status_failed(run_id, "Input validation failed")
            return False
        
        log.info("AutoML stage inputs validated successfully")
        
        # Structured log: Validation passed
        logger.log_structured_event(
            structured_log,
            "input_validation_passed",
            {"stage": constants.AUTOML_STAGE},
            "AutoML stage input validation passed"
        )
        
        # =============================
        # 2. LOAD METADATA AND DATA
        # =============================
        log.info("Loading metadata and cleaned data...")
        
        try:
            # Load metadata
            metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
            target_info_dict = metadata_dict['target_info']
            target_info = schemas.TargetInfo(**target_info_dict)
            
            log.info(f"Target info loaded: column='{target_info.name}', task_type='{target_info.task_type}'")
            
            # Load cleaned data
            df_ml_ready = storage.read_cleaned_data(run_id)
            if df_ml_ready is None:
                raise Exception("Could not load cleaned data")
            
            log.info(f"Cleaned data loaded: shape={df_ml_ready.shape}")
            
            # Structured log: Data loaded
            logger.log_structured_event(
                structured_log,
                "data_loaded",
                {
                    "target_column": target_info.name,
                    "task_type": target_info.task_type,
                    "data_shape": {"rows": df_ml_ready.shape[0], "columns": df_ml_ready.shape[1]}
                },
                f"Training data loaded: {df_ml_ready.shape}"
            )
            
        except Exception as e:
            log.error(f"Failed to load inputs: {e}")
            logger.log_structured_error(
                structured_log,
                "data_loading_failed",
                f"Failed to load inputs: {str(e)}",
                {"stage": constants.AUTOML_STAGE}
            )
            _update_status_failed(run_id, f"Failed to load inputs: {str(e)}")
            return False
        
        # =============================
        # 3. RUN AUTOML
        # =============================
        log.info("Starting PyCaret AutoML experiment...")
        
        # Structured log: Training started
        logger.log_structured_event(
            structured_log,
            "training_started",
            {
                "tool": "PyCaret",
                "dataset_shape": {"rows": df_ml_ready.shape[0], "columns": df_ml_ready.shape[1]},
                "target_column": target_info.name,
                "task_type": target_info.task_type
            },
            "AutoML training started with PyCaret"
        )
        
        try:
            # Run PyCaret AutoML
            best_model, metrics, model_name = pycaret_logic.run_pycaret_experiment(
                df_ml_ready=df_ml_ready,
                target_column_name=target_info.name,
                task_type=target_info.task_type,
                run_id=run_id,
                pycaret_model_dir=storage.get_run_dir(run_id) / constants.MODEL_DIR,
                test_mode=test_mode
            )
            
            log.info(f"AutoML experiment completed. Best model: {model_name}")
            log.info(f"Performance metrics: {metrics}")
            
            # Structured log: Training completed with metrics
            logger.log_structured_event(
                structured_log,
                "training_completed",
                {
                    "model_name": model_name,
                    "metrics": metrics,
                    "tool": "PyCaret"
                },
                f"AutoML training completed: {model_name}"
            )
            
            # Log individual metrics as structured events
            for metric_name, metric_value in metrics.items():
                logger.log_structured_metric(
                    structured_log,
                    metric_name,
                    metric_value,
                    "performance",
                    {"model_name": model_name, "task_type": target_info.task_type}
                )
            
        except Exception as e:
            log.error(f"PyCaret AutoML failed: {e}")
            logger.log_structured_error(
                structured_log,
                "training_failed",
                f"PyCaret AutoML failed: {str(e)}",
                {"tool": "PyCaret", "stage": constants.AUTOML_STAGE}
            )
            _update_status_failed(run_id, f"AutoML training failed: {str(e)}")
            return False
        
        # =============================
        # 4. SAVE MODEL
        # =============================
        log.info("Saving trained model...")
        
        try:
            # Verify model was saved by the pycaret_experiment function
            run_dir = storage.get_run_dir(run_id)
            model_dir = run_dir / constants.MODEL_DIR
            model_path = model_dir / "pycaret_pipeline.pkl"
            
            if model_path.exists():
                log.info(f"Model verified saved at: {model_path}")
                
                # Structured log: Model saved
                logger.log_structured_event(
                    structured_log,
                    "model_saved",
                    {
                        "model_path": str(model_path.relative_to(run_dir)),
                        "model_name": model_name,
                        "file_size_bytes": model_path.stat().st_size
                    },
                    f"Model saved: {model_path.name}"
                )
            else:
                raise FileNotFoundError(f"Model was not saved at expected location: {model_path}")
            
        except Exception as e:
            log.error(f"Failed to save model: {e}")
            logger.log_structured_error(
                structured_log,
                "model_save_failed",
                f"Failed to save model: {str(e)}",
                {"stage": constants.AUTOML_STAGE}
            )
            _update_status_failed(run_id, f"Failed to save model: {str(e)}")
            return False
        
        # =============================
        # 5. UPDATE METADATA WITH AUTOML INFO
        # =============================
        log.info("Updating metadata with AutoML results...")
        
        try:
            # Create automl_info dictionary
            automl_info = {
                'tool_used': 'PyCaret',
                'best_model_name': model_name,
                'pycaret_pipeline_path': str(Path(constants.MODEL_DIR) / 'pycaret_pipeline.pkl'),  # Relative to run_dir
                'performance_metrics': metrics,
                'automl_completed_at': datetime.now(timezone.utc).isoformat(),
                'target_column': target_info.name,
                'task_type': target_info.task_type,
                'dataset_shape_for_training': list(df_ml_ready.shape)
            }
            
            # Add automl_info to existing metadata
            metadata_dict['automl_info'] = automl_info
            
            # Save updated metadata
            storage.write_json_atomic(run_id, constants.METADATA_FILENAME, metadata_dict)
            log.info("Metadata updated with AutoML results")
            
            # Structured log: Metadata updated
            logger.log_structured_event(
                structured_log,
                "metadata_updated",
                {"automl_info_keys": list(automl_info.keys())},
                "Metadata updated with AutoML results"
            )
            
        except Exception as e:
            log.error(f"Failed to update metadata: {e}")
            logger.log_structured_error(
                structured_log,
                "metadata_update_failed",
                f"Failed to update metadata: {str(e)}",
                {"stage": constants.AUTOML_STAGE}
            )
            _update_status_failed(run_id, f"Failed to update metadata: {str(e)}")
            return False
        
        # =============================
        # 6. UPDATE STATUS TO COMPLETED
        # =============================
        try:
            status_data = {
                "stage": constants.AUTOML_STAGE,
                "status": "completed",
                "message": f"AutoML completed. Best model: {model_name}"
            }
            storage.write_json_atomic(run_id, constants.STATUS_FILENAME, status_data)
            log.info("Status updated to completed")
            
            # Structured log: Status updated
            logger.log_structured_event(
                structured_log,
                "status_updated",
                {"status": "completed", "stage": constants.AUTOML_STAGE},
                "AutoML stage status updated to completed"
            )
            
        except Exception as e:
            log.warning(f"Could not update final status: {e}")
        
        # =============================
        # 7. LOG COMPLETION SUMMARY
        # =============================
        end_time = datetime.now()
        training_duration = (end_time - start_time).total_seconds()
        
        # Log detailed completion for technical log
        log.info("="*50)
        log.info("AUTOML STAGE COMPLETED SUCCESSFULLY")
        log.info("="*50)
        log.info(f"Dataset shape: {df_ml_ready.shape}")
        log.info(f"Target column: {target_info.name}")
        log.info(f"Task type: {target_info.task_type}")
        log.info(f"Best model: {model_name}")
        log.info(f"Performance metrics:")
        for metric_name, metric_value in metrics.items():
            log.info(f"  - {metric_name}: {metric_value}")
        log.info(f"Output files:")
        log.info(f"  - {constants.MODEL_DIR}/pycaret_pipeline.pkl")
        log.info(f"  - {constants.METADATA_FILENAME} (updated)")
        log.info(f"  - {constants.STATUS_FILENAME} (updated)")
        log.info("="*50)
        
        # Structured log: Stage completed
        logger.log_structured_event(
            structured_log,
            "stage_completed",
            {
                "stage": constants.AUTOML_STAGE,
                "duration_seconds": training_duration,
                "model_name": model_name,
                "metrics_count": len(metrics),
                "completed_at": end_time.isoformat()
            },
            f"AutoML stage completed successfully in {training_duration:.1f}s"
        )
        
        # Log high-level summary for pipeline summary
        summary_logger.log_automl_summary(
            model_name=model_name,
            task_type=target_info.task_type,
            target_column=target_info.name,
            training_shape=df_ml_ready.shape,
            metrics=metrics,
            training_duration=training_duration
        )
        
        return True
        
    except Exception as e:
        log.error(f"Unexpected error in AutoML stage: {e}")
        log.error("Full traceback:", exc_info=True)
        
        # Structured log: Unexpected error
        logger.log_structured_error(
            structured_log,
            "unexpected_error",
            f"Unexpected error in AutoML stage: {str(e)}",
            {"stage": constants.AUTOML_STAGE}
        )
        
        _update_status_failed(run_id, f"Unexpected error: {str(e)}")
        return False


def _update_status_failed(run_id: str, error_message: str) -> None:
    """
    Helper function to update status.json to failed state.
    
    Args:
        run_id: Run identifier
        error_message: Error message to include in status
    """
    try:
        status_data = {
            "stage": constants.AUTOML_STAGE,
            "status": "failed",
            "message": f"AutoML failed: {error_message}"
        }
        storage.write_json_atomic(run_id, constants.STATUS_FILENAME, status_data)
    except Exception as e:
        # If we can't even update status, log it but don't raise
        log = logger.get_stage_logger(run_id, constants.AUTOML_STAGE)
        log.error(f"Could not update status to failed: {e}")


def validate_automl_stage_inputs(run_id: str) -> bool:
    """
    Validate that all required inputs for the AutoML stage are available.
    
    Args:
        run_id: Run identifier
        
    Returns:
        True if all inputs are valid, False otherwise
    """
    log = logger.get_stage_logger(run_id, constants.AUTOML_STAGE)
    
    try:
        # Check if run directory exists
        run_dir = storage.get_run_dir(run_id)
        if not run_dir.exists():
            log.error(f"Run directory does not exist: {run_dir}")
            return False
        
        # Check if metadata.json exists
        metadata_path = run_dir / constants.METADATA_FILENAME
        if not metadata_path.exists():
            log.error(f"Metadata file does not exist: {metadata_path}")
            return False
        
        # Check if cleaned_data.csv exists
        cleaned_data_path = run_dir / constants.CLEANED_DATA_FILE
        if not cleaned_data_path.exists():
            log.error(f"Cleaned data file does not exist: {cleaned_data_path}")
            return False
        
        # Try to load and validate metadata structure
        try:
            metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
            
            # Check for required keys
            if 'target_info' not in metadata_dict:
                log.error("Missing 'target_info' in metadata")
                return False
            
            if 'prep_info' not in metadata_dict:
                log.error("Missing 'prep_info' in metadata - prep stage must be completed first")
                return False
            
            # Validate target_info can be parsed
            target_info_dict = metadata_dict['target_info']
            target_info = schemas.TargetInfo(**target_info_dict)
            
            # Validate prep_info exists and has expected structure
            prep_info = metadata_dict['prep_info']
            if not isinstance(prep_info, dict):
                log.error("prep_info is not a dictionary")
                return False
            
            # Check for key prep_info fields
            required_prep_fields = ['final_shape_after_prep', 'cleaning_steps_performed']
            for field in required_prep_fields:
                if field not in prep_info:
                    log.error(f"Missing required field in prep_info: {field}")
                    return False
            
            log.info("All AutoML stage inputs validated successfully")
            return True
            
        except Exception as e:
            log.error(f"Metadata validation failed: {e}")
            return False
        
    except Exception as e:
        log.error(f"Input validation failed: {e}")
        return False


def get_automl_stage_summary(run_id: str) -> Optional[dict]:
    """
    Get a summary of the AutoML stage results for a given run.
    
    Args:
        run_id: Run identifier
        
    Returns:
        Dictionary with AutoML stage summary or None if not available
    """
    try:
        metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
        automl_info = metadata_dict.get('automl_info')
        
        if not automl_info:
            return None
        
        # Check if model file exists
        model_file_path = storage.get_run_dir(run_id) / automl_info.get('pycaret_pipeline_path', '')
        
        return {
            'run_id': run_id,
            'tool_used': automl_info.get('tool_used'),
            'best_model_name': automl_info.get('best_model_name'),
            'task_type': automl_info.get('task_type'),
            'target_column': automl_info.get('target_column'),
            'performance_metrics': automl_info.get('performance_metrics', {}),
            'training_dataset_shape': automl_info.get('dataset_shape_for_training'),
            'model_file_exists': model_file_path.exists() if model_file_path else False,
            'completed_at': automl_info.get('automl_completed_at')
        }
        
    except Exception:
        return None
</file>

<file path="step_5_automl/pycaret_logic.py">
"""
PyCaret interaction logic for The Projection Wizard.
Handles AutoML training using PyCaret for classification and regression tasks.
"""

import pandas as pd
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import traceback

from common import logger, constants


def run_pycaret_experiment(
    df_ml_ready: pd.DataFrame,
    target_column_name: str,
    task_type: str,
    run_id: str,
    pycaret_model_dir: Path,
    session_id: int = 123,
    top_n_models_to_compare: int = 3,
    allow_lightgbm_and_xgboost: bool = True,
    test_mode: bool = False
) -> Tuple[Optional[Any], Optional[dict], Optional[str]]:
    """
    Run PyCaret AutoML experiment for classification or regression.
    
    Args:
        df_ml_ready: The ML-ready DataFrame from cleaned_data.csv
        target_column_name: Name of the target column
        task_type: From target_info.task_type ("classification" or "regression")
        run_id: For logging and unique experiment naming
        pycaret_model_dir: Path object to data/runs/<run_id>/model/ where PyCaret will save its pipeline
        session_id: Integer for PyCaret reproducibility
        top_n_models_to_compare: How many top models from compare_models to consider
        allow_lightgbm_and_xgboost: Flag to include/exclude potentially problematic models
        test_mode: Flag to allow AutoML to work with very small datasets for testing purposes
        
    Returns:
        Tuple of (final_pycaret_pipeline, performance_metrics, best_model_name_str)
        Returns (None, None, None) if any step fails
    """
    # Get logger for this run
    log = logger.get_stage_logger(run_id, constants.AUTOML_STAGE)
    
    try:
        log.info(f"Starting PyCaret experiment for {task_type} task")
        log.info(f"Target column: {target_column_name}")
        log.info(f"Dataset shape: {df_ml_ready.shape}")
        log.info(f"Session ID: {session_id}")
        log.info(f"Top N models to compare: {top_n_models_to_compare}")
        log.info(f"Allow LightGBM/XGBoost: {allow_lightgbm_and_xgboost}")
        if test_mode:
            log.warning("TEST MODE ENABLED - allowing very small datasets")
        
        # Basic validation (always performed)
        if df_ml_ready.empty:
            log.error("Input DataFrame is empty")
            return None, None, None
            
        if target_column_name not in df_ml_ready.columns:
            log.error(f"Target column '{target_column_name}' not found in DataFrame")
            log.error(f"Available columns: {list(df_ml_ready.columns)}")
            return None, None, None
            
        if task_type not in ["classification", "regression"]:
            log.error(f"Invalid task_type: {task_type}. Must be 'classification' or 'regression'")
            return None, None, None
        
        # Full validation (unless in test mode)
        if not test_mode:
            is_valid, validation_issues = validate_pycaret_inputs(
                df_ml_ready=df_ml_ready,
                target_column_name=target_column_name,
                task_type=task_type
            )
            
            if not is_valid:
                log.error("Input validation failed for PyCaret:")
                for issue in validation_issues:
                    log.error(f"  - {issue}")
                return None, None, None
        else:
            log.warning("Skipping dataset size validation due to test mode")
        
        # Check for missing values in target
        target_nulls = df_ml_ready[target_column_name].isnull().sum()
        if target_nulls > 0:
            log.warning(f"Target column has {target_nulls} missing values")
            # Drop rows with missing targets for PyCaret
            df_ml_ready = df_ml_ready.dropna(subset=[target_column_name])
            log.info(f"Dropped rows with missing targets, new shape: {df_ml_ready.shape}")
        
        # Import appropriate PyCaret module based on task type
        if task_type == "classification":
            try:
                from pycaret.classification import (
                    setup, compare_models, finalize_model, save_model, 
                    predict_model, pull, get_config
                )
                log.info("Successfully imported PyCaret classification modules")
            except ImportError as e:
                log.error(f"Failed to import PyCaret classification modules: {e}")
                return None, None, None
        else:  # regression
            try:
                from pycaret.regression import (
                    setup, compare_models, finalize_model, save_model,
                    predict_model, pull, get_config
                )
                log.info("Successfully imported PyCaret regression modules")
            except ImportError as e:
                log.error(f"Failed to import PyCaret regression modules: {e}")
                return None, None, None
        
        # =============================
        # 1. PYCARET SETUP
        # =============================
        log.info("Setting up PyCaret environment...")
        
        try:
            pc_setup = setup(
                data=df_ml_ready,
                target=target_column_name,
                session_id=session_id,
                log_experiment=False,
                verbose=False,
                html=False,
                use_gpu=False,
                train_size=0.8  # 80% for training, 20% for test
            )
            log.info("PyCaret setup completed successfully")
            log.info(f"Setup configuration: {type(pc_setup)}")
        except Exception as e:
            log.error(f"PyCaret setup failed: {e}")
            log.error(f"Traceback: {traceback.format_exc()}")
            return None, None, None
        
        # =============================
        # 2. COMPARE MODELS
        # =============================
        log.info("Comparing models...")
        
        try:
            # Determine which models to include/exclude
            include_models = None
            exclude_models = []
            
            if not allow_lightgbm_and_xgboost:
                exclude_models = ['lightgbm', 'xgboost']
                log.info(f"Excluding models: {exclude_models}")
            
            # Adjust cross-validation folds based on dataset size
            n_rows = len(df_ml_ready)
            if test_mode and n_rows < 30:
                cv_folds = 2  # 2-fold for very small test datasets
                log.warning(f"Using minimal {cv_folds}-fold CV in test mode for {n_rows} rows")
            elif n_rows < 50:
                cv_folds = 3  # 3-fold for small datasets
            elif n_rows < 100:
                cv_folds = 5  # Default 5-fold for medium datasets
            else:
                cv_folds = 5  # 5-fold for larger datasets
            
            log.info(f"Using {cv_folds}-fold cross-validation for dataset with {n_rows} rows")
            
            # Compare models to find the best ones
            if exclude_models:
                top_models = compare_models(
                    n_select=top_n_models_to_compare,
                    exclude=exclude_models,
                    verbose=False,
                    fold=cv_folds
                )
            else:
                top_models = compare_models(
                    n_select=top_n_models_to_compare,
                    verbose=False,
                    fold=cv_folds
                )
            
            log.info(f"Model comparison completed, got {len(top_models) if isinstance(top_models, list) else 1} model(s)")
            
            # Handle single model vs list of models
            if not isinstance(top_models, list):
                if top_models is None:
                    log.error("No models returned from compare_models - creating fallback model")
                    # Create a simple fallback model
                    best_initial_model = _create_fallback_model(task_type, log)
                    if best_initial_model is None:
                        return None, None, None
                else:
                    best_initial_model = top_models
                    log.info("Single model returned from compare_models")
            else:
                if len(top_models) == 0:
                    log.error("No models returned from compare_models - creating fallback model")
                    # Create a simple fallback model
                    best_initial_model = _create_fallback_model(task_type, log)
                    if best_initial_model is None:
                        return None, None, None
                else:
                    best_initial_model = top_models[0]
                    log.info(f"Using first model from {len(top_models)} compared models")
            
            # Get model name
            try:
                model_name = str(type(best_initial_model).__name__)
                log.info(f"Best initial model: {model_name}")
            except:
                model_name = "Unknown Model"
                log.warning("Could not determine model name")
            
        except Exception as e:
            log.error(f"Model comparison failed: {e}")
            log.error(f"Traceback: {traceback.format_exc()}")
            return None, None, None
        
        # =============================
        # 3. FINALIZE MODEL (SKIP TUNING FOR MVP)
        # =============================
        log.info("Finalizing model (training on full dataset)...")
        
        try:
            # For MVP, skip tuning and directly finalize
            tuned_model = best_initial_model
            
            # Finalize trains on the full dataset (including hold-out test set)
            final_pipeline = finalize_model(tuned_model)
            log.info("Model finalization completed successfully")
            
        except Exception as e:
            log.error(f"Model finalization failed: {e}")
            log.error(f"Traceback: {traceback.format_exc()}")
            return None, None, None
        
        # =============================
        # 4. SAVE MODEL
        # =============================
        log.info("Saving PyCaret pipeline...")
        
        try:
            # Ensure the model directory exists
            pycaret_model_dir.mkdir(parents=True, exist_ok=True)
            
            # Save the finalized pipeline
            model_save_path = str(pycaret_model_dir / 'pycaret_pipeline')
            save_model(final_pipeline, model_save_path)
            
            # PyCaret automatically adds .pkl extension
            actual_model_file = pycaret_model_dir / 'pycaret_pipeline.pkl'
            if actual_model_file.exists():
                log.info(f"Model saved successfully: {actual_model_file}")
            else:
                log.warning(f"Model file not found at expected location: {actual_model_file}")
            
        except Exception as e:
            log.error(f"Model saving failed: {e}")
            log.error(f"Traceback: {traceback.format_exc()}")
            return None, None, None
        
        # =============================
        # 5. EXTRACT PERFORMANCE METRICS
        # =============================
        log.info("Extracting performance metrics...")
        
        try:
            # Get the last metrics from PyCaret
            metrics_df = pull()
            
            if metrics_df is not None and not metrics_df.empty:
                # Convert metrics DataFrame to dictionary
                performance_metrics = {}
                
                if task_type == "classification":
                    # Extract key classification metrics
                    metric_columns = ['AUC', 'Accuracy', 'F1', 'Precision', 'Recall']
                    for metric in metric_columns:
                        if metric in metrics_df.columns:
                            # Get the value (usually in the first row)
                            value = metrics_df[metric].iloc[0] if len(metrics_df) > 0 else None
                            if value is not None:
                                performance_metrics[metric] = float(value)
                                log.info(f"  {metric}: {value:.4f}")
                
                else:  # regression
                    # Extract key regression metrics
                    metric_columns = ['R2', 'RMSE', 'MAE', 'MAPE']
                    for metric in metric_columns:
                        if metric in metrics_df.columns:
                            value = metrics_df[metric].iloc[0] if len(metrics_df) > 0 else None
                            if value is not None:
                                performance_metrics[metric] = float(value)
                                log.info(f"  {metric}: {value:.4f}")
                
                # If no standard metrics found, try to extract whatever is available
                if not performance_metrics:
                    log.warning("No standard metrics found, extracting available metrics")
                    for col in metrics_df.columns:
                        try:
                            value = metrics_df[col].iloc[0]
                            if isinstance(value, (int, float)):
                                performance_metrics[col] = float(value)
                        except:
                            continue
                
                log.info(f"Extracted {len(performance_metrics)} performance metrics")
                
            else:
                log.warning("Could not extract metrics from PyCaret, creating basic metrics")
                performance_metrics = {"status": "completed", "metrics_available": False}
        
        except Exception as e:
            log.warning(f"Failed to extract performance metrics: {e}")
            performance_metrics = {"status": "completed", "metrics_extraction_failed": True}
        
        # =============================
        # 6. GET BEST MODEL NAME
        # =============================
        try:
            # Try to get a more descriptive model name
            if hasattr(final_pipeline, 'named_steps'):
                # If it's a pipeline, get the last step (the actual model)
                estimator_step = list(final_pipeline.named_steps.values())[-1]
                best_model_name_str = str(type(estimator_step).__name__)
            else:
                # Direct model
                best_model_name_str = str(type(final_pipeline).__name__)
            
            # Clean up the name
            if best_model_name_str.endswith('Classifier') or best_model_name_str.endswith('Regressor'):
                # Remove common suffixes for cleaner display
                pass
            
            log.info(f"Final model name: {best_model_name_str}")
            
        except Exception as e:
            log.warning(f"Could not determine final model name: {e}")
            best_model_name_str = model_name if 'model_name' in locals() else "Unknown Model"
        
        # =============================
        # SUCCESS
        # =============================
        log.info("PyCaret experiment completed successfully")
        log.info(f"Final pipeline type: {type(final_pipeline)}")
        log.info(f"Model name: {best_model_name_str}")
        log.info(f"Performance metrics: {len(performance_metrics)} metrics extracted")
        
        return final_pipeline, performance_metrics, best_model_name_str
        
    except Exception as e:
        log.error(f"Unexpected error in PyCaret experiment: {e}")
        log.error(f"Traceback: {traceback.format_exc()}")
        return None, None, None


def validate_pycaret_inputs(
    df_ml_ready: pd.DataFrame,
    target_column_name: str,
    task_type: str
) -> Tuple[bool, List[str]]:
    """
    Validate inputs for PyCaret experiment.
    
    Args:
        df_ml_ready: The ML-ready DataFrame
        target_column_name: Name of the target column
        task_type: Task type ("classification" or "regression")
        
    Returns:
        Tuple of (is_valid, list_of_issues)
    """
    issues = []
    
    # Check DataFrame
    if df_ml_ready is None or df_ml_ready.empty:
        issues.append("DataFrame is None or empty")
        return False, issues
    
    # Check target column
    if not target_column_name or target_column_name not in df_ml_ready.columns:
        issues.append(f"Target column '{target_column_name}' not found in DataFrame")
    
    # Check task type
    if task_type not in ["classification", "regression"]:
        issues.append(f"Invalid task_type: {task_type}")
    
    # Check for minimum rows
    MIN_ROWS_FOR_ML = 30  # Increased from 10 to 30 for reliable cross-validation
    if len(df_ml_ready) < MIN_ROWS_FOR_ML:
        issues.append(f"Dataset too small: {len(df_ml_ready)} rows (minimum {MIN_ROWS_FOR_ML} required for reliable AutoML)")
    
    # Check for minimum features
    if len(df_ml_ready.columns) < 2:
        issues.append(f"Too few columns: {len(df_ml_ready.columns)} (minimum 2 required)")
    
    # Check target column has valid values
    if target_column_name in df_ml_ready.columns:
        target_nulls = df_ml_ready[target_column_name].isnull().sum()
        total_rows = len(df_ml_ready)
        
        if target_nulls == total_rows:
            issues.append("Target column is entirely null")
        elif target_nulls > total_rows * 0.5:
            issues.append(f"Target column has too many nulls: {target_nulls}/{total_rows} ({target_nulls/total_rows*100:.1f}%)")
        
        # Check for target variability
        unique_targets = df_ml_ready[target_column_name].nunique()
        if unique_targets < 2:
            issues.append(f"Target column has insufficient variability: {unique_targets} unique values")
    
    return len(issues) == 0, issues 


def _create_fallback_model(task_type: str, log):
    """
    Create a simple fallback model when PyCaret's compare_models fails.
    
    Args:
        task_type: "classification" or "regression"
        log: Logger instance
        
    Returns:
        A simple model or None if creation fails
    """
    try:
        log.info(f"Creating fallback model for {task_type}")
        
        if task_type == "classification":
            from pycaret.classification import create_model
            # Use a simple, robust classifier
            fallback_model = create_model('lr')  # Logistic Regression
            log.info("Created fallback Logistic Regression model")
        else:  # regression
            from pycaret.regression import create_model
            # Use a simple, robust regressor
            fallback_model = create_model('lr')  # Linear Regression
            log.info("Created fallback Linear Regression model")
        
        return fallback_model
        
    except Exception as e:
        log.error(f"Failed to create fallback model: {e}")
        return None
</file>

<file path="step_6_explain/explain_runner.py">
"""
Explainability Stage Runner for The Projection Wizard.
Orchestrates the complete model explainability stage using SHAP for global explanations.
"""

import pandas as pd
import joblib
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, Tuple
import warnings

from common import storage, constants, schemas, logger
from . import shap_logic

# Suppress warnings during SHAP processing
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=RuntimeWarning)


def run_explainability_stage(run_id: str) -> bool:
    """
    Execute the explainability stage to generate SHAP global feature importance analysis.
    
    This stage:
    1. Loads the trained PyCaret pipeline and metadata
    2. Loads the cleaned ML-ready data
    3. Validates inputs for SHAP analysis
    4. Generates and saves a SHAP summary plot
    5. Updates metadata with explainability results
    
    Args:
        run_id: Unique run identifier
        
    Returns:
        True if stage completes successfully, False otherwise
    """
    # Get stage-specific loggers for this run
    log = logger.get_stage_logger(run_id, constants.EXPLAIN_STAGE)
    structured_log = logger.get_stage_structured_logger(run_id, constants.EXPLAIN_STAGE)
    
    # Track timing
    start_time = datetime.now()
    
    try:
        # Log stage start
        log.info(f"Starting explainability stage for run {run_id}")
        log.info("="*50)
        log.info("EXPLAINABILITY STAGE - SHAP GLOBAL ANALYSIS")
        log.info("="*50)
        
        # Structured log: Stage started
        logger.log_structured_event(
            structured_log,
            "stage_started",
            {"stage": constants.EXPLAIN_STAGE},
            "Explainability stage started"
        )
        
        # Check if validation stage failed - prevent explainability from running
        try:
            status_data = storage.read_json(run_id, constants.STATUS_FILENAME)
            if status_data and status_data.get('stage') == constants.VALIDATION_STAGE:
                if status_data.get('status') == 'failed':
                    error_msg = "Cannot run explainability: validation stage failed"
                    validation_errors = status_data.get('errors', [])
                    if validation_errors:
                        error_msg += f" - Reasons: {'; '.join(validation_errors[:3])}"
                    
                    log.error(error_msg)
                    logger.log_structured_error(
                        structured_log,
                        "validation_failed_prerequisite",
                        error_msg,
                        {"stage": constants.EXPLAIN_STAGE, "validation_errors": validation_errors}
                    )
                    _update_status_failed(run_id, error_msg)
                    return False
        except Exception as e:
            log.warning(f"Could not check validation status: {e}")
            # Continue execution - don't fail on status check errors
        
        # Update status to running
        try:
            status_data = {
                "stage": constants.EXPLAIN_STAGE,
                "status": "running",
                "message": "Model explainability analysis in progress..."
            }
            storage.write_json_atomic(run_id, constants.STATUS_FILENAME, status_data)
            
            # Structured log: Status updated
            logger.log_structured_event(
                structured_log,
                "status_updated",
                {"status": "running", "stage": constants.EXPLAIN_STAGE},
                "Status updated to running"
            )
            
        except Exception as e:
            log.warning(f"Could not update status to running: {e}")
            logger.log_structured_error(
                structured_log,
                "status_update_failed",
                f"Could not update status to running: {e}",
                {"stage": constants.EXPLAIN_STAGE}
            )
        
        # =============================
        # 1. LOAD AND VALIDATE INPUTS
        # =============================
        log.info("Loading inputs: metadata, model pipeline, and cleaned data")
        
        # Load metadata.json
        try:
            metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
        except Exception as e:
            log.error(f"Failed to load metadata.json: {e}")
            logger.log_structured_error(
                structured_log,
                "metadata_load_failed",
                f"Failed to load metadata.json: {e}",
                {"stage": constants.EXPLAIN_STAGE}
            )
            _update_status_failed(run_id, f"Failed to load metadata: {str(e)}")
            return False
        
        if not metadata_dict:
            log.error("metadata.json is empty or invalid")
            logger.log_structured_error(
                structured_log,
                "metadata_invalid",
                "metadata.json is empty or invalid",
                {"stage": constants.EXPLAIN_STAGE}
            )
            _update_status_failed(run_id, "Empty or invalid metadata")
            return False
        
        # Get run directory
        run_dir = storage.get_run_dir(run_id)
        
        # Structured log: Metadata loaded
        logger.log_structured_event(
            structured_log,
            "metadata_loaded",
            {
                "file": constants.METADATA_FILENAME,
                "metadata_keys": list(metadata_dict.keys()),
                "run_directory": str(run_dir)
            },
            "Metadata loaded successfully"
        )
        
        # Validate required metadata components
        validation_success, metadata_components = _validate_metadata_for_explainability(
            metadata_dict, log
        )
        if not validation_success:
            logger.log_structured_error(
                structured_log,
                "metadata_validation_failed",
                "Invalid metadata for explainability stage",
                {"stage": constants.EXPLAIN_STAGE}
            )
            _update_status_failed(run_id, "Invalid metadata for explainability stage")
            return False
        
        target_info, automl_info = metadata_components
        
        # Structured log: Metadata validated
        logger.log_structured_event(
            structured_log,
            "metadata_validated",
            {
                "target_column": target_info.name,
                "task_type": target_info.task_type,
                "model_name": automl_info.best_model_name,
                "pipeline_path": automl_info.pycaret_pipeline_path
            },
            f"Metadata validation passed: {target_info.name} ({target_info.task_type})"
        )
        
        # Load PyCaret pipeline
        try:
            pycaret_pipeline_path = run_dir / automl_info.pycaret_pipeline_path
            
            if not pycaret_pipeline_path.exists():
                log.error(f"PyCaret pipeline not found: {pycaret_pipeline_path}")
                logger.log_structured_error(
                    structured_log,
                    "pipeline_file_not_found",
                    f"PyCaret pipeline not found: {pycaret_pipeline_path}",
                    {"stage": constants.EXPLAIN_STAGE, "pipeline_path": str(pycaret_pipeline_path)}
                )
                _update_status_failed(run_id, "PyCaret pipeline file not found")
                return False
            
            log.info(f"Loading PyCaret pipeline from: {pycaret_pipeline_path}")
            pycaret_pipeline = joblib.load(pycaret_pipeline_path)
            log.info("PyCaret pipeline loaded successfully")
            
            # Structured log: Pipeline loaded
            logger.log_structured_event(
                structured_log,
                "pipeline_loaded",
                {
                    "pipeline_path": str(pycaret_pipeline_path),
                    "model_name": automl_info.best_model_name
                },
                f"PyCaret pipeline loaded: {automl_info.best_model_name}"
            )
            
        except Exception as e:
            log.error(f"Failed to load PyCaret pipeline: {e}")
            logger.log_structured_error(
                structured_log,
                "pipeline_load_failed",
                f"Failed to load PyCaret pipeline: {e}",
                {"stage": constants.EXPLAIN_STAGE, "pipeline_path": str(pycaret_pipeline_path)}
            )
            _update_status_failed(run_id, f"Failed to load model pipeline: {str(e)}")
            return False
        
        # Load cleaned data
        try:
            cleaned_data_path = run_dir / constants.CLEANED_DATA_FILE
            
            if not cleaned_data_path.exists():
                log.error(f"Cleaned data file not found: {cleaned_data_path}")
                logger.log_structured_error(
                    structured_log,
                    "cleaned_data_not_found",
                    f"Cleaned data file not found: {cleaned_data_path}",
                    {"stage": constants.EXPLAIN_STAGE, "data_path": str(cleaned_data_path)}
                )
                _update_status_failed(run_id, "Cleaned data file not found")
                return False
            
            df_ml_ready = pd.read_csv(cleaned_data_path)
            log.info(f"Loaded cleaned data: shape {df_ml_ready.shape}")
            
            if df_ml_ready.empty:
                log.error("Cleaned data is empty")
                logger.log_structured_error(
                    structured_log,
                    "cleaned_data_empty",
                    "Cleaned data is empty",
                    {"stage": constants.EXPLAIN_STAGE}
                )
                _update_status_failed(run_id, "Cleaned data file is empty")
                return False
            
            # Structured log: Data loaded
            logger.log_structured_event(
                structured_log,
                "data_loaded",
                {
                    "data_shape": {"rows": df_ml_ready.shape[0], "columns": df_ml_ready.shape[1]},
                    "data_file": constants.CLEANED_DATA_FILE
                },
                f"Cleaned data loaded: {df_ml_ready.shape}"
            )
            
        except Exception as e:
            log.error(f"Failed to load cleaned data: {e}")
            logger.log_structured_error(
                structured_log,
                "data_load_failed",
                f"Failed to load cleaned data: {e}",
                {"stage": constants.EXPLAIN_STAGE, "data_path": str(cleaned_data_path)}
            )
            _update_status_failed(run_id, f"Failed to read cleaned data: {str(e)}")
            return False
        
        # Prepare feature data (remove target column)
        target_column = target_info.name
        if target_column not in df_ml_ready.columns:
            log.error(f"Target column '{target_column}' not found in cleaned data")
            logger.log_structured_error(
                structured_log,
                "target_column_missing",
                f"Target column '{target_column}' not found in cleaned data",
                {"stage": constants.EXPLAIN_STAGE, "target_column": target_column, "available_columns": list(df_ml_ready.columns)}
            )
            _update_status_failed(run_id, f"Target column '{target_column}' not found")
            return False
        
        X_data = df_ml_ready.drop(columns=[target_column])
        log.info(f"Feature data prepared: {X_data.shape} (removed target column '{target_column}')")
        
        # Structured log: Feature data prepared
        logger.log_structured_event(
            structured_log,
            "feature_data_prepared",
            {
                "feature_shape": {"rows": X_data.shape[0], "columns": X_data.shape[1]},
                "target_column": target_column,
                "feature_columns": list(X_data.columns)
            },
            f"Feature data prepared: {X_data.shape} features"
        )
        
        # =============================
        # 2. VALIDATE INPUTS FOR SHAP
        # =============================
        log.info("Validating inputs for SHAP analysis...")
        
        try:
            is_valid, validation_issues = shap_logic.validate_shap_inputs(
                pycaret_pipeline=pycaret_pipeline,
                X_data_sample=X_data,
                task_type=target_info.task_type,
                logger=log
            )
            
            if not is_valid:
                log.error("SHAP input validation failed:")
                for issue in validation_issues:
                    log.error(f"  - {issue}")
                logger.log_structured_error(
                    structured_log,
                    "shap_validation_failed",
                    f"SHAP validation failed: {'; '.join(validation_issues)}",
                    {"stage": constants.EXPLAIN_STAGE, "validation_issues": validation_issues}
                )
                _update_status_failed(run_id, f"SHAP validation failed: {'; '.join(validation_issues)}")
                return False
            
            log.info("SHAP input validation passed")
            
            # Structured log: SHAP validation passed
            logger.log_structured_event(
                structured_log,
                "shap_validation_passed",
                {"validation_issues_count": 0},
                "SHAP input validation passed"
            )
            
        except Exception as e:
            log.error(f"SHAP input validation error: {e}")
            logger.log_structured_error(
                structured_log,
                "shap_validation_error",
                f"SHAP input validation error: {e}",
                {"stage": constants.EXPLAIN_STAGE}
            )
            _update_status_failed(run_id, f"SHAP validation error: {str(e)}")
            return False
        
        # Test pipeline prediction capability
        log.info("Testing pipeline prediction capability...")
        try:
            if not shap_logic.test_pipeline_prediction(
                pycaret_pipeline, X_data, target_info.task_type, log
            ):
                log.error("Pipeline prediction test failed")
                logger.log_structured_error(
                    structured_log,
                    "pipeline_prediction_test_failed",
                    "Model pipeline prediction test failed",
                    {"stage": constants.EXPLAIN_STAGE}
                )
                _update_status_failed(run_id, "Model pipeline prediction test failed")
                return False
            
            log.info("Pipeline prediction test passed")
            
            # Structured log: Pipeline test passed
            logger.log_structured_event(
                structured_log,
                "pipeline_test_passed",
                {"test_type": "prediction_capability"},
                "Pipeline prediction test passed"
            )
            
        except Exception as e:
            log.error(f"Pipeline prediction test error: {e}")
            logger.log_structured_error(
                structured_log,
                "pipeline_test_error",
                f"Pipeline prediction test error: {e}",
                {"stage": constants.EXPLAIN_STAGE}
            )
            _update_status_failed(run_id, f"Pipeline prediction test error: {str(e)}")
            return False
        
        # =============================
        # 3. GENERATE SHAP SUMMARY PLOT
        # =============================
        log.info("Generating SHAP summary plot...")
        
        # Create plots directory and define plot path
        plots_dir = run_dir / constants.PLOTS_DIR
        plots_dir.mkdir(exist_ok=True)
        plot_save_path = plots_dir / constants.SHAP_SUMMARY_PLOT
        
        log.info(f"SHAP plot will be saved to: {plot_save_path}")
        
        # Structured log: Plot generation started
        logger.log_structured_event(
            structured_log,
            "plot_generation_started",
            {
                "plot_type": "shap_summary",
                "plots_directory": str(plots_dir),
                "plot_file": constants.SHAP_SUMMARY_PLOT
            },
            "SHAP summary plot generation started"
        )
        
        try:
            # Generate SHAP summary plot
            plot_success = shap_logic.generate_shap_summary_plot(
                pycaret_pipeline=pycaret_pipeline,
                X_data_sample=X_data,
                plot_save_path=plot_save_path,
                task_type=target_info.task_type,
                logger=log
            )
            
            if not plot_success:
                log.error("SHAP summary plot generation failed")
                logger.log_structured_error(
                    structured_log,
                    "plot_generation_failed",
                    "SHAP summary plot generation failed",
                    {"stage": constants.EXPLAIN_STAGE}
                )
                _update_status_failed(run_id, "SHAP plot generation failed")
                return False
            
            log.info("SHAP summary plot generated successfully")
            
            # Structured log: Plot generated
            logger.log_structured_event(
                structured_log,
                "plot_generated",
                {
                    "plot_type": "shap_summary",
                    "plots_directory": str(plots_dir),
                    "plot_file": constants.SHAP_SUMMARY_PLOT
                },
                "SHAP summary plot generated successfully"
            )
            
        except Exception as e:
            log.error(f"SHAP plot generation error: {e}")
            logger.log_structured_error(
                structured_log,
                "plot_generation_error",
                f"SHAP plot generation error: {e}",
                {"stage": constants.EXPLAIN_STAGE}
            )
            _update_status_failed(run_id, f"SHAP plot generation error: {str(e)}")
            return False
        
        # =============================
        # 4. UPDATE METADATA WITH EXPLAINABILITY INFO
        # =============================
        log.info("Updating metadata with explainability results...")
        
        try:
            # Create explainability info
            explain_info = {
                'tool_used': 'SHAP',
                'explanation_type': 'global_summary',
                'shap_summary_plot_path': str(Path(constants.PLOTS_DIR) / constants.SHAP_SUMMARY_PLOT),
                'explain_completed_at': datetime.utcnow().isoformat(),
                'target_column': target_info.name,
                'task_type': target_info.task_type,
                'features_explained': len(X_data.columns),
                'samples_used_for_explanation': len(X_data)
            }
            
            # Update metadata with explainability info
            metadata_dict['explain_info'] = explain_info
            storage.write_json_atomic(run_id, constants.METADATA_FILENAME, metadata_dict)
            
            log.info("Metadata updated with explainability information")
            
            # Structured log: Metadata updated
            logger.log_structured_event(
                structured_log,
                "metadata_updated",
                {
                    "file": constants.METADATA_FILENAME,
                    "metadata_keys": list(metadata_dict.keys()),
                    "run_directory": str(run_dir)
                },
                "Metadata updated with explainability information"
            )
            
        except Exception as e:
            log.error(f"Failed to update metadata: {e}")
            logger.log_structured_error(
                structured_log,
                "metadata_update_failed",
                f"Failed to update metadata: {e}",
                {"stage": constants.EXPLAIN_STAGE}
            )
            _update_status_failed(run_id, f"Failed to update metadata: {str(e)}")
            return False
        
        # =============================
        # 5. UPDATE PIPELINE STATUS
        # =============================
        log.info("Updating pipeline status to completed...")
        
        try:
            status_data = {
                "stage": constants.EXPLAIN_STAGE,
                "status": "completed",
                "message": "Model explainability analysis completed successfully"
            }
            storage.write_json_atomic(run_id, constants.STATUS_FILENAME, status_data)
            
            # Structured log: Status updated
            logger.log_structured_event(
                structured_log,
                "status_updated",
                {"status": "completed", "stage": constants.EXPLAIN_STAGE},
                "Status updated to completed"
            )
            
        except Exception as e:
            log.warning(f"Could not update final status: {e}")
            logger.log_structured_error(
                structured_log,
                "final_status_update_failed",
                f"Could not update final status: {e}",
                {"stage": constants.EXPLAIN_STAGE}
            )
        
        # Calculate stage duration
        end_time = datetime.now()
        stage_duration = (end_time - start_time).total_seconds()
        
        log.info("="*50)
        log.info("EXPLAINABILITY STAGE COMPLETED SUCCESSFULLY")
        log.info("="*50)
        log.info(f"SHAP summary plot saved to: {plot_save_path}")
        log.info(f"Features explained: {len(X_data.columns)}")
        log.info(f"Samples used: {len(X_data)}")
        
        # Structured log: Stage completed
        logger.log_structured_event(
            structured_log,
            "stage_completed",
            {
                "stage": constants.EXPLAIN_STAGE,
                "success": True,
                "duration_seconds": stage_duration,
                "completed_at": end_time.isoformat(),
                "plots_generated": [constants.SHAP_SUMMARY_PLOT],
                "features_explained": len(X_data.columns),
                "samples_used": len(X_data)
            },
            f"Explainability stage completed successfully in {stage_duration:.1f}s"
        )
        
        return True
        
    except Exception as e:
        log.error(f"Unexpected error in explainability stage: {e}")
        logger.log_structured_error(
            structured_log,
            "unexpected_error",
            f"Unexpected error in explainability stage: {e}",
            {"stage": constants.EXPLAIN_STAGE}
        )
        _update_status_failed(run_id, f"Unexpected error: {str(e)}")
        return False


def _validate_metadata_for_explainability(
    metadata_dict: dict, 
    log: logger.logging.Logger
) -> Tuple[bool, Optional[Tuple[schemas.TargetInfo, schemas.AutoMLInfo]]]:
    """
    Validate that metadata contains required information for explainability stage.
    
    Args:
        metadata_dict: Loaded metadata dictionary
        log: Logger instance
        
    Returns:
        Tuple of (is_valid, (target_info, automl_info) or None)
    """
    try:
        # Check for target_info
        target_info_dict = metadata_dict.get('target_info')
        if not target_info_dict:
            log.error("No target_info found in metadata")
            return False, None
        
        try:
            target_info = schemas.TargetInfo(**target_info_dict)
            log.info(f"Target info loaded: column='{target_info.name}', task_type='{target_info.task_type}'")
        except Exception as e:
            log.error(f"Failed to parse target_info: {e}")
            return False, None
        
        # Check for automl_info
        automl_info_dict = metadata_dict.get('automl_info')
        if not automl_info_dict:
            log.error("No automl_info found in metadata - AutoML stage must be completed first")
            return False, None
        
        try:
            automl_info = schemas.AutoMLInfo(**automl_info_dict)
            log.info(f"AutoML info loaded: tool='{automl_info.tool_used}', model='{automl_info.best_model_name}'")
        except Exception as e:
            log.error(f"Failed to parse automl_info: {e}")
            return False, None
        
        # Validate pipeline path exists in automl_info
        if not automl_info.pycaret_pipeline_path:
            log.error("No pycaret_pipeline_path found in automl_info")
            return False, None
        
        log.info("Metadata validation passed for explainability stage")
        return True, (target_info, automl_info)
        
    except Exception as e:
        log.error(f"Metadata validation error: {e}")
        return False, None


def _update_status_failed(run_id: str, error_message: str) -> None:
    """
    Update pipeline status to failed with error message.
    
    Args:
        run_id: Unique run identifier
        error_message: Error message to include in status
    """
    try:
        status_data = {
            "stage": constants.EXPLAIN_STAGE,
            "status": "failed",
            "message": error_message
        }
        storage.write_json_atomic(run_id, constants.STATUS_FILENAME, status_data)
    except Exception as e:
        # If we can't even update status, log it but don't fail further
        print(f"Warning: Could not update failed status for run {run_id}: {e}")


def validate_explainability_stage_inputs(run_id: str) -> bool:
    """
    Validate that all required inputs exist for the explainability stage.
    
    Args:
        run_id: Unique run identifier
        
    Returns:
        True if all inputs are valid, False otherwise
    """
    try:
        # Check metadata
        metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
        if not metadata_dict:
            return False
        
        # Validate metadata components
        validation_success, _ = _validate_metadata_for_explainability(
            metadata_dict, logger.get_logger(run_id, "validation")
        )
        if not validation_success:
            return False
        
        # Check files exist
        run_dir = storage.get_run_dir(run_id)
        
        # Check cleaned data
        cleaned_data_path = run_dir / constants.CLEANED_DATA_FILE
        if not cleaned_data_path.exists():
            return False
        
        # Check PyCaret pipeline
        automl_info_dict = metadata_dict.get('automl_info', {})
        pipeline_path = automl_info_dict.get('pycaret_pipeline_path')
        if not pipeline_path:
            return False
        
        pycaret_pipeline_path = run_dir / pipeline_path
        if not pycaret_pipeline_path.exists():
            return False
        
        return True
        
    except Exception:
        return False


def get_explainability_stage_summary(run_id: str) -> Optional[dict]:
    """
    Get summary information about the explainability stage results.
    
    Args:
        run_id: Unique run identifier
        
    Returns:
        Dictionary with explainability stage summary, or None if not completed
    """
    try:
        metadata_dict = storage.read_json(run_id, constants.METADATA_FILENAME)
        if not metadata_dict:
            return None
        
        explain_info = metadata_dict.get('explain_info')
        if not explain_info:
            return None
        
        # Get additional context
        target_info_dict = metadata_dict.get('target_info', {})
        automl_info_dict = metadata_dict.get('automl_info', {})
        
        run_dir = storage.get_run_dir(run_id)
        plot_path = run_dir / explain_info.get('shap_summary_plot_path', '')
        
        summary = {
            'explain_completed': True,
            'tool_used': explain_info.get('tool_used', 'SHAP'),
            'explanation_type': explain_info.get('explanation_type', 'global_summary'),
            'target_column': explain_info.get('target_column'),
            'task_type': explain_info.get('task_type'),
            'features_explained': explain_info.get('features_explained'),
            'samples_used': explain_info.get('samples_used_for_explanation'),
            'completed_at': explain_info.get('explain_completed_at'),
            'plot_file_exists': plot_path.exists() if plot_path else False,
            'plot_path': str(plot_path) if plot_path and plot_path.exists() else None,
            'best_model_name': automl_info_dict.get('best_model_name'),
            'performance_metrics': automl_info_dict.get('performance_metrics', {})
        }
        
        return summary
        
    except Exception:
        return None
</file>

<file path="step_6_explain/shap_logic.py">
"""
SHAP Logic for The Projection Wizard.
Core logic for generating SHAP explanations and summary plots.
"""

import shap
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Any, Optional, Union, Callable
import warnings

# Suppress some warnings that might come from SHAP/matplotlib
warnings.filterwarnings('ignore', category=FutureWarning, module='shap')
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')


def _create_prediction_function(pipeline: Any, task_type: str, logger: Optional[logging.Logger] = None) -> Callable:
    """
    Create an appropriate prediction function for SHAP based on available methods.
    
    Args:
        pipeline: The model pipeline
        task_type: "classification" or "regression"
        logger: Optional logger
        
    Returns:
        A callable function for SHAP to use for predictions
    """
    if logger is None:
        logger = logging.getLogger(__name__)
    
    if task_type == "classification":
        if hasattr(pipeline, 'predict_proba'):
            logger.info("Using predict_proba for SHAP classification")
            return pipeline.predict_proba
        elif hasattr(pipeline, 'decision_function'):
            logger.info("Using decision_function wrapper for SHAP classification")
            
            def decision_function_wrapper(X):
                """Convert decision function output to probability-like format"""
                decision_scores = pipeline.decision_function(X)
                
                # Handle different output shapes
                if decision_scores.ndim == 1:
                    # Binary classification: convert to 2D probabilities
                    # Use sigmoid to convert to [0, 1] range
                    proba_positive = 1 / (1 + np.exp(-decision_scores))
                    proba_negative = 1 - proba_positive
                    return np.column_stack([proba_negative, proba_positive])
                else:
                    # Multi-class: softmax transformation
                    exp_scores = np.exp(decision_scores - np.max(decision_scores, axis=1, keepdims=True))
                    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
            
            return decision_function_wrapper
        else:
            # Fall back to predict and create dummy probabilities
            logger.warning("Using predict with dummy probabilities for SHAP classification")
            
            def predict_wrapper(X):
                """Convert class predictions to dummy probabilities"""
                predictions = pipeline.predict(X)
                # Create dummy probabilities: 0.9 for predicted class, 0.1 for others
                unique_classes = np.unique(predictions)
                n_classes = len(unique_classes)
                n_samples = len(predictions)
                
                # Create probability matrix
                probabilities = np.full((n_samples, n_classes), 0.1 / (n_classes - 1) if n_classes > 1 else 0.5)
                
                for i, pred in enumerate(predictions):
                    class_idx = np.where(unique_classes == pred)[0][0]
                    probabilities[i, class_idx] = 0.9
                
                return probabilities
            
            return predict_wrapper
    
    else:  # regression
        if hasattr(pipeline, 'predict'):
            logger.info("Using predict for SHAP regression")
            return pipeline.predict
        else:
            raise ValueError("Pipeline has no suitable prediction method for regression")


def generate_shap_summary_plot(
    pycaret_pipeline: Any, 
    X_data_sample: pd.DataFrame, 
    plot_save_path: Path, 
    task_type: str,
    logger: Optional[logging.Logger] = None
) -> bool:
    """
    Generate a SHAP summary plot for the given PyCaret pipeline and save it as an image.
    
    Args:
        pycaret_pipeline: The loaded PyCaret pipeline object (from pycaret_pipeline.pkl)
        X_data_sample: A pandas DataFrame sample of the features (without target column)
        plot_save_path: Full Path object where the SHAP summary plot image will be saved
        task_type: "classification" or "regression" to guide SHAP explainer type
        logger: Optional logger for detailed logging
        
    Returns:
        True if plot generation and saving were successful, False otherwise
    """
    if logger is None:
        logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"Starting SHAP summary plot generation for {task_type} task")
        logger.info(f"Input data sample shape: {X_data_sample.shape}")
        logger.info(f"Plot save path: {plot_save_path}")
        
        # Ensure the output directory exists
        plot_save_path.parent.mkdir(parents=True, exist_ok=True)
        
        # =====================================
        # 1. PREPARE DATA SAMPLE FOR SHAP
        # =====================================
        
        # Limit sample size for performance if needed
        max_sample_size = 500
        if len(X_data_sample) > max_sample_size:
            logger.info(f"Sampling {max_sample_size} rows from {len(X_data_sample)} for SHAP performance")
            X_sample = X_data_sample.sample(n=max_sample_size, random_state=42)
        else:
            X_sample = X_data_sample.copy()
        
        logger.info(f"Using {len(X_sample)} samples for SHAP explanation")
        
        # Ensure data is numeric (should be after prep stage, but validate)
        if not all(X_sample.dtypes.apply(lambda x: np.issubdtype(x, np.number))):
            logger.warning("Non-numeric columns detected in data sample")
            # Log column types for debugging
            for col, dtype in X_sample.dtypes.items():
                if not np.issubdtype(dtype, np.number):
                    logger.warning(f"  Non-numeric column '{col}': {dtype}")
        
        # Convert boolean columns to integers to prevent SHAP isfinite errors
        bool_columns = X_sample.select_dtypes(include=['bool']).columns
        if len(bool_columns) > 0:
            logger.info(f"Converting {len(bool_columns)} boolean columns to integers for SHAP compatibility")
            X_sample = X_sample.copy()
            X_sample[bool_columns] = X_sample[bool_columns].astype(int)
            logger.info("Boolean to integer conversion completed")
        
        # Final validation that all columns are numeric
        non_numeric_cols = []
        for col in X_sample.columns:
            if not np.issubdtype(X_sample[col].dtype, np.number):
                non_numeric_cols.append(f"{col} ({X_sample[col].dtype})")
        
        if non_numeric_cols:
            logger.error(f"Still have non-numeric columns after conversion: {non_numeric_cols}")
            return False
        
        # =====================================
        # 2. CREATE SHAP EXPLAINER
        # =====================================
        
        logger.info("Creating SHAP explainer...")
        
        try:
            # Get appropriate prediction function
            prediction_function = _create_prediction_function(pycaret_pipeline, task_type, logger)
            
            # Create SHAP explainer
            logger.info("Creating SHAP explainer with adaptive prediction function")
            explainer = shap.Explainer(prediction_function, X_sample)
            
            logger.info("SHAP explainer created successfully")
            
        except Exception as e:
            logger.error(f"Failed to create SHAP explainer: {e}")
            # Try fallback approach with Kernel explainer
            try:
                logger.info("Attempting fallback with KernelExplainer...")
                prediction_function = _create_prediction_function(pycaret_pipeline, task_type, logger)
                explainer = shap.KernelExplainer(
                    prediction_function, 
                    shap.sample(X_sample, min(50, len(X_sample)))
                )
                logger.info("Fallback KernelExplainer created successfully")
            except Exception as e2:
                logger.error(f"Fallback explainer also failed: {e2}")
                return False
        
        # =====================================
        # 3. CALCULATE SHAP VALUES
        # =====================================
        
        logger.info("Calculating SHAP values...")
        
        try:
            # Calculate SHAP values for the sample
            shap_values = explainer(X_sample)
            logger.info("SHAP values calculated successfully")
            
            # Log shape information for debugging
            if hasattr(shap_values, 'values'):
                logger.info(f"SHAP values shape: {shap_values.values.shape}")
            else:
                logger.info(f"SHAP values type: {type(shap_values)}")
            
        except Exception as e:
            logger.error(f"Failed to calculate SHAP values: {e}")
            return False
        
        # =====================================
        # 4. HANDLE MULTI-CLASS CLASSIFICATION
        # =====================================
        
        # For classification with multiple outputs (multi-class), select appropriate values
        shap_values_for_plot = shap_values
        
        if task_type == "classification" and hasattr(shap_values, 'values'):
            if len(shap_values.values.shape) == 3:  # Multi-class case
                logger.info(f"Multi-class classification detected, shape: {shap_values.values.shape}")
                # Use the positive class (index 1) for binary, or sum for multi-class
                if shap_values.values.shape[2] == 2:
                    # Binary classification - use positive class
                    logger.info("Using positive class (index 1) for binary classification")
                    shap_values_for_plot = shap.Explanation(
                        values=shap_values.values[:, :, 1],
                        base_values=shap_values.base_values[:, 1] if hasattr(shap_values, 'base_values') else None,
                        data=shap_values.data if hasattr(shap_values, 'data') else X_sample.values,
                        feature_names=list(X_sample.columns)
                    )
                else:
                    # Multi-class - use mean absolute values across classes
                    logger.info("Using mean absolute values across classes for multi-class")
                    mean_abs_values = np.mean(np.abs(shap_values.values), axis=2)
                    shap_values_for_plot = shap.Explanation(
                        values=mean_abs_values,
                        base_values=None,
                        data=shap_values.data if hasattr(shap_values, 'data') else X_sample.values,
                        feature_names=list(X_sample.columns)
                    )
        
        # =====================================
        # 5. GENERATE AND SAVE PLOT
        # =====================================
        
        logger.info("Generating SHAP summary plot...")
        
        try:
            # Create a new figure to ensure clean state
            plt.figure(figsize=(10, 8))
            
            # Generate SHAP summary plot
            # Use bar plot for cleaner visualization in MVP
            shap.summary_plot(
                shap_values_for_plot, 
                X_sample, 
                show=False,  # Don't show, just prepare for saving
                plot_type="bar",  # Bar plot is cleaner for MVP
                max_display=20  # Limit to top 20 features for readability
            )
            
            # Improve plot appearance
            plt.title(f"SHAP Feature Importance - {task_type.title()}", fontsize=14, fontweight='bold')
            plt.xlabel("Mean |SHAP Value|", fontsize=12)
            plt.tight_layout()
            
            # Save the plot
            plt.savefig(
                plot_save_path, 
                bbox_inches='tight', 
                dpi=150,  # Good quality for reports
                facecolor='white',
                edgecolor='none'
            )
            
            # Close the figure to free memory
            plt.close()
            
            logger.info(f"SHAP summary plot saved successfully to: {plot_save_path}")
            
            # Verify file was created and has reasonable size
            if plot_save_path.exists():
                file_size_kb = plot_save_path.stat().st_size / 1024
                logger.info(f"Plot file size: {file_size_kb:.1f} KB")
                
                if file_size_kb < 1:  # Less than 1KB probably indicates an issue
                    logger.warning("Plot file size is very small, may indicate an issue")
                    return False
            else:
                logger.error("Plot file was not created")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to generate or save SHAP plot: {e}")
            # Ensure we close any open figures
            plt.close('all')
            return False
    
    except Exception as e:
        logger.error(f"Unexpected error in SHAP plot generation: {e}")
        # Ensure we close any open figures in case of error
        plt.close('all')
        return False


def validate_shap_inputs(
    pycaret_pipeline: Any,
    X_data_sample: pd.DataFrame,
    task_type: str,
    logger: Optional[logging.Logger] = None
) -> tuple[bool, list[str]]:
    """
    Validate inputs for SHAP analysis.
    
    Args:
        pycaret_pipeline: PyCaret pipeline object
        X_data_sample: Feature data sample
        task_type: "classification" or "regression"
        logger: Optional logger
        
    Returns:
        Tuple of (is_valid, list_of_issues)
    """
    if logger is None:
        logger = logging.getLogger(__name__)
    
    issues = []
    
    # Check pipeline
    if pycaret_pipeline is None:
        issues.append("PyCaret pipeline is None")
    
    # Check data sample
    if X_data_sample is None:
        issues.append("Data sample is None")
    elif X_data_sample.empty:
        issues.append("Data sample is empty")
    elif len(X_data_sample) < 2:
        issues.append(f"Data sample too small: {len(X_data_sample)} rows (minimum 2 required)")
    
    # Check if data has features
    if X_data_sample is not None and len(X_data_sample.columns) == 0:
        issues.append("Data sample has no features")
    
    # Check task type
    if task_type not in ["classification", "regression"]:
        issues.append(f"Invalid task type: {task_type} (must be 'classification' or 'regression')")
    
    # Check if pipeline has required methods
    if pycaret_pipeline is not None:
        if task_type == "classification":
            # For classification, check for any usable prediction method
            has_predict_proba = hasattr(pycaret_pipeline, 'predict_proba')
            has_decision_function = hasattr(pycaret_pipeline, 'decision_function')
            has_predict = hasattr(pycaret_pipeline, 'predict')
            
            if not (has_predict_proba or has_decision_function or has_predict):
                issues.append("Pipeline missing prediction methods (predict_proba, decision_function, or predict) required for classification")
        elif task_type == "regression" and not hasattr(pycaret_pipeline, 'predict'):
            issues.append("Pipeline missing predict method required for regression")
    
    is_valid = len(issues) == 0
    
    if logger and not is_valid:
        logger.warning("SHAP input validation failed:")
        for issue in issues:
            logger.warning(f"  - {issue}")
    
    return is_valid, issues


def test_pipeline_prediction(
    pycaret_pipeline: Any,
    X_data_sample: pd.DataFrame,
    task_type: str,
    logger: Optional[logging.Logger] = None
) -> bool:
    """
    Test if the pipeline can make predictions on the sample data.
    
    Args:
        pycaret_pipeline: PyCaret pipeline object
        X_data_sample: Feature data sample
        task_type: "classification" or "regression"
        logger: Optional logger
        
    Returns:
        True if prediction test passes, False otherwise
    """
    if logger is None:
        logger = logging.getLogger(__name__)
    
    try:
        # Take a small sample for testing
        test_sample = X_data_sample.head(min(5, len(X_data_sample)))
        
        if task_type == "classification":
            # Test the prediction function that will be used by SHAP
            try:
                prediction_function = _create_prediction_function(pycaret_pipeline, task_type, logger)
                result = prediction_function(test_sample)
                logger.info(f"Prediction test passed - output shape: {result.shape}")
                return True
            except Exception as e:
                logger.error(f"Classification prediction function test failed: {e}")
                return False
        else:
            # Test predict for regression
            pred_result = pycaret_pipeline.predict(test_sample)
            logger.info(f"Prediction test passed - predict shape: {pred_result.shape}")
            return True
        
    except Exception as e:
        logger.error(f"Pipeline prediction test failed: {e}")
        return False
</file>

<file path="orchestrator.py">
"""
Pipeline orchestrator for The Projection Wizard.
Provides a simple interface for running multiple pipeline stages sequentially.
"""

from common import logger


def run_from_schema_confirm(run_id: str) -> bool:
    """
    Run steps 3-7 synchronously after feature schemas are confirmed.
    Returns True on full success, False otherwise.

    Args:
        run_id: The ID of the run to process

    Returns:
        True if all stages completed successfully, False if any stage failed
    """
    # Get logger for orchestration
    orchestrator_logger = logger.get_logger(run_id, "api_orchestrator")

    orchestrator_logger.info("Starting pipeline execution: stages 3-7")

    try:
        # Import stage runners
        from pipeline.step_3_validation import validation_runner
        from pipeline.step_4_prep import prep_runner
        from pipeline.step_5_automl import automl_runner
        from pipeline.step_6_explain import explain_runner

        # Run each stage in sequence; bail on first failure

        # Stage 3: Validation
        orchestrator_logger.info("Starting stage 3: Validation")
        if not validation_runner.run_validation_stage(run_id):
            orchestrator_logger.error("Stage 3 (validation) failed")
            return False
        orchestrator_logger.info("Stage 3 (validation) completed successfully")

        # Stage 4: Data Preparation
        orchestrator_logger.info("Starting stage 4: Data Preparation")
        if not prep_runner.run_preparation_stage(run_id):
            orchestrator_logger.error("Stage 4 (preparation) failed")
            return False
        orchestrator_logger.info(
            "Stage 4 (preparation) completed successfully"
        )

        # Stage 5: AutoML
        orchestrator_logger.info("Starting stage 5: AutoML")
        if not automl_runner.run_automl_stage(run_id):
            orchestrator_logger.error("Stage 5 (automl) failed")
            return False
        orchestrator_logger.info("Stage 5 (automl) completed successfully")

        # Stage 6: Explainability
        orchestrator_logger.info("Starting stage 6: Explainability")
        if not explain_runner.run_explainability_stage(run_id):
            orchestrator_logger.error("Stage 6 (explain) failed")
            return False
        orchestrator_logger.info("Stage 6 (explain) completed successfully")

        orchestrator_logger.info("All pipeline stages completed successfully")
        return True

    except Exception as e:
        orchestrator_logger.error(
            f"Unexpected error in pipeline orchestrator: {str(e)}"
        )
        return False
</file>

</files>
